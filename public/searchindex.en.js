var relearn_searchindex = [
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Decorator plugin adds an array of llm/v1/chat messages to either the start or end of an LLM consumer’s chat history. This allows you to pre-engineer complex prompts, or steer (and guard) prompts in such a way that the modification to the consumer’s LLM message is completely transparent.\nYou can use this plugin to pre-set a system prompt, set up specific prompt history, add words and phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway.\ncat \u003e ai-prompt-decorator.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai options: temperature: 1.0 - name: ai-prompt-decorator instance_name: ai-prompt-decorator-openai config: prompts: prepend: - role: system content: \"You will always respond in the Portuguese (Brazil) language.\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-decorator.yaml Send a request now:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-5\" }' | jq You can now click Next to proceed further.",
    "description": "The AI Prompt Decorator plugin adds an array of llm/v1/chat messages to either the start or end of an LLM consumer’s chat history. This allows you to pre-engineer complex prompts, or steer (and guard) prompts in such a way that the modification to the consumer’s LLM message is completely transparent.\nYou can use this plugin to pre-set a system prompt, set up specific prompt history, add words and phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway.",
    "tags": [],
    "title": "AI Prompt Decorator",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/1-prompt-decorator/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Template plugin lets you provide tuned AI prompts to users. Users only need to fill in the blanks with variable placeholders in the following format: {{variable}}. This lets admins set up templates, which can be then be used by anyone in the organization. It also allows admins to present an LLM as an API in its own right - for example, a bot that can provide software class examples and/or suggestions.\nThis plugin also sanitizes string inputs to ensure that JSON control characters are escaped, preventing arbitrary prompt injection.\nWhen calling a template, simply replace the messages (llm/v1/chat) or prompt (llm/v1/completions) with a template reference, in the following format: {template://TEMPLATE_NAME}\nHere’s an example of template definition:\ncat \u003e ai-prompt-template.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-prompt-template instance_name: ai-prompt-template-openai enabled: true config: allow_untemplated_requests: true templates: - name: template1 template: |- { \"messages\": [ { \"role\": \"user\", \"content\": \"Explain to me what {{thing}} is.\" } ] } EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-template.yaml Now, send a request referring the template:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": \"{template://template1}\", \"properties\": { \"thing\": \"niilism\" } }' | jq Kong-gratulations! have now reached the end of this module by authenticating your API requests with AWS Cognito. You can now click Next to proceed with the next module.",
    "description": "The AI Prompt Template plugin lets you provide tuned AI prompts to users. Users only need to fill in the blanks with variable placeholders in the following format: {{variable}}. This lets admins set up templates, which can be then be used by anyone in the organization. It also allows admins to present an LLM as an API in its own right - for example, a bot that can provide software class examples and/or suggestions.",
    "tags": [],
    "title": "AI Prompt Template",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/2-prompt-template/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Let’s get started with a simple round-robin policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: round-robin targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Send the request few times. Note we are going to receive responses from both LLMs, in a round-robin way.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq Here’s the OpenAI’s gpt-4.1 response:\n{ \"id\": \"chatcmpl-C3lbQFtYTrEWwq7N6zKWh4CHl3idX\", \"object\": \"chat.completion\", \"created\": 1755014004, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"The title of \\\"greatest Polish writer\\\" is somewhat subjective and can depend on whom you ask and which literary genre or era is in focus. However, several names consistently rise to the top in critical consensus and cultural significance:\\n\\n1. **Adam Mickiewicz (1798–1855):** Often considered the national poet of Poland and a key figure in Romanticism, Mickiewicz is renowned for works such as *Pan Tadeusz* and *Dziady* (Forefathers' Eve). His impact on Polish identity and literature is unparalleled.\\n\\n2. **Henryk Sienkiewicz (1846–1916):** Winner of the Nobel Prize for Literature in 1905, Sienkiewicz is famous for historical novels such as *Quo Vadis* and his \\\"Trilogy\\\" (*With Fire and Sword*, *The Deluge*, *Fire in the Steppe*).\\n\\n3. **Wisława Szymborska (1923–2012):** One of the most celebrated modern poets, Szymborska received the Nobel Prize in 1996. Her poetry is noted for its wit, irony, and philosophical depth.\\n\\n4. **Czesław Miłosz (1911–2004):** A Nobel Prize-winning poet (1980), essayist, and thinker, Miłosz's impact on world literature and Polish thought is immense.\\n\\nGiven the foundational role Adam Mickiewicz played in defining Polish literature and his enduring influence, **Mickiewicz is most frequently regarded as the greatest Polish writer, particularly in terms of cultural and historical significance**.\\n\\nHowever, for different periods or genres, Sienkiewicz, Szymborska, or Miłosz may be regarded as the greatest by some readers or scholars.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 15, \"completion_tokens\": 364, \"total_tokens\": 379, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_799e4ca3f1\" } And here is llama3.2:1b’s:\n{ \"object\": \"chat.completion\", \"model\": \"llama3.2:1b\", \"choices\": [ { \"index\": 0, \"finish_reason\": \"stop\", \"message\": { \"content\": \"The greatest Polish writer is often debated among scholars and literary enthusiasts. However, some of the most highly regarded Polish writers include:\\n\\n1. Juliusz Słowacki (1795-1861): Considered one of the greatest Polish writers, Słowacki was a poet, playwright, and translator. He is known for his lyric poetry and plays that often explored themes of love, nature, and social issues.\\n2. Adam Mickiewicz (1809-1849): A leading figure in Poland's national revival movement, Mickiewicz was a poet, composer, and politician. He wrote the epic poem \\\"Pan Tadeusz,\\\" which is considered one of the greatest Polish works of literature.\\n3. Józef Piłsudski (1867-1935): While not traditionally considered a writer, Piłsudski was a prominent statesman and military leader who played a key role in Poland's struggle for independence. He also wrote two literary novels, \\\"The Tragedy of Maksimilian\\\" and \\\"The Trial of Maximilian.\\\"\\n4. Cyprian Kamil Norwid (1821-1883): A poet, playwright, and critic, Norwid was one of the most influential Polish writers of his time. He is known for his innovative style and exploration of themes such as love, nature, and social justice.\\n5. Henryk Sienkiewicz (1846-1916): Although primarily a novelist and playwright, Sienkiewicz's works often explored themes of politics, history, and morality. His novel \\\"Quo Vadis\\\" is considered one of the greatest Polish novels of all time.\\n\\nHowever, if I had to choose one writer who is widely regarded as the greatest Polish writer, it would be Juliusz Słowacki.\", \"role\": \"assistant\" } } ], \"usage\": { \"completion_tokens\": 370, \"total_tokens\": 403, \"prompt_tokens\": 33 }, \"created\": 1755014233 }",
    "description": "Let’s get started with a simple round-robin policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: round-robin targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Round Robin",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/2-round-robin/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Guard plugin lets you to configure a series of PCRE-compatible regular expressions as allow or deny lists, to guard against misuse of llm/v1/chat or llm/v1/completions requests.\nYou can use this plugin to allow or block specific prompts, words, phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway. It does this by scanning all chat messages (where the role is user) for the specific expressions set. You can use a combination of allow and deny rules to preserve integrity and compliance when serving an LLM service using Kong Gateway.\nFor llm/v1/chat type models: You can optionally configure the plugin to ignore existing chat history, wherein it will only scan the trailing user message. For llm/v1/completions type models: There is only one prompt field, thus the whole prompt is scanned on every request. The plugin matches lists of regular expressions to requests through AI Proxy. The matching behavior is as follows:\nIf any deny expressions are set, and the request matches any regex pattern in the deny list, the caller receives a 400 response. If any allow expressions are set, but the request matches none of the allowed expressions, the caller also receives a 400 response. If any allow expressions are set, and the request matches one of the allow expressions, the request passes through to the LLM. If there are both deny and allow expressions set, the deny condition takes precedence over allow. Any request that matches an entry in the deny list will return a 400 response, even if it also matches an expression in the allow list. If the request does not match an expression in the deny list, then it must match an expression in the allow list to be passed through to the LLM Here’s an example to allow only valid credit cards numbers:\ncat \u003e ai-prompt-guard.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-prompt-guard instance_name: ai-prompt-guard-openai enabled: true config: allow_all_conversation_history: true allow_patterns: - \".*\\\\\\\"card\\\\\\\".*\\\\\\\"4[0-9]{3}\\\\*{12}\\\\\\\"\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-guard.yaml Send a request with a valid credit card pattern:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Validate this card: {\\\"card\\\": \\\"4111************\\\", \\\"cvv\\\": \\\"000\\\"}\" } ] }' | jq '.' Now, send a non-valid number:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Validate this card: {\\\"card\\\": \\\"4111xyz************\\\", \\\"cvv\\\": \\\"000\\\"}\" } ] }' | jq '.' The expect result is:\n{ \"error\": { \"message\": \"bad request\" } }",
    "description": "The AI Prompt Guard plugin lets you to configure a series of PCRE-compatible regular expressions as allow or deny lists, to guard against misuse of llm/v1/chat or llm/v1/completions requests.\nYou can use this plugin to allow or block specific prompts, words, phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway. It does this by scanning all chat messages (where the role is user) for the specific expressions set. You can use a combination of allow and deny rules to preserve integrity and compliance when serving an LLM service using Kong Gateway.",
    "tags": [],
    "title": "AI Prompt Guard",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/3-prompt-guard/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Now, let’s redirect 80% of the request to OpenAI’s gpt-4.1 with a weight based policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} weight: 80 - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" weight: 20 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml",
    "description": "Now, let’s redirect 80% of the request to OpenAI’s gpt-4.1 with a weight based policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} weight: 80 - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" weight: 20 EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Weight",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/3-weight/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.\nCreate a file with the following declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Test the Route again.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq Lowest Usage policy The lowest-usage algorithm in AI Proxy Advanced is based on the volume of usage for each model. It balances the load by distributing requests to models with the lowest usage, measured by factors such as prompt token counts, response token counts, or other resource metrics.\nReplace the declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-usage targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration:\ndeck gateway reset --konnect-control-plane-name kong-aws --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml And test the Route again.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq",
    "description": "Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.\nCreate a file with the following declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Lowest-Latency and Lowest-Usage",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/4-lowest-latency-usage/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Semantic The semantic algorithm distributes requests to different models based on the similarity between the prompt in the request and the description provided in the model configuration. This allows Kong to automatically select the model that is best suited for the given domain or use case. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: semantic embeddings: auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: \"text-embedding-3-small\" vectordb: dimensions: 1024 distance_metric: cosine strategy: redis threshold: 1.0 redis: host: \"redis-stack.redis.svc.cluster.local\" port: 6379 database: 0 targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} description: \"mathematics, algebra, calculus, trigonometry\" - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" description: \"piano, orchestra, liszt, classical music\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Send a request related to Mathematics. The response should come from OpenAI’s gpt-4.1\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Tell me about the last theorem of Fermat\" } ] }' | jq On the other hand, Llama3.1 should be responsible for requests related to Classical Music.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who wrote the Hungarian Rhapsodies piano pieces?\" } ] }' | jq curl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Tell me a contemporaty pianist of Chopin\" } ] }' | jq If you check Redis, you’ll se there are two entries, related to the models\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan Expected output \"ai_proxy_advanced_semantic:01c84f59-b7c3-418b-818d-4369ef3e55ef:8f74aeaab95482bb37fbd69cd42154dcd6d321e1631ffdfd1802e1609d4c2481\" \"ai_proxy_advanced_semantic:01c84f59-b7c3-418b-818d-4369ef3e55ef:72a33ce9079fd34f6fb3624c3a4ba1a0df0c1aad267986db2249dc26a8808a41\"",
    "description": "Semantic The semantic algorithm distributes requests to different models based on the similarity between the prompt in the request and the description provided in the model configuration. This allows Kong to automatically select the model that is best suited for the given domain or use case. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.",
    "tags": [],
    "title": "Semantic Routing",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/5-semantic-routing/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "Knowledge Attendees should have basic knowledge of containers and other products like Keycloak, Redis, Prometheus etc.\nAttendees should have basic knowledge of GenAI models, specially LLMs and Embedding models as well as providers like OpenAI, Mistral, Anthropic, AWS, GCP, Azure.\nKong Academy Complete two badges via Kong Academy: Kong Gateway Foundations and Kong Gateway Operations\nEnvironment 1. Kong Konnect Subscription You will need a Kong Konnect Subscription to execute this workshop.\n2. decK (Declarations for Kong) Please, make sure you have decK installed in your local environment.\n3. LLM Some AI use cases require a GenAI infracture. Please make sure you have a Mistral API Key.\n4. Command Line Utilities In this workshop, we will use the following command line utilities\ncurl jq yq jwt-cli wget",
    "description": "Knowledge Attendees should have basic knowledge of containers and other products like Keycloak, Redis, Prometheus etc.\nAttendees should have basic knowledge of GenAI models, specially LLMs and Embedding models as well as providers like OpenAI, Mistral, Anthropic, AWS, GCP, Azure.\nKong Academy Complete two badges via Kong Academy: Kong Gateway Foundations and Kong Gateway Operations\nEnvironment 1. Kong Konnect Subscription You will need a Kong Konnect Subscription to execute this workshop.",
    "tags": [],
    "title": "Prerequisites",
    "uri": "/10-prerequisites/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Prerequisites",
    "content": "Konnect Plus Subscription You will need a Konnect subscription. Follow the steps below to obtain a Konnect Plus subscription. Initially $500 of credits are provided for up to 30 days, after the credits have expired or the time has finished Konnect Plus will charge based on Konnect Plus pricing. Following the workshop instructions will use less than $500 credits. After the workshop has finished the Konnect Plus environment can be deleted.\nClick on the Registration link and present your credentials.\nKonnect will send you an email to confirm the subscription. Click on the link in email to confirm your subscription.\nThe Konnect environment can be accessed via the Konnect log in page.\nAfter logging in create an organisation name, select a region, then answer a few questions.\nCredit available can be monitored though Plan and Usage page.",
    "description": "Konnect Plus Subscription You will need a Konnect subscription. Follow the steps below to obtain a Konnect Plus subscription. Initially $500 of credits are provided for up to 30 days, after the credits have expired or the time has finished Konnect Plus will charge based on Konnect Plus pricing. Following the workshop instructions will use less than $500 credits. After the workshop has finished the Konnect Plus environment can be deleted.",
    "tags": [],
    "title": "Konnect Subscription",
    "uri": "/10-prerequisites/konnect-subscription/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "The Kong Konnect platform provides a cloud control plane (CP), which manages all service configurations. It propagates those configurations to all Runtime control planes, which use in-memory storage. These nodes can be installed anywhere, on-premise or in AWS.\nFor today workshop, we will be focusing on Kong Gateway. Kong Gateway data plane listen for traffic on the proxy port 443 by default. The data plane evaluates incoming client API requests and routes them to the appropriate backend APIs. While routing requests and providing responses, policies can be applied with plugins as necessary.\nKonnect modules Kong Konnect Enterprise features are described in this section, including modules and plugins that extend and enhance the functionality of the Kong Konnect platform.\nControl Plane (Gateway Manager) Control Plane empowers your teams to securely collaborate and manage their own set of runtimes and services without the risk of impacting other teams and projects. Control Plane instantly provisions hosted Kong Gateway control planes and supports securely attaching Kong Gateway data planes from your cloud or hybrid environments.\nThrough the Control Plane, increase the security of your APIs with out-of-the-box enterprise and community plugins, including OpenID Connect, Open Policy Agent, Mutual TLS, and more.\nAI Manager Manage all of your LLMs in a single dashboard providing a unified control plane to create, manage, and monitor LLMs using the Konnect platform. With AI Manager you can assign Gateway Services and define how traffic is distributed across models, enable streaming responses and manage authentication through the AI Gateway, monitor request and token volumes, track error rates, and measure average latency with historical comparisons, etc.\nDev Portal Streamline developer onboarding with the Dev Portal, which offers a self-service developer experience to discover, register, and consume published services from your Service Hub catalog. This customizable experience can be used to match your own unique branding and highlights the documentation and interactive API specifications of your services. Enable application registration to automatically secure your APIs with a variety of authorization providers.\nAnalytics Use Analytics to gain deep insights into service, route, and application usage and health monitoring data. Keep your finger on the pulse of the health of your API products with custom reports and contextual dashboards. In addition, you can enhance the native monitoring and analytics capabilities with Kong Gateway plugins that enable streaming monitoring metrics to third-party analytics providers.\nTeams To help secure and govern your environment, Konnect provides the ability to manage authorization with teams. You can use Konnect’s predefined teams for a standard set of roles, or create custom teams with any roles you choose. Invite users and add them to these teams to manage user access. You can also map groups from your existing identity provider into Konnect teams.\nFurther Reading Gateway Manager AI Manager Dev Portal Analytics",
    "description": "The Kong Konnect platform provides a cloud control plane (CP), which manages all service configurations. It propagates those configurations to all Runtime control planes, which use in-memory storage. These nodes can be installed anywhere, on-premise or in AWS.\nFor today workshop, we will be focusing on Kong Gateway. Kong Gateway data plane listen for traffic on the proxy port 443 by default. The data plane evaluates incoming client API requests and routes them to the appropriate backend APIs. While routing requests and providing responses, policies can be applied with plugins as necessary.",
    "tags": [],
    "title": "Kong Konnect Architectural Overview",
    "uri": "/architecture/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "This chapter will walk you through\nKonnect Control Plane and Data Plane. Access Kong Data Plane. Here’s a Reference Architecture that will be implemented in this workshop:\nKong Konnect Control Plane: responsible for managing your APIs Kong Konnect Data Plane: connected to the Control Plane, it is responsible for processing all the incoming requests sent by the consumers. Kong provides a plugin framework, where each one of them is responsible for a specific functionality. As a can see, there are two main collections of plugins: On the left, the historic and regular API Gateway plugins, implementing all sort of policies including, for example, OIDC based Authentication processes with Keycloak, Amazon Cognito and Okta or Observability with Prometheus/Grafana and Dynatrace. On the right, another plugin collection for AI-based use cases. For example, the AI Rate Limiting plugin implements policies like this based on the number of tokens consumed be the requests. Or, as another example is the AI Semantic Cache plugin, which caches data based on the semantics related to the responses coming from the LLM models. Kong AI Gateway supports, out of the box, a variety of infrastructures, including not just OpenAI, but also Amazon Bedrock, Google Gemini, Mistral, Anthropic, etc. In order to deal with embeddings, the Gateway also supports also vector databases. Kong Gateway protects not just the LLM Models but also the upstream services, including your application micros surfaces or services. Konnect Control Planes Kong Konnect is the main Kong offering for hybrid deployments, including:\nSelf-managed Data Planes Dedicated Cloud Gateways - Data Plane nodes that are fully managed by Kong Serverless gateways - lightweight API gateways, also managed by Kong Kong Ingress Controller which allows you to run Kong Gateway as a Kubernetes Ingress. To check all these options, click + New Gateway inside the API Gateway menu option:\nServerless Proxy URL Konnect trial creates, by default, a Serverless API Gateway, naming it serverless-default Log in to the Kong Konnect UI. Inside the “API Gateway” page you should see your your Serverless Control Plane:\nCopy the value of your Control Plane Proxy URL and keep it handy. That’s the URL you Data Plane is located.\nSend a request to your Serverless Data Plane Save your URL in an enviroment variable:\nexport DATA_PLANE_URL=\u003cYOUR_DATA_PLANE_URL\u003e You can use curl to send the first request to the Data Plane\ncurl $DATA_PLANE_URL Expected result\n{ \"message\":\"no Route matched with those values\", \"request_id\":\"84fac2649eb6ae01f4d920115a4df70d\" } Konnect Control Plane and Kong Objects There are multiple ways to create new Kong Objects in your Control Plane:\nKonnect User Interface. RESTful Admin API, a fundamental mechanism for administration purposes. Kong Gateway Operator (KGO) and Kubernetes CRDs. deck - Declarations for Kong. To get an easier and faster deployment, this workshop uses deck and Konnect UI.\nThis tutorial is intended to be used for labs and PoC only. There are many aspects and processes, typically implemented in production sites, not described here. For example: Digital Certificate issuing, Cluster monitoring, etc. For a production ready deployment, refer Kong on Terraform Constructs, available here\nYou can now click Next to begin the module.",
    "description": "This chapter will walk you through\nKonnect Control Plane and Data Plane. Access Kong Data Plane. Here’s a Reference Architecture that will be implemented in this workshop:\nKong Konnect Control Plane: responsible for managing your APIs Kong Konnect Data Plane: connected to the Control Plane, it is responsible for processing all the incoming requests sent by the consumers. Kong provides a plugin framework, where each one of them is responsible for a specific functionality. As a can see, there are two main collections of plugins: On the left, the historic and regular API Gateway plugins, implementing all sort of policies including, for example, OIDC based Authentication processes with Keycloak, Amazon Cognito and Okta or Observability with Prometheus/Grafana and Dynatrace. On the right, another plugin collection for AI-based use cases. For example, the AI Rate Limiting plugin implements policies like this based on the number of tokens consumed be the requests. Or, as another example is the AI Semantic Cache plugin, which caches data based on the semantics related to the responses coming from the LLM models. Kong AI Gateway supports, out of the box, a variety of infrastructures, including not just OpenAI, but also Amazon Bedrock, Google Gemini, Mistral, Anthropic, etc. In order to deal with embeddings, the Gateway also supports also vector databases. Kong Gateway protects not just the LLM Models but also the upstream services, including your application micros surfaces or services. Konnect Control Planes Kong Konnect is the main Kong offering for hybrid deployments, including:",
    "tags": [],
    "title": "Konnect Setup",
    "uri": "/11-konnect-setup/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Setup",
    "content": "decK requires a Konnect Personal Access Token (PAT) to manage your Control Plane. To generate your PAT, click on your initials in the upper right corner of the Konnect home page, then select Personal Access Tokens. Click on + Generate Token, name your PAT, set its expiration time, and be sure to copy and save it in an evironment variable, as Konnect won’t display it again.\nNote Be sure to copy and save your PAT, as Konnect won’t display it again.\nKonnect PAT secret Save PAT in an environment variables export PAT=\u003cPASTE_THE_CONTENTS_OF_COPIED_PAT\u003e Test your PAT deck gateway ping --konnect-control-plane-name serverless-default --konnect-token $PAT You should get a response like this\nSuccessfully Konnected to the AcquaOrg organization!",
    "description": "decK requires a Konnect Personal Access Token (PAT) to manage your Control Plane. To generate your PAT, click on your initials in the upper right corner of the Konnect home page, then select Personal Access Tokens. Click on + Generate Token, name your PAT, set its expiration time, and be sure to copy and save it in an evironment variable, as Konnect won’t display it again.",
    "tags": [],
    "title": "PAT - Personal Access Token",
    "uri": "/11-konnect-setup/112-personal-access-token/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "With our Control Plane created and Data Plane layer deployed it’s time to create an API and expose an application. In this module, we will:\nUse Konnect UI to import an OpenAPI specification. Use decK to define a Kong Service based on an endpoint provided by the application and a Kong Route on top of the Kong Service to expose the application. Enable Kong Plugins to the Kong Route or Kong Service. Define Kong Consumers to represent the entities sending request to the Gateway and enable Kong Plugin to them. With decK (declarations for Kong) you can manage Kong Konnect configuration in a declaratively way.\ndecK operates on state files. decK state files describe the configuration of Kong API Gateway. State files encapsulate the complete configuration of Kong in a declarative format, including services, routes, plugins, consumers, and other entities that define how requests are processed and routed through Kong.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Konnect Gateway Manager",
    "description": "With our Control Plane created and Data Plane layer deployed it’s time to create an API and expose an application. In this module, we will:\nUse Konnect UI to import an OpenAPI specification. Use decK to define a Kong Service based on an endpoint provided by the application and a Kong Route on top of the Kong Service to expose the application. Enable Kong Plugins to the Kong Route or Kong Service. Define Kong Consumers to represent the entities sending request to the Gateway and enable Kong Plugin to them. With decK (declarations for Kong) you can manage Kong Konnect configuration in a declaratively way.",
    "tags": [],
    "title": "Kong API Gateway",
    "uri": "/12-api-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway",
    "content": "Kong Gateway can proxy:\nLayer 7 protocol, including: REST, GraphQL, gRPC, Websocket, SOAP, Kafka Layer 4 TCP and UDP Streaming Kong Gateway Service Gateway Services represent the upstream services in your system. These applications are the business logic components of your system responsible for responding to requests.\nThe configuration of a Gateway Service defines the connectivity details between the Kong Gateway and the upstream service, along with other metadata. Generally, you should map one Gateway Service to each upstream service.\nFor simple deployments, the upstream URL can be provided directly in the Gateway Service. For sophisticated traffic management needs, a Gateway Service can point at an Upstream.\nIn the following diagram, seven Kong Gateway Services objects should be defined in Kong.\nKong Route Gateway Services, in conjunction with Routes, let you expose your upstream services to clients with Kong Gateway, defining an entry point for client requests.\nA Kong Route defines rules that match client requests and associate them with a Kong Service. Routing can occur by PATH, URI, HEADERS, etc.\nA Kong Service can have many Kong Routes associated with it.\nKong Plugin Plugins can be attached to a Service, and will run against every request that triggers a request to the Service that they’re attached to.\nPlugins extend the functionality of Kong Gateway. They can be applied to different entities (i.e. Routes, Services, etc.).\nKong Gateway provides 90+ plugins out of the box and allows for Custom Plugins to be created",
    "description": "Kong Gateway can proxy:\nLayer 7 protocol, including: REST, GraphQL, gRPC, Websocket, SOAP, Kafka Layer 4 TCP and UDP Streaming Kong Gateway Service Gateway Services represent the upstream services in your system. These applications are the business logic components of your system responsible for responding to requests.\nThe configuration of a Gateway Service defines the connectivity details between the Kong Gateway and the upstream service, along with other metadata. Generally, you should map one Gateway Service to each upstream service.",
    "tags": [],
    "title": "Kong Gateway Service, Kong Route and Kong Plugin",
    "uri": "/12-api-gateway/121-kong-service-route/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway",
    "content": "For our first Kong Service, we are going to import an OpenAPI spec (OAS) to our Control Plane. The Control Plane will convert the spec into Kong Service and Kong Routes.\nDownload the bankong.yaml spec. In your Control Plane, click on Import via OAS spec. Choose the bankong.yaml spec and click Continue. Review the Import Summary and click Import Notice the Services and Routes that will be imported Notice declarative representation of this import as well (more on this later) You should see your new Kong Service and Routes:\nTest the API Deployment curl -i $DATA_PLANE_URL/transactions You should get a response from the API. Notice the x-kong-* headers like request id, proxy latency, upstream latency. Run the request again, what do you notice about the proxy latency now?\nHTTP/2 200 content-type: application/json; charset=utf-8 content-length: 517 x-kong-request-id: 79d76d16883197392033bd590536481b x-powered-by: Express vary: Origin, Accept-Encoding access-control-allow-credentials: true cache-control: no-cache pragma: no-cache expires: -1 x-content-type-options: nosniff etag: W/\"205-u4o2XSHOR6oYVCAyD/5BTbm6Xgk\" date: Tue, 23 Sep 2025 11:58:13 GMT server: kong/3.11.0.1-enterprise-edition x-kong-upstream-latency: 39 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.1-enterprise-edition, 1.1 kong/3.11.0.0-enterprise-edition [ { \"source\": \"DE8412325587359375895\", \"senderName\": \"Max Mustermann\", \"destination\": \"GR872659435350353\", \"amount\": 10.2, \"currency\": \"EUR\", \"subject\": \"The money we have talked about\", \"id\": \"b88f7029-fa93-41a5-9462-4884e544bf63\" }, { \"source\": \"UK8412325587359375895\", \"senderName\": \"Mister Smith\", \"destination\": \"GR872559435350353\", \"amount\": 10000, \"currency\": \"EUR\", \"subject\": \"Invoice #34078ja\", \"id\": \"143aadce-f995-4503-ba6e-01ed01c6af88\" } ]",
    "description": "For our first Kong Service, we are going to import an OpenAPI spec (OAS) to our Control Plane. The Control Plane will convert the spec into Kong Service and Kong Routes.\nDownload the bankong.yaml spec. In your Control Plane, click on Import via OAS spec. Choose the bankong.yaml spec and click Continue. Review the Import Summary and click Import Notice the Services and Routes that will be imported Notice declarative representation of this import as well (more on this later) You should see your new Kong Service and Routes:",
    "tags": [],
    "title": "Import an OpenAPI specification",
    "uri": "/12-api-gateway/122-oas-import/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway",
    "content": "Now, let’s use decK to create Kong Objects. This time, you’ll create and expose a service to the HTTPbin API. HTTPbin is an echo-type application that returns requests back to the requester as responses.\nPing Konnect with decK Before start using decK, you should ping Konnect to check if the connecting is up. Note we assume you have the PAT environment variable set. Please, refer to the previous section to learn how to issue a PAT.\ndeck gateway ping --konnect-control-plane-name serverless-default --konnect-token $PAT Expected Output\nSuccessfully Konnected to the AcquaOrg organization! Create a Kong Gateway Service and Kong Route Create the following declaration first. Remarks:\nNote the host and port refers to the HTTPbin’s endpoint http://httpbin.konghq.com:80. The declaration tags the objects so you can managing them apart from other ones. cat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /httpbin-route EOF Submit the declaration Now, you can use the following command to sync your Konnect Control Plane with the declaration. Note that all other existing objects will be deleted.\ndeck gateway sync --konnect-token $PAT httpbin.yaml Expected Output\ncreating service httpbin-service creating route httpbin-route Summary: Created: 2 Updated: 0 Deleted: 0 You should see your new service’s overview page.\nConsume the Route We are to use the same ELB provisioned during the Data Plane deployment:\ncurl -v $DATA_PLANE_URL/httpbin-route/get If successful, you should see the httpbin output:\n* Host kong-cceb6a93c9usmc2hk.kongcloud.dev:443 was resolved. * IPv6: (none) * IPv4: 66.51.127.198 * Trying 66.51.127.198:443... * Connected to kong-cceb6a93c9usmc2hk.kongcloud.dev (66.51.127.198) port 443 * ALPN: curl offers h2,http/1.1 * (304) (OUT), TLS handshake, Client hello (1): * CAfile: /etc/ssl/cert.pem * CApath: none * (304) (IN), TLS handshake, Server hello (2): * (304) (IN), TLS handshake, Unknown (8): * (304) (IN), TLS handshake, Certificate (11): * (304) (IN), TLS handshake, CERT verify (15): * (304) (IN), TLS handshake, Finished (20): * (304) (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / AEAD-CHACHA20-POLY1305-SHA256 / [blank] / UNDEF * ALPN: server accepted h2 * Server certificate: * subject: CN=*.kongcloud.dev * start date: Aug 15 23:28:00 2025 GMT * expire date: Nov 13 23:27:59 2025 GMT * subjectAltName: host \"kong-cceb6a93c9usmc2hk.kongcloud.dev\" matched cert's \"*.kongcloud.dev\" * issuer: C=US; O=Let's Encrypt; CN=E6 * SSL certificate verify ok. * using HTTP/2 * [HTTP/2] [1] OPENED stream for https://kong-cceb6a93c9usmc2hk.kongcloud.dev/httpbin-route/get * [HTTP/2] [1] [:method: GET] * [HTTP/2] [1] [:scheme: https] * [HTTP/2] [1] [:authority: kong-cceb6a93c9usmc2hk.kongcloud.dev] * [HTTP/2] [1] [:path: /httpbin-route/get] * [HTTP/2] [1] [user-agent: curl/8.7.1] * [HTTP/2] [1] [accept: */*] \u003e GET /httpbin-route/get HTTP/2 \u003e Host: kong-cceb6a93c9usmc2hk.kongcloud.dev \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/2 200 \u003c content-type: application/json \u003c content-length: 500 \u003c x-kong-request-id: 738fe4313578a34415154c6833df4e40 \u003c server: gunicorn/19.9.0 \u003c date: Tue, 23 Sep 2025 12:10:29 GMT \u003c access-control-allow-origin: * \u003c access-control-allow-credentials: true \u003c x-kong-upstream-latency: 12 \u003c x-kong-proxy-latency: 69 \u003c via: 1.1 kong/3.11.0.0-enterprise-edition \u003c { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Host\": \"httpbin.konghq.com\", \"User-Agent\": \"curl/8.7.1\", \"X-Forwarded-Host\": \"kong-cceb6a93c9usmc2hk.kongcloud.dev\", \"X-Forwarded-Path\": \"/httpbin-route/get\", \"X-Forwarded-Prefix\": \"/httpbin-route\", \"X-Kong-Request-Id\": \"738fe4313578a34415154c6833df4e40\" }, \"origin\": \"186.204.54.49, 66.51.127.198, 172.16.12.194\", \"url\": \"https://kong-cceb6a93c9usmc2hk.kongcloud.dev/get\" } * Connection #0 to host kong-cceb6a93c9usmc2hk.kongcloud.dev left intact```",
    "description": "Now, let’s use decK to create Kong Objects. This time, you’ll create and expose a service to the HTTPbin API. HTTPbin is an echo-type application that returns requests back to the requester as responses.\nPing Konnect with decK Before start using decK, you should ping Konnect to check if the connecting is up. Note we assume you have the PAT environment variable set. Please, refer to the previous section to learn how to issue a PAT.",
    "tags": [],
    "title": "Use decK to create Kong Objects",
    "uri": "/12-api-gateway/123-kong-service-route-deck/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "Concept APIOps applies DevOps principles (automation, version control, CI/CD) to API lifecycle management. Here’s a diagrama describing an APIOps lifecycle with Kong Technologies.\nThere are 3 main steps used to deliver APIs to Kong Gateway. These workflows are completed in sequence, with administrator approval between each step. Completion of final workflow results in a deployed API to Kong Konnect.\nOpenAPI to Kong: This step takes an OpenAPI specification and converts it to a decK file that can be used by Kong Gateway. Additionally, service specific patches are applied to the decK file allowing administrators to set overrides for specific APIs. Stage decK changes: This step takes the decK file generated by the OpenAPI to Kong step and compares it to the current state of the Kong Gateway. It generates a diff and stages the changes as a PR. decK sync: This step is triggered by the merging of deck configuration files to the main branch (by the previous workflow PR approval). The step triggers a deck gateway sync command on the configuration files and applies the changes to Kong Gateway. decK and APIOps With Kong Konnect and decK, APIs can be treated as code — versioned, tested, and deployed through pipelines.\nIn this section we are going to implement a simple flow starting with the OpenAPI specification, converting it to a Kong declaration and applying to the Control Plane.\nDownload the OpenAPI specification Download the kong-air-apis.yaml OpenAPI spec. The spec defines routes to consume APIs exposed by the KongAir API Server\nConvert the OpenAPI spec into Kong configuration deck file openapi2kong --spec ./kong-air-apis.yaml -o kong.yaml Add tags for traceability deck file add-tags kong-konnect-workshop -s kong.yaml -o kong.yaml Merge plugins configuration Create a plugin configuration file\ncat \u003e plugins.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - kong-konnect-workshop plugins: - name: proxy-cache enabled: true config: strategy: memory cache_ttl: 30 EOF Merge the files\ndeck file merge ./kong.yaml ./plugins.yaml -o kong.yaml Sync with Konnect deck gateway sync kong.yaml \\ --konnect-addr \"https://us.api.konghq.com\" \\ --konnect-control-plane-name serverless-default \\ --konnect-token $PAT \\ --select-tag kong-konnect-workshop Expected output:\ncreating service routes-service creating route routes-service_get-route creating route routes-service_health-check creating route routes-service_get-routes creating plugin proxy-cache (global) Summary: Created: 5 Updated: 0 Deleted: 0 Consume a Kong Route curl -s $DATA_PLANE_URL/routes | jq CI tool integration As a best practice, for a CI tool integration, these steps should be included in a script and integrated into a CI tool of your choice (e.g., Jenkins, GitHub Actions, GitLab CI).\nThe script should be triggered the script on every code push or pull request.\nLastly, you should store your Konnect token as secrets.\nKey Takeaways Treat APIs as code: store OpenAPI + Kong configs in version control. Automate API deployment with decK for repeatability and reliability. CI/CD tools can run Bash scripts to enable APIOps in real-world projects. Next Steps Extend the script with tests (linting, validation, diff before sync). Explore GitHub Actions or Jenkins examples for pipeline automation. Move towards full GitOps by integrating declarative API configs with source control workflows. Kong Konnect Automation Options Besides decK, Kong offers other options to implement APIOps automation:\nTerraform: Used for both Konnect platform and gateway configuration Konnect APIs: Used for both Konnect platform and gateway configuration Reset your Control Plane deck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f",
    "description": "Concept APIOps applies DevOps principles (automation, version control, CI/CD) to API lifecycle management. Here’s a diagrama describing an APIOps lifecycle with Kong Technologies.\nThere are 3 main steps used to deliver APIs to Kong Gateway. These workflows are completed in sequence, with administrator approval between each step. Completion of final workflow results in a deployed API to Kong Konnect.\nOpenAPI to Kong: This step takes an OpenAPI specification and converts it to a decK file that can be used by Kong Gateway. Additionally, service specific patches are applied to the decK file allowing administrators to set overrides for specific APIs. Stage decK changes: This step takes the decK file generated by the OpenAPI to Kong step and compares it to the current state of the Kong Gateway. It generates a diff and stages the changes as a PR. decK sync: This step is triggered by the merging of deck configuration files to the main branch (by the previous workflow PR approval). The step triggers a deck gateway sync command on the configuration files and applies the changes to Kong Gateway. decK and APIOps With Kong Konnect and decK, APIs can be treated as code — versioned, tested, and deployed through pipelines.",
    "tags": [],
    "title": "APIOps and decK",
    "uri": "/14-apiops-deck/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Proxy plugin is the fundamental AI Gateway component. It lets you transform and proxy requests to a number of AI providers and models. The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nThe following table describes which providers and requests the AI Proxy plugin supports:\nObs 1: OpenAI has marked Completions as legacy and recommends using the Chat Completions API for developing new applications.\nObs 2: Starting with Kong AI Gateway 3.11, new GenAI APIs are supported:\nGetting Started with Kong AI Gateway We are going to get started with a simple configuration. The following decK declaration enables the AI Proxy plugin to the Kong Gateway Service, to send requests to the LLM and consume the Ollama’s lamma3.2:1b FM and OpenAI’s gpt-5 FM with chat LLM requests.\nUpdate your ai-proxy.yaml file with that. Make sure you have the DECK_OPENAI_API_KEY environment variable set with your OpenAI’s API Key.\ncat \u003e ai-proxy.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-5 options: temperature: 1.0 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy.yaml OpenAI API Kong AI Gateway provides one API to access all of the LLMs it supports. To accomplish this, Kong AI Gateway has standardized on the OpenAI API specification. This will help developers to onboard more quickly by providing them with an API specification that they’re already familiar with. You can start using LLMs behind the AI Gateway simply by redirecting your requests to a URL that points to a route of the AI Gateway.\nSend a request to Kong AI Gateway Now, send a request to Kong AI Gateway following the OpenAI API Chat specification as a reference:\ncurl -s -X POST \\ --url http://$DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ] }' | jq Expected Output\nNote the response also complies to the OpenAI API spec:\n{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ] }' | jq { \"id\": \"chatcmpl-C3jWHoMI65rb0Ojkai1NjBq0JoRMG\", \"object\": \"chat.completion\", \"created\": 1755005997, \"model\": \"gpt-5-2025-08-07\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Pi (π) is the mathematical constant equal to the ratio of a circle’s circumference to its diameter. It’s the same for all circles.\\n\\n- Approximate value: 3.141592653589793…\\n- Nature: irrational (non-terminating, non-repeating) and transcendental.\\n- Common formulas:\\n - Circumference: C = 2πr\\n - Area of a circle: A = πr²\\n - Appears widely, e.g., e^(iπ) + 1 = 0, normal distribution, waves/Fourier analysis.\\n- Handy approximations: 22/7 ≈ 3.142857, 355/113 ≈ 3.14159292.\\n\\nIf you want more digits or historical background, say the word.\", \"refusal\": null, \"annotations\": [] }, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 10, \"completion_tokens\": 621, \"total_tokens\": 631, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 448, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": null } You can also consume the Ollama’s route\ncurl -s -X POST \\ --url http://$DATA_PLANE_LB/ollama-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ] }' | jq AI Proxy configuration parameters The AI Proxy plugin is responsible for a variety of topics. For example:\nRequest and response formats appropriate for the configured provider and route_type settings. provider can be set as anthropic, azure, bedrock, cohere, gemini, huggingface, llama2, mistral or openai. The route_type AI Proxy configuration parameter defines which kind of request the AI Gateway is going to perform. It must be one of: audio/v1/audio/speech audio/v1/audio/transcriptions audio/v1/audio/translations image/v1/images/edits image/v1/images/generations llm/v1/assistants llm/v1/batches llm/v1/chat llm/v1/completions llm/v1/embeddings llm/v1/files llm/v1/responses preserve realtime/v1/realtime Authentication on behalf of the Kong API consumer. Decorating the request with parameters from the config.model.options block, appropriate for the chosen provider. For our case, we tell the temperature we are going to use. Define the model to be consume when sending the request As you may have noticed our AI Proxy plugin defines the model it should consume. That is can be done for individual requests, if required. Change the ai-proxy.yaml file, removing the model’s name parameter and apply the declaration again:\ncat \u003e ai-proxy.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai options: temperature: 1.0 EOF deck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy.yaml Send the request specifing the model:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-5\" }' or\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-4\" }' Note the Kong AI Proxy plugin adds a new X-Kong-LLM-Model header with the model we consumer: openai/gpt-5 or openai/gpt-4\nStreaming Normally, a request is processed and completely buffered by the LLM before being sent back to Kong AI Gateway and then to the caller in a single large JSON block. This process can be time-consuming, depending on the request parameters, and the complexity of the request sent to the LLM model. To avoid making the user wait for their chat response with a loading animation, most models can stream each word (or sets of words and tokens) back to the client. This allows the chat response to be rendered in real time.\nThe config AI Proxy configuration section has a response_streaming parameter to define the response streaming. By default is set as allow but it can be set with deny or always.\nAs an example, if you send the same request with the stream parameter as true you should see a response like this:\ncurl -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-4\", \"stream\": true }' data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\",\"refusal\":null},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"D5jIQAiER0kD2\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Pi\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"3S9RmT4NS9k3b\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" (\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"3ARtgUA4COqRA\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\"π\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"IS99TImGO4SoLp\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\")\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"8jpC3eE7bQvh7b\"} ... data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"YHL01GZcTNF1Wh\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"obfuscation\":\"vf2t9C6t3\"} data: [DONE] Extra Model Options The Kong AI Proxy provides other configuration options. For example:\nmax_tokens: defines the max_tokens, if using chat or completion models. temperature: it is a number between 0 and 5 and it defines the matching temperature, if using chat or completion models. top_p: a number between 0 and 1 defining the top-p probability mass, if supported. top_k: an integer between 0 and 500 defining the top-k most likely tokens, if supported. Kong-gratulations! have now reached the end of this module by caching API responses. You can now click Next to proceed with the next module.",
    "description": "The AI Proxy plugin is the fundamental AI Gateway component. It lets you transform and proxy requests to a number of AI providers and models. The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nThe following table describes which providers and requests the AI Proxy plugin supports:",
    "tags": [],
    "title": "AI Proxy",
    "uri": "/16-ai-gateway/17-use-cases/150-ai-proxy/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "The Konnect Developer Portal is a customizable website for developers to locate, access, and consume API services. It enables developers to browse and search API documentation, try API operations, and manage their own credentials. The Portal supports both internal and external APIs through flexible deployment options.\nDev Portal APIs allow you to publish APIs using OpenAPI or AsyncAPI specifications and Markdown documentation. You can provide highly customizable content at both the site and API level to offer additional context to developers.\nKonnect Developer Portal provides an extensive list of benefits to Developers as well as Organizations:\nDevelopers\nAccelerates Onboarding: Self-service and instant access to documentation of APIs and testing tools Empowers Innovation through discovery of APIs and usage instructions Build applications faster Foster culture of innovation Improve Developer Experience Discovery of APIs In browser testing and troubleshooting tools Organizations\nDrives API Adoption Public or Partner facing portal markets APIs, fueling innovation and new revenue streams Reduces Support Overhead Comprehensive and searchable documentation and self-service tools shifts the burden of support Ensures Governance and Security Portal acts as single source of truth Ensures developers are using the correct, approved versions of APIs and adhering to policies Enhances Collaboration This section is divided into two parts:\nCreate a Developer Portal and publish an API Create API Products to consume the API",
    "description": "The Konnect Developer Portal is a customizable website for developers to locate, access, and consume API services. It enables developers to browse and search API documentation, try API operations, and manage their own credentials. The Portal supports both internal and external APIs through flexible deployment options.\nDev Portal APIs allow you to publish APIs using OpenAPI or AsyncAPI specifications and Markdown documentation. You can provide highly customizable content at both the site and API level to offer additional context to developers.",
    "tags": [],
    "title": "Konnect Developer Portal",
    "uri": "/15-developer-portal/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "Proxy Caching provides a reverse proxy cache implementation for Kong. It caches response entities based on configurable response code and content type, as well as request method. It can cache per-Consumer or per-API. Cache entities are stored for a configurable period of time, after which subsequent requests to the same resource will re-fetch and re-store the resource. Cache entities can also be forcefully purged via the Admin API prior to their expiration time.\nKong Gateway Plugin list Before enabling the Proxy Caching, let’s check the list of plugins Konnect provides. Inside the serverless-default Control Plane, click on Plugins menu option and + New plugin. You should the following page with all plugins available:\nEnabling a Kong Plugin on a Kong Service Create another declaration with plugins option. With this option you can enable and configure the plugin on your Kong Service.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.konghq.com port: 80 plugins: - name: proxy-cache instance_name: proxy-cache1 config: strategy: memory cache_ttl: 30 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route EOF For the plugin configuration we used the following settings:\nstrategy with memory. The plugin will use the Runtime Instance’s memory to implement to cache. cache_ttl with 30, which means the plugin will clear all data that reached this time limit. All plugin configuration paramenters are described inside Kong Plugin Hub portal, in its specific documentation page.\nSubmit the new declaration deck gateway sync --konnect-token $PAT httpbin.yaml Expected Output\ncreating plugin proxy-cache for service httpbin-service Summary: Created: 1 Updated: 0 Deleted: 0 Consume the Service If you consume the service again, you’ll see some new headers describing the caching status:\ncurl -i $DATA_PLANE_URL/httpbin-route/get HTTP/2 200 content-type: application/json content-length: 500 x-kong-request-id: abd3f90c6ecbbb0a0939fb2edab2b40d x-cache-key: f44e43eff1a09eeb35e0436a117f95f9363267d79089b8bdd950f85ca6247e97 x-cache-status: Miss server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:16:15 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 13 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Host\": \"httpbin.konghq.com\", \"User-Agent\": \"curl/8.7.1\", \"X-Forwarded-Host\": \"kong-cceb6a93c9usmc2hk.kongcloud.dev\", \"X-Forwarded-Path\": \"/httpbin-route/get\", \"X-Forwarded-Prefix\": \"/httpbin-route\", \"X-Kong-Request-Id\": \"abd3f90c6ecbbb0a0939fb2edab2b40d\" }, \"origin\": \"186.204.54.49, 66.51.127.198, 172.16.12.194\", \"url\": \"https://kong-cceb6a93c9usmc2hk.kongcloud.dev/get\" } Notice that, for the first request we get Miss for the X-Cache-Status header, meaning that the Runtime Instance didn’t have any data avaialble in the cache and had to connect to the Upstream Service, httpbin.org.\nIf we send a new request, the Runtime Instance has all it needs to satify the request, therefore the status is Hit. Note that the latency time has dropped considerably.\n% curl -i $DATA_PLANE_URL/httpbin-route/get HTTP/2 200 content-type: application/json x-kong-request-id: 4ad5c907f84c167c3eb3f716200ae17c x-cache-key: f44e43eff1a09eeb35e0436a117f95f9363267d79089b8bdd950f85ca6247e97 date: Tue, 23 Sep 2025 12:16:46 GMT server: gunicorn/19.9.0 age: 3 x-cache-status: Hit access-control-allow-origin: * access-control-allow-credentials: true content-length: 500 x-kong-upstream-latency: 0 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Host\": \"httpbin.konghq.com\", \"User-Agent\": \"curl/8.7.1\", \"X-Forwarded-Host\": \"kong-cceb6a93c9usmc2hk.kongcloud.dev\", \"X-Forwarded-Path\": \"/httpbin-route/get\", \"X-Forwarded-Prefix\": \"/httpbin-route\", \"X-Kong-Request-Id\": \"70184e54f9235642a310362396089529\" }, \"origin\": \"186.204.54.49, 66.51.127.198, 172.16.12.194\", \"url\": \"https://kong-cceb6a93c9usmc2hk.kongcloud.dev/get\" } Enabling a Kong Plugin on a Kong Route Now, we are going to define a Rate Limiting policy for our Service. This time, you are going to enable the Rate Limiting plugin to the Kong Route, not to the Kong Gateway Service. In this sense, new Routes defined for the Service will not have the Rate Limiting plugin enabled, only the Proxy Caching.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.konghq.com port: 80 plugins: - name: proxy-cache config: strategy: memory cache_ttl: 30 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 3 EOF The configuration includes:\nminute as 3, which means the Route can be consumed only 3 times a given minute. Submit the declaration deck gateway sync --konnect-token $PAT httpbin.yaml Consume the Service If you consume the service again, you’ll see, besides the caching related headers, new ones describing the status of current rate limiting policy:\ncurl -i $DATA_PLANE_URL/httpbin-route/get HTTP/2 200 content-type: application/json content-length: 500 x-kong-request-id: dc73a70617cde444eada947550425656 ratelimit-limit: 3 ratelimit-remaining: 2 x-ratelimit-limit-minute: 3 x-ratelimit-remaining-minute: 2 ratelimit-reset: 29 x-cache-key: f44e43eff1a09eeb35e0436a117f95f9363267d79089b8bdd950f85ca6247e97 x-cache-status: Miss server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:19:31 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 8 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Host\": \"httpbin.konghq.com\", \"User-Agent\": \"curl/8.7.1\", \"X-Forwarded-Host\": \"kong-cceb6a93c9usmc2hk.kongcloud.dev\", \"X-Forwarded-Path\": \"/httpbin-route/get\", \"X-Forwarded-Prefix\": \"/httpbin-route\", \"X-Kong-Request-Id\": \"dc73a70617cde444eada947550425656\" }, \"origin\": \"186.204.54.49, 66.51.127.198, 172.16.12.194\", \"url\": \"https://kong-cceb6a93c9usmc2hk.kongcloud.dev/get\" } If you keep sending new requests to the Runtime Instance, eventually, you’ll get a 429 error code, meaning you have reached the consumption rate limiting policy for this Route.\n% curl -i $DATA_PLANE_URL/httpbin-route/get HTTP/2 429 date: Tue, 23 Sep 2025 12:19:37 GMT content-type: application/json; charset=utf-8 x-kong-request-id: bf61f204c78bc7404721310d6a35ec11 retry-after: 23 ratelimit-limit: 3 ratelimit-remaining: 0 x-ratelimit-limit-minute: 3 x-ratelimit-remaining-minute: 0 ratelimit-reset: 23 content-length: 92 x-kong-response-latency: 0 server: kong/3.11.0.0-enterprise-edition { \"message\":\"API rate limit exceeded\", \"request_id\":\"bf61f204c78bc7404721310d6a35ec11\" } Enabling a Kong Plugin globally Besides scoping a plugin to a Kong Service or Route, we can apply it globally also. When we do it so, all Services ans Routes will enforce the police described by the plugin.\nFor example, let’s apply the Proxy Caching plugin globally.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route plugins: - name: proxy-cache config: strategy: memory cache_ttl: 30 services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.konghq.com port: 80 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 3 EOF Submit the declaration deck gateway sync --konnect-token $PAT httpbin.yaml Expected output Note the first Proxy Cache instance is deleted to get the Control Plane state synced with the declaratio:\ncreating plugin proxy-cache (global) deleting plugin proxy-cache for service httpbin-service Summary: Created: 1 Updated: 0 Deleted: 1 After testing the configuration, reset the Control Plane:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f",
    "description": "Proxy Caching provides a reverse proxy cache implementation for Kong. It caches response entities based on configurable response code and content type, as well as request method. It can cache per-Consumer or per-API. Cache entities are stored for a configurable period of time, after which subsequent requests to the same resource will re-fetch and re-store the resource. Cache entities can also be forcefully purged via the Admin API prior to their expiration time.",
    "tags": [],
    "title": "Proxy Caching and Rate Limiting",
    "uri": "/12-api-gateway/15-use-cases/150-proxy-caching-rate-limiting/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway",
    "content": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nProxy caching Rate Limiting API Key with Kong Consumers Request Transformer Request Callout OpenID Connect with Kong Identity These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "description": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nProxy caching Rate Limiting API Key with Kong Consumers Request Transformer Request Callout OpenID Connect with Kong Identity These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "tags": [],
    "title": "Use Cases",
    "uri": "/12-api-gateway/15-use-cases/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Developer Portal",
    "content": "Basic Developer Portal implementation In this section we are going to cover the basic steps to get a Developer Portal deployed\nKong Service and Route Create a Kong Service based on http://httpbin.konghq.com Create a Kong Route with path / Globally enable the CORS plugin to your Control Plane. That’s needed to solve the relationship between the Dev Portal and the Upstream Service. Dev Portal creation Choose the Dev Portal menu option and click + Create a new portal. Create a Private Portal named portal1. In the New Portal page accept the default values and click Save Click Go to overview. You should see the Overview page of your portal Check the Portal configuration: User authentication: Konnect built-in - That defines the default mechanism the Dev Portal uses for the user authentication. Besides the build-in option, it can be configured as OIDC or SAML. Default authentication strategy: key-auth - That defines the mechanism to control the API consumption inside the Dev Portal. Test your Dev Portal Sign Up If you click on the Dev Portal URL, you will play the Developer role and see your new portal home page. Since there’s no developer created, click Sing up and register to the Portal. Type your name and use a real email since the Dev Portal will send a confirmation request to it.\nCheck your email and click the Confirm your email address. Still playing the developer role, you should get redirected to the Dev Portal to define your password.\nAfter creating the password, if you try to login, you’ll receive an error message saying “Your account is disabled or pending approval”. That’s because, by default, the Dev Portal was created with the Auto approved developers option disabled, meaning the administrator has to manually approve the new developers registrations.\nApprove the Developer registration Getting back to the Dev Portal Administrator role, return to the Dev Portal menu option and choose Access and approvals. You can approve the new developer registration in the page:\nLogin to the Dev Portal Playing the Developer role again, try to login to the Dev Portal one more time. You should get redirected to the Dev Portal home page. Click the API tab. You are supposed to get an empty page since we don’t have any API published.\nAPI creation Prepare your OpenAPI specification Download the httpbin_spec.yaml OpenAPI specification. From the Konnect Dev Portal perspective, the spec has two main configurations:\nThe servers section. Make sure the url parameters has your Proxy URL: - url: \u003cYOUR_PROXY_URL\u003e Note the spec has added specific DevPortal elements in the security \u0026 securitySchemes sections. That means the DevPortal will use the Key Auth plugin to control the API Consumption inside the Portal. ################################# # Kong DevPortal Security mechanism ################################# security: - ApiKeyAuth: [] ################################# components: securitySchemes: ################################# # Kong Gateway Key-Auth ################################# ApiKeyAuth: type: apiKey in: header name: apikey ################################# Create your API Choose the APIs menu option inside Dev Portal and click + New API. Upload your httpbin_orig.yaml and click Create. You should see your httpbin API page:\nTest your API Click the API Specification tab. Click try it! in the Returns Origin IP\nAdd a documentation Click the Documentation tab. Create a new and empty document page with both name and slug as doc1. Click edit and type some documentation. Click save and switch the Published toggle on.\nAPI Publication Go to the Portals tab and click Publish API. Choose portal1and make sure you have the API visibility set as Private. Click Publish API. You should see the portal1 listed inside the tab.\nYou are going to play the Developer role again. Click on the URL shown in the Portals tab to get redirected to the Dev Portal. Login to it, if needed. Inside the Dev Portal home page, click the APIs tab. You should see the API you’ve just published.\nClick View APIs. You should see the documentation page with the data you entered before. If you click Overview you should see the httpbin API specification rendered in the page. If you click Try it for Returns Origin IP you should be able to send request to the Data Plane and consume the API. RBAC\nGo to APIs -\u003e Gateway Service. Link the API to the Kong Gateway Service. Show the Konnect App Auth plugin is enabled to the Service. You can link to a Konnect Gateway Service to allow developers to create applications and generate credentials or API keys. This is available to data planes running Kong Gateway 3.6 or later. When you link a service with an API, Konnect automatically adds the Konnect Application Auth (KAA) plugin on that Service. The KAA plugin is responsible for applying authentication and authorization on the Service. The authentication strategy that you select for the API defines how clients authenticate. While you can’t directly modify the KAA plugin as it’s managed by Konnect, you can modify the plugin’s behavior by adding JSON to the advanced configuration of your application auth strategy.\nPlay the developer role. If you try to consume the API will get a 401\nPortal -\u003e Settings -\u003e Security -\u003e turn RBAC on to your Portal\nPortal -\u003e Access and approvals -\u003e create a Team\nAdd the developer to the team\nInside your team -\u003e APIs -\u003e Add a new role with the existing API and “API Consumer” role\nPlay the developer role. If you try to consume the API you still get a 401\nApp\nPlay the developer role. Click “Use this API” and create an application (the Auth strategy is the default - api key auth)\nCopy the Credential (API Key - 3cI5F8xFj7DAkeAEfFA5vpHnQJjByYmx)\nApprove the Application\nPlay the developer role. Add your API Key in the Authentication box. You should be able to consume the API (e.g. Returns Origin IP)\nChoose your app and navigate to the Credentials tab.\n1. Create a Dev Portal Navigate to Konnect and go to Dev Portal. Click Create Dev Portal and fill in the required details (name, authentication settings, visibility, etc.). Save and note your Dev Portal URL. 2. Publish an API to the Dev Portal Go to Dev Portal \u003e APIs. Click New API and provide the API name, version, and upload an OpenAPI spec or Markdown documentation. (Optional) Link the API to a Gateway Service to enable developer self-service and authentication. Publish the API to your Dev Portal and select an authentication strategy if required. ℹ️ You must have the Product Publisher role to publish APIs to the portal.\n3. Register as a Developer and Create an Application Open your Dev Portal URL in a new browser window. Sign up as a new developer (or sign in if you already have an account). Once approved, go to My Apps and create a new application. Register your application for one or more APIs and generate credentials (API key or OIDC, depending on the authentication strategy). 4. Explore API Products (Classic Dev Portal v2) In Konnect, navigate to API Products. Create a new API Product, add a version, and link it to a Gateway Service. Add documentation (OpenAPI spec and/or Markdown). Publish the API Product to a classic Dev Portal (v2) by selecting the portal and confirming publication. Key Takeaways The Konnect Dev Portal enables you to publish, document, and manage APIs for internal, partner, or public consumption. APIs can be published to the Dev Portal with OpenAPI/AsyncAPI specs and Markdown documentation, and linked to Gateway Services for authentication and self-service. Developers can self-register, create applications, and generate credentials directly from the Dev Portal. API Products (v2) allow you to bundle multiple services and versions, manage documentation, and publish to classic Dev Portals for broader consumption. All of these actions can be performed via the Konnect UI, providing a user-friendly, centralized management experience for your API ecosystem. Next Steps Explore advanced Dev Portal customization options (branding, custom pages, and layouts). Review analytics for your APIs and API Products in Konnect. Learn about automating API and Dev Portal management using Konnect APIs or Terraform. References:\nKonnect Dev Portal Overview Publish your API to Dev Portal API Products and Classic Dev Portal Developer Self-Service and App Registration All steps above are achievable via the Konnect UI as described in the official documentation.",
    "description": "Basic Developer Portal implementation In this section we are going to cover the basic steps to get a Developer Portal deployed\nKong Service and Route Create a Kong Service based on http://httpbin.konghq.com Create a Kong Route with path / Globally enable the CORS plugin to your Control Plane. That’s needed to solve the relationship between the Dev Portal and the Upstream Service. Dev Portal creation Choose the Dev Portal menu option and click + Create a new portal. Create a Private Portal named portal1. In the New Portal page accept the default values and click Save Click Go to overview. You should see the Overview page of your portal Check the Portal configuration: User authentication: Konnect built-in - That defines the default mechanism the Dev Portal uses for the user authentication. Besides the build-in option, it can be configured as OIDC or SAML. Default authentication strategy: key-auth - That defines the mechanism to control the API consumption inside the Dev Portal. Test your Dev Portal Sign Up If you click on the Dev Portal URL, you will play the Developer role and see your new portal home page. Since there’s no developer created, click Sing up and register to the Portal. Type your name and use a real email since the Dev Portal will send a confirmation request to it.",
    "tags": [],
    "title": "Dev Portal creation and API publication",
    "uri": "/15-developer-portal/151-api-publication/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "For Prompt Engineering use cases, Kong AI Gateway provides:\nAI Prompt Decorator plugin: it injects messages at the start or end of a caller’s chat history. AI Prompt Guard plugin: it lets you configure a series of PCRE-compatible regular expressions to allow and block specific prompts, words, phrases, or otherwise and have more control over the LLM. AI Prompt Template plugin: it’s responsible for pre-configuring AI prompts to users All plugins extend the functionality of the AI Proxy plugin, and requires AI Proxy to be configured first.\nYou can now click Next to proceed further.",
    "description": "For Prompt Engineering use cases, Kong AI Gateway provides:\nAI Prompt Decorator plugin: it injects messages at the start or end of a caller’s chat history. AI Prompt Guard plugin: it lets you configure a series of PCRE-compatible regular expressions to allow and block specific prompts, words, phrases, or otherwise and have more control over the LLM. AI Prompt Template plugin: it’s responsible for pre-configuring AI prompts to users All plugins extend the functionality of the AI Proxy plugin, and requires AI Proxy to be configured first.",
    "tags": [],
    "title": "Prompt Engineering",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "To get started with API Authentication, let’s implement a basic Key Authentication mechanism. API Keys are one of the foundamental security mechanisms provided by Konnect. In order to consume an API, the consumer should inject a previously created API Key in the header of the request. The API consumption is allowed if the Gateway recognizes the API Key. Consumers add their API key either in a query string parameter, a header, or a request body to authenticate their requests and consume the application.\nA Kong Consumer represents a consumer (user or application) of a Service. A Kong Consumer is tightly coupled to an Authentication mechanism the Kong Gateway provides.\nPlease, check the Key-Auth plugin plugin and Kong Consumer documentation pages to learn more about them.\nEnable the Key Authentication Plugin on the Kong Route cat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 EOF deck gateway sync --konnect-token $PAT key-auth.yaml Consume the Route Now, if you try the Route, you’ll get a specific 401 error code meaning that, since you don’t have any API Key injected in your request, you are not allowd to consume it.\ncurl -i $DATA_PLANE_URL/key-auth-route/get HTTP/2 401 date: Tue, 23 Sep 2025 12:28:41 GMT content-type: application/json; charset=utf-8 x-kong-request-id: 5082dc799dcc913c3e27581f84eba120 www-authenticate: Key content-length: 96 x-kong-response-latency: 1 server: kong/3.11.0.0-enterprise-edition { \"message\":\"No API key found in request\", \"request_id\":\"5082dc799dcc913c3e27581f84eba120\" } Create a Kong Consumer In order to consume the Route we need to create a Kong Consumer. Here’s its declaration:\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT key-auth.yaml Consume the Route with the API Key Now, you need to inject the Key you’ve just created, as a header, in your requests. Using HTTPie, you can do it easily like this:\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:123456' HTTP/2 200 content-type: application/json content-length: 702 x-kong-request-id: b54df81da8dde905312cd55d5600f638 server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:29:57 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 10 x-kong-proxy-latency: 2 via: 1.1 kong/3.11.0.0-enterprise-edition Of course, if you inject a wrong key, you get a specific error like this:\n% curl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:12' HTTP/2 401 date: Tue, 23 Sep 2025 12:30:23 GMT content-type: application/json; charset=utf-8 x-kong-request-id: 31c73846e3a250366a42d805f0282b4b www-authenticate: Key content-length: 81 x-kong-response-latency: 1 server: kong/3.11.0.0-enterprise-edition NOTE\nThe header has to have the API Key name, which is, in our case, apikey. That was the default name provided by Konnect when you enabled the Key Authentication on the Kong Route. You can change the plugin configuration, if you will. Kong Consumer Policies With the API Key policy in place, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIt’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:\nconsumer1: apikey = 123456 rate limiting policy = 5 rpm consumer2: apikey = 987654 rate limiting policy = 8 rpm Doing that, the Data Plane is capable to not just protect the Route but to identify the consumer based on the key injected to enforce specific policies to the consumer.\nFor this section we’re implementing a Rate Limiting policy. Keep in mind that a Consumer might have other plugins also enabled such as Request Transformer, TCP Log, etc.\nNew Consumer Create the second consumer2, just like you did with the first one, with the 987654 key.\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 - keyauth_credentials: - key: \"987654\" username: consumer2 EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT key-auth.yaml If you will, you can inject both keys to your requests.\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:123456' or\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:987654' Consumers’ Policy Now let’s enhance the plugins declaration enabling the Rate Limiting plugin to each one of our consumers.\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 5 - keyauth_credentials: - key: \"987654\" username: consumer2 plugins: - name: rate-limiting instance_name: rate-limiting2 config: minute: 8 EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT key-auth.yaml Consumer the Route using different API Keys. First of all let’s consume the Route with the Consumer1’s API Key:\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:123456' Expected Output\nHTTP/2 200 content-type: application/json content-length: 702 x-kong-request-id: 863f8f3fc16dd32bdf8f045465857c7e ratelimit-limit: 5 ratelimit-remaining: 4 x-ratelimit-limit-minute: 5 x-ratelimit-remaining-minute: 4 ratelimit-reset: 35 server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:33:25 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 8 x-kong-proxy-latency: 2 via: 1.1 kong/3.11.0.0-enterprise-edition Now, let’s consume it with the Consumer2’s API Key. As you can see the Data Plane is processing the Rate Limiting processes independently.\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:987654' Expected Output\nHTTP/2 200 content-type: application/json content-length: 702 x-kong-request-id: 3c99310c94ed3157f1cdb4604e0a8c4e ratelimit-limit: 8 ratelimit-remaining: 7 x-ratelimit-limit-minute: 8 x-ratelimit-remaining-minute: 7 ratelimit-reset: 9 server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:33:51 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 9 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition If we keep sending requests using the first API Key, eventually, as expected, we’ll get an error code:\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:123456' Expected Output\nHTTP/2 429 date: Tue, 23 Sep 2025 12:34:21 GMT content-type: application/json; charset=utf-8 x-kong-request-id: 3e890328c589ee56b57a0171c4c89267 retry-after: 39 ratelimit-limit: 5 ratelimit-remaining: 0 x-ratelimit-limit-minute: 5 x-ratelimit-remaining-minute: 0 ratelimit-reset: 39 content-length: 92 x-kong-response-latency: 0 server: kong/3.11.0.0-enterprise-edition However, the second API Key is still allowed to consume the Kong Route:\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:987654' Expected Output\nHTTP/2 200 content-type: application/json content-length: 702 x-kong-request-id: 86f7c102950a06452a4e0f66c290f30e ratelimit-limit: 8 ratelimit-remaining: 7 x-ratelimit-limit-minute: 8 x-ratelimit-remaining-minute: 7 ratelimit-reset: 26 server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:34:34 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 8 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.\nOptional Reading Applying Kong Plugins on Services, Routes or Globally helps us to implement an extensive list of policies in the API Gateway layer. However, so far, we are not controlling who is sending the requests to the Data Plane. That is, anyone who has the Runtime Instance ELB address is capable to send requests to it and consumer the Services.\nAPI Gateway Authentication is an important way to control the data that is allowed to be transmitted using your APIs. Basically, it checks that a particular consumer has permission to access the API, using a predefined set of credentials.\nKong Gateway has a library of plugins that provide simple ways to implement the best known and most widely used methods of API gateway authentication. Here are some of the commonly used ones:\nBasic Authentication Key Authentication OAuth 2.0 Authentication LDAP Authentication OpenID Connect Kong Plugin Hub provides documentation about all Authentication based plugins. Refer to the following link to read more about API Gateway Authentication",
    "description": "To get started with API Authentication, let’s implement a basic Key Authentication mechanism. API Keys are one of the foundamental security mechanisms provided by Konnect. In order to consume an API, the consumer should inject a previously created API Key in the header of the request. The API consumption is allowed if the Gateway recognizes the API Key. Consumers add their API key either in a query string parameter, a header, or a request body to authenticate their requests and consume the application.",
    "tags": [],
    "title": "API Key Authentication and Rate Limiting",
    "uri": "/12-api-gateway/15-use-cases/152-key-authn-rate-limiting/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Developer Portal",
    "content": "Basic Developer Portal implementation In this section we are going to cover the basic steps to get a Developer Portal deployed\nKong Service and Route Create a Kong Service based on http://httpbin.konghq.com Create a Kong Route with path / Globally enable the CORS plugin to your Control Plane. That’s needed to solve the relationship between the Dev Portal and the Upstream Service. Dev Portal creation Choose the Dev Portal menu option and click + Create a new portal. Create a Private Portal named portal1. In the New Portal page accept the default values and click Save Click Go to overview. You should see the Overview page of your portal Check the Portal configuration: User authentication: Konnect built-in - That defines the default mechanism the Dev Portal uses for the user authentication. Besides the build-in option, it can be configured as OIDC or SAML. Default authentication strategy: key-auth - That defines the mechanism to control the API consumption inside the Dev Portal. Test your Dev Portal Sign Up If you click on the Dev Portal URL, you will play the Developer role and see your new portal home page. Since there’s no developer created, click Sing up and register to the Portal. Type your name and use a real email since the Dev Portal will send a confirmation request to it.\nCheck your email and click the Confirm your email address. Still playing the developer role, you should get redirected to the Dev Portal to define your password.\nAfter creating the password, if you try to login, you’ll receive an error message saying “Your account is disabled or pending approval”. That’s because, by default, the Dev Portal was created with the Auto approved developers option disabled, meaning the administrator has to manually approve the new developers registrations.\nApprove the Developer registration Getting back to the Dev Portal Administrator role, return to the Dev Portal menu option and choose Access and approvals. You can approve the new developer registration in the page:\nLogin to the Dev Portal Playing the Developer role again, try to login to the Dev Portal one more time. You should get redirected to the Dev Portal home page. Click the API tab. You are supposed to get an empty page since we don’t have any API published.\nAPI creation Prepare your OpenAPI specification Download the httpbin_spec.yaml OpenAPI specification. From the Konnect Dev Portal perspective, the spec has two main configurations:\nThe servers section. Make sure the url parameters has your Proxy URL: - url: \u003cYOUR_PROXY_URL\u003e Note the spec has added specific DevPortal elements in the security \u0026 securitySchemes sections. That means the DevPortal will use the Key Auth plugin to control the API Consumption inside the Portal. ################################# # Kong DevPortal Security mechanism ################################# security: - ApiKeyAuth: [] ################################# components: securitySchemes: ################################# # Kong Gateway Key-Auth ################################# ApiKeyAuth: type: apiKey in: header name: apikey ################################# Create your API Choose the APIs menu option inside Dev Portal and click + New API. Upload your httpbin_orig.yaml and click Create. You should see your httpbin API page:\nTest your API Click the API Specification tab. Click try it! in the Returns Origin IP\nAdd a documentation Click the Documentation tab. Create a new and empty document page with both name and slug as doc1. Click edit and type some documentation. Click save and switch the Published toggle on.\nPublish the API\nGo to APIs -\u003e Portals tab and publish the API to the Portal with Private visibility\nPlay the developer role. Reload the Portal and see the new API available\nYou should be able to consume the API, e.g. “Returns Origin IP”.\nRBAC\nGo to APIs -\u003e Gateway Service. Link the API to the Kong Gateway Service. Show the Konnect App Auth plugin is enabled to the Service. You can link to a Konnect Gateway Service to allow developers to create applications and generate credentials or API keys. This is available to data planes running Kong Gateway 3.6 or later. When you link a service with an API, Konnect automatically adds the Konnect Application Auth (KAA) plugin on that Service. The KAA plugin is responsible for applying authentication and authorization on the Service. The authentication strategy that you select for the API defines how clients authenticate. While you can’t directly modify the KAA plugin as it’s managed by Konnect, you can modify the plugin’s behavior by adding JSON to the advanced configuration of your application auth strategy.\nPlay the developer role. If you try to consume the API will get a 401\nPortal -\u003e Settings -\u003e Security -\u003e turn RBAC on to your Portal\nPortal -\u003e Access and approvals -\u003e create a Team\nAdd the developer to the team\nInside your team -\u003e APIs -\u003e Add a new role with the existing API and “API Consumer” role\nPlay the developer role. If you try to consume the API you still get a 401\nApp\nPlay the developer role. Click “Use this API” and create an application (the Auth strategy is the default - api key auth)\nCopy the Credential (API Key - 3cI5F8xFj7DAkeAEfFA5vpHnQJjByYmx)\nApprove the Application\nPlay the developer role. Add your API Key in the Authentication box. You should be able to consume the API (e.g. Returns Origin IP)\nChoose your app and navigate to the Credentials tab.\n1. Create a Dev Portal Navigate to Konnect and go to Dev Portal. Click Create Dev Portal and fill in the required details (name, authentication settings, visibility, etc.). Save and note your Dev Portal URL. 2. Publish an API to the Dev Portal Go to Dev Portal \u003e APIs. Click New API and provide the API name, version, and upload an OpenAPI spec or Markdown documentation. (Optional) Link the API to a Gateway Service to enable developer self-service and authentication. Publish the API to your Dev Portal and select an authentication strategy if required. ℹ️ You must have the Product Publisher role to publish APIs to the portal.\n3. Register as a Developer and Create an Application Open your Dev Portal URL in a new browser window. Sign up as a new developer (or sign in if you already have an account). Once approved, go to My Apps and create a new application. Register your application for one or more APIs and generate credentials (API key or OIDC, depending on the authentication strategy). 4. Explore API Products (Classic Dev Portal v2) In Konnect, navigate to API Products. Create a new API Product, add a version, and link it to a Gateway Service. Add documentation (OpenAPI spec and/or Markdown). Publish the API Product to a classic Dev Portal (v2) by selecting the portal and confirming publication. Key Takeaways The Konnect Dev Portal enables you to publish, document, and manage APIs for internal, partner, or public consumption. APIs can be published to the Dev Portal with OpenAPI/AsyncAPI specs and Markdown documentation, and linked to Gateway Services for authentication and self-service. Developers can self-register, create applications, and generate credentials directly from the Dev Portal. API Products (v2) allow you to bundle multiple services and versions, manage documentation, and publish to classic Dev Portals for broader consumption. All of these actions can be performed via the Konnect UI, providing a user-friendly, centralized management experience for your API ecosystem. Next Steps Explore advanced Dev Portal customization options (branding, custom pages, and layouts). Review analytics for your APIs and API Products in Konnect. Learn about automating API and Dev Portal management using Konnect APIs or Terraform. References:\nKonnect Dev Portal Overview Publish your API to Dev Portal API Products and Classic Dev Portal Developer Self-Service and App Registration All steps above are achievable via the Konnect UI as described in the official documentation.",
    "description": "Basic Developer Portal implementation In this section we are going to cover the basic steps to get a Developer Portal deployed\nKong Service and Route Create a Kong Service based on http://httpbin.konghq.com Create a Kong Route with path / Globally enable the CORS plugin to your Control Plane. That’s needed to solve the relationship between the Dev Portal and the Upstream Service. Dev Portal creation Choose the Dev Portal menu option and click + Create a new portal. Create a Private Portal named portal1. In the New Portal page accept the default values and click Save Click Go to overview. You should see the Overview page of your portal Check the Portal configuration: User authentication: Konnect built-in - That defines the default mechanism the Dev Portal uses for the user authentication. Besides the build-in option, it can be configured as OIDC or SAML. Default authentication strategy: key-auth - That defines the mechanism to control the API consumption inside the Dev Portal. Test your Dev Portal Sign Up If you click on the Dev Portal URL, you will play the Developer role and see your new portal home page. Since there’s no developer created, click Sing up and register to the Portal. Type your name and use a real email since the Dev Portal will send a confirmation request to it.",
    "tags": [],
    "title": "API Product",
    "uri": "/15-developer-portal/152-api-product/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "The Response Transformer plugin modifies the upstream response (e.g. response from the server) before returning it to the client.\nIn this section, you will configure the Response Transformer plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\ncat \u003e response-transformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: response-transformer-route paths: - /response-transformer-route plugins: - name: response-transformer instance_name: response-transformer1 config: add: headers: - demo:injected-by-kong EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT response-transformer.yaml Verify Test to make sure Kong transforms the request to the echo server and httpbin server.\ncurl --head $DATA_PLANE_URL/response-transformer-route/get HTTP/2 200 content-type: application/json content-length: 526 x-kong-request-id: 29f8371c1b1cc446119b7f5df69a6128 server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:40:54 GMT access-control-allow-origin: * access-control-allow-credentials: true demo: injected-by-kong x-kong-upstream-latency: 12 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition Expected Results Notice that demo: injected-by-kong is injected in the header.\nCleanup Reset the Control Plane to ensure that the plugins do not interfere with any other modules in the workshop for demo purposes and each workshop module code continues to function independently.\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f In real world scenario, you can enable as many plugins as you like depending on your use cases.\nKong-gratulations! have now reached the end of this module by configuring the Kong Route to include demo: injected-by-kong before responding to the client. You can now click Next to proceed with the next module.",
    "description": "The Response Transformer plugin modifies the upstream response (e.g. response from the server) before returning it to the client.\nIn this section, you will configure the Response Transformer plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\ncat \u003e response-transformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: response-transformer-route paths: - /response-transformer-route plugins: - name: response-transformer instance_name: response-transformer1 config: add: headers: - demo:injected-by-kong EOF Submit the declaration",
    "tags": [],
    "title": "Response Transformer",
    "uri": "/12-api-gateway/15-use-cases/153-response-transformer/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "The Request Callout plugin allows you to insert arbitrary API calls before proxying a request to the upstream service.\nIn this section, you will configure the Request Callout plugin on the Kong Route. Specifically, you will configure the plugin to do the following:\nCall Wikipedia using the “srseach” header as a parameter. The number of hits found and returned by Wikipeadia is added as a new header to the request. The request is sent to httpbin application which echoes the number of hits. Hit Wikipedia Just to get an idea what the Wikipedia response, send the following request:\ncurl -s \"https://en.wikipedia.org/w/api.php?srsearch=Miles%20Davis\u0026action=query\u0026list=search\u0026format=json\" | jq '.query.searchinfo.totalhits' You should get a number like 43812, which represents the number of total hits related to Miles Davis\nCreate the Request Callout Plugin Take the plugins declaration and enable the Request Callout plugin to the Route.\ncat \u003e request-callout.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: request-callout-route paths: - /request-callout-route plugins: - name: request-callout instance_name: request-callout1 config: callouts: - name: wikipedia request: url: https://en.wikipedia.org/w/api.php method: GET query: forward: true by_lua: local srsearch = kong.request.get_header(\"srsearch\"); local srsearch_encoded = ngx.escape_uri(srsearch) query = \"srsearch=\" .. srsearch_encoded .. \"\u0026action=query\u0026list=search\u0026format=json\"; kong.log.inspect(query); kong.ctx.shared.callouts.wikipedia.request.params.query = query response: body: decode: true by_lua: kong.service.request.add_header(\"wikipedia-total-hits-header\", kong.ctx.shared.callouts.wikipedia.response.body.query.searchinfo.totalhits) EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT request-callout.yaml Verify Send the request to Kong and check the response\ncurl -s \"$DATA_PLANE_URL/request-callout-route/get\" -H srsearch:\"Miles Davis\" | jq { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Content-Length\": \"0\", \"Host\": \"httpbin.kong.svc.cluster.local:8000\", \"Srsearch\": \"Miles Davis\", \"User-Agent\": \"curl/8.7.1\", \"Wipikedia-Total-Hits-Header\": \"43555\", \"X-Forwarded-Host\": \"127.0.0.1\", \"X-Forwarded-Path\": \"/request-callout-route/get\", \"X-Forwarded-Prefix\": \"/request-callout-route\", \"X-Kong-Request-Id\": \"6e4df528567f446630c6ae5c0b461c2e\" }, \"origin\": \"10.244.0.1\", \"url\": \"http://httpbin.kong.svc.cluster.local:8000/get\" } Expected Results Notice that new Wikipedia-Total-Hits-Header header is injected.\nCleanup Reset the Control Plane to ensure that the plugins do not interfere with any other modules in the workshop for demo purposes and each workshop module code continues to function independently.\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f In real world scenario, you can enable as many plugins as you like depending on your use cases.\nKong-gratulations! have now reached the end of this module. You can now click Next to proceed with the next module.",
    "description": "The Request Callout plugin allows you to insert arbitrary API calls before proxying a request to the upstream service.\nIn this section, you will configure the Request Callout plugin on the Kong Route. Specifically, you will configure the plugin to do the following:\nCall Wikipedia using the “srseach” header as a parameter. The number of hits found and returned by Wikipeadia is added as a new header to the request. The request is sent to httpbin application which echoes the number of hits. Hit Wikipedia Just to get an idea what the Wikipedia response, send the following request:",
    "tags": [],
    "title": "Request Callout",
    "uri": "/12-api-gateway/15-use-cases/154-request-callout/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Request Transformer and AI Response Transformer plugins integrate with the LLM on Amazon Bedrock, enabling introspection and transformation of the request’s body before proxying it to the Upstream Service and prior to forwarding the response to the client.\nThe plugins support llm/v1/chat style requests for all of the same providers as the AI Proxy plugin. It also use all of the same configuration and tuning parameters as the AI Proxy plugin, under the config.llm block.\nThe AI Request Transformer plugin runs before all of the AI Prompt plugins and the AI Proxy plugin, allowing it to also introspect LLM requests against the same, or a different, LLM. On the other hand, the AI Response Transformer plugin runs after the AI Proxy plugin, and after proxying to the Upstream Service, allowing it to also introspect LLM responses against the same, or a different, LLM service.\nThe diagram shows the journey of a consumer’s request through Kong Gateway to the backend service, where it is transformed by both an AI LLM service and Kong’s AI Request Transformer and the AI Response Transformer plugins.\nFor each plugin the configuration and usage processes are:\nThe Kong Gateway admin sets up an llm: configuration block, following the same configuration format as the AI Proxy plugin, and the same driver capabilities. The Kong Gateway admin sets up a prompt for the request introspection. The prompt becomes the system message in the LLM chat request, and prepares the LLM with transformation instructions for the incoming user request body (for the AI Request Transformer plugin) and for the returning upstream response body (for the AI Response Transformer plugin) The user makes an HTTP(S) call. Before proxying the user’s request to the backend, Kong Gateway sets the entire request body as the user message in the LLM chat request, and then sends it to the configured LLM service. After receiving the response from the backend, Kong Gateway sets the entire response body as the user message in the LLM chat request, then sends it to the configured LLM service. The LLM service returns a response assistant message, which is subsequently set as the upstream request body. The following example is going to apply the plugins to transform both request and reponse when consuming the httpbin Upstream Service.\nNow, configure both plugins. Keep in mind that the plugins are totally independent from each other so, the configuration depends on your use case.\ncat \u003e ai-request-response-tranformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /httpbin-route plugins: - name: ai-request-transformer instance_name: ai-request-transformer enabled: true config: prompt: In my JSON message, anywhere there is a JSON tag for a \"city\", also add a \"country\" tag with the name of the country in which the city resides. Return me only the JSON message, no extra text.\" llm: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-response-transformer instance_name: ai-response-transformer enabled: true config: prompt: For any city name, add its current temperature, in brackets next to it. Reply with the JSON result only. llm: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-request-response-tranformer.yaml curl -s -X POST \\ --url $DATA_PLANE_LB/httpbin-route/post \\ --header 'Content-Type: application/json' \\ --data '{ \"user\": { \"name\": \"Kong User\", \"city\": \"Tokyo\" } }' | jq Expected output { \"user\": { \"name\": \"Kong User\", \"city\": \"Tokyo [12°C]\", \"country\": \"Japan\" } } Kong-gratulations! have now reached the end of this module by using Kong Gateway to invoke a AWS Lambda function. You can now click Next to proceed with the next chapter.",
    "description": "The AI Request Transformer and AI Response Transformer plugins integrate with the LLM on Amazon Bedrock, enabling introspection and transformation of the request’s body before proxying it to the Upstream Service and prior to forwarding the response to the client.\nThe plugins support llm/v1/chat style requests for all of the same providers as the AI Proxy plugin. It also use all of the same configuration and tuning parameters as the AI Proxy plugin, under the config.llm block.",
    "tags": [],
    "title": "AI Request and Response Transfomers",
    "uri": "/16-ai-gateway/17-use-cases/155-request-response-transformer/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "Semantic caching enhances data retrieval efficiency by focusing on the meaning or context of queries rather than just exact matches. It stores responses based on the underlying intent and semantic similarities between different queries and can then retrieve those cached queries when a similar request is made.\nWhen a new request is made, the system can retrieve and reuse previously cached responses if they are contextually relevant, even if the phrasing is different. This method reduces redundant processing, speeds up response times, and ensures that answers are more relevant to the user’s intent, ultimately improving overall system performance and user experience.\nFor example, if a user asks, “how to integrate our API with a mobile app” and later asks, “what are the steps for connecting our API to a smartphone application?”, the system understands that both questions are asking for the same information. It can then retrieve and reuse previously cached responses, even if the wording is different. This approach reduces processing time and speeds up responses.\nThe AI Semantic Cache plugin may not be ideal for you if:\nIf you have limited hardware or budget. Storing semantic vectors and running similarity searches require a lot of storage and computing power, which could be an issue. If your data doesn’t rely on semantics, or exact matches work fine, semantic caching may offer little benefit. Traditional or keyword-based caching might be more efficient. How it works The diagram below illustrates the semantic caching mechanism implemented by the AI Semantic Cache plugin.\nThe process involves three parts: request handling, embedding generation, and response caching.\nFirst, a user starts a chat request with the LLM. The AI Semantic Cache plugin queries the vector database to see if there are any semantically similar requests that have already been cached. If there is a match, the vector database returns the cached response to the user. If there isn’t a match, the AI Semantic Cache plugin prompts the embeddings LLM to generate an embedding for the response. The AI Semantic Cache plugin uses a vector database and cache to store responses to requests. The plugin can then retrieve a cached response if a new request matches the semantics of a previous request, or it can tell the vector database to store a new response if there are no matches. With the AI Semantic Cache plugin, you can configure a cache of your choice to store the responses from the LLM. Currently, the plugin supports Redis as a cache.\nRedis as a Vector database We are going to configure the AI Semantic Cache to consume the Redis deployment available in the EKS Cluster. Redis, this time, will play the Vector database role.\nApply the Semantic Cache plugin cat \u003e ai-semantic-cache.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - semantic-cache - llm _konnect: control_plane_name: kong-workshop services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai enabled: true config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-semantic-cache instance_name: ai-semantic-cache-openai enabled: true config: embeddings: auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: \"text-embedding-3-small\" vectordb: dimensions: 1024 distance_metric: cosine strategy: redis threshold: 0.2 redis: host: \"redis-stack.redis.svc.cluster.local\" port: 6379 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-semantic-cache.yaml Check Redis Before sending request, you can scan the Redis database:\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan 1st Request Since we don’t have any cached data, the first request is going to return “Miss”:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected response HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-Cache-Status: Miss x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 Date: Tue, 12 Aug 2025 14:47:48 GMT x-ratelimit-remaining-tokens: 29993 access-control-expose-headers: X-Request-ID openai-organization: user-4qzstwunaw6d1dhwnga5bc5q openai-processing-ms: 7218 x-ratelimit-reset-requests: 120ms openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-cache-status: DYNAMIC openai-version: 2020-10-01 Server: cloudflare X-Content-Type-Options: nosniff x-envoy-upstream-service-time: 7420 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload CF-RAY: 96e0c4e97b364d3b-GRU x-ratelimit-limit-requests: 500 x-request-id: req_ae7f43291824451dbfec2a27b1a3ec2a x-ratelimit-reset-tokens: 14ms alt-svc: h3=\":443\"; ma=86400 X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2005 X-Kong-Upstream-Latency: 8820 X-Kong-Proxy-Latency: 876 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 8fd73d1623140f675ed93b0dcb4aeb16 { \"id\": \"chatcmpl-C3kZpdNpSx8eaIHhsLg14RhZgFBww\", \"object\": \"chat.completion\", \"created\": 1755010061, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (full name: James Marshall Hendrix, born November 27, 1942 – died September 18, 1970) was an American guitarist, singer, and songwriter, widely regarded as one of the most influential electric guitarists in the history of popular music. Emerging in the late 1960s, Hendrix revolutionized the way the guitar was played, using feedback, distortion, and an array of innovative techniques that transformed rock, blues, and psychedelic music.\\n\\nHendrix rose to fame with his band, **The Jimi Hendrix Experience**, delivering classic albums such as *Are You Experienced* (1967) and *Electric Ladyland* (1968). His groundbreaking performances included a legendary rendition of \\\"The Star-Spangled Banner\\\" at Woodstock in 1969.\\n\\nDespite his career only spanning about four years, Hendrix's influence endures through his recordings and his impact on generations of musicians. He was posthumously inducted into the Rock and Roll Hall of Fame in 1992. Some of his most famous songs include \\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"Voodoo Child (Slight Return),\\\" and \\\"All Along the Watchtower.\\\" Hendrix died at the age of 27, becoming one of the most iconic members of the so-called \\\"27 Club.\\\"\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 264, \"total_tokens\": 277, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_51e1070cf2\" } Check Redis again The Redis database has an entry now:\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan Expected response \"kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36\" 2nd Request The Semantic Cache plugin will use the cached data for similar requests:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Tell me more about Jimi Hendrix\" } ] }' Expected response HTTP/1.1 200 OK Date: Tue, 12 Aug 2025 14:48:55 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-Cache-Status: Hit Age: 67 X-Cache-Key: kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36 X-Cache-Ttl: 233 Content-Length: 1814 X-Kong-Response-Latency: 1438 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 2debcb5db5e6f3637bef912cca963a5d {\"object\":\"chat.completion\",\"created\":1755010061,\"id\":\"2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36\",\"usage\":{\"completion_tokens\":264,\"prompt_tokens_details\":{\"cached_tokens\":0,\"audio_tokens\":0},\"completion_tokens_details\":{\"accepted_prediction_tokens\":0,\"audio_tokens\":0,\"rejected_prediction_tokens\":0,\"reasoning_tokens\":0},\"total_tokens\":277,\"prompt_tokens\":13},\"model\":\"gpt-4.1-2025-04-14\",\"service_tier\":\"default\",\"system_fingerprint\":\"fp_51e1070cf2\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"annotations\":{},\"role\":\"assistant\",\"refusal\":null,\"content\":\"**Jimi Hendrix** (full name: James Marshall Hendrix, born November 27, 1942 – died September 18, 1970) was an American guitarist, singer, and songwriter, widely regarded as one of the most influential electric guitarists in the history of popular music. Emerging in the late 1960s, Hendrix revolutionized the way the guitar was played, using feedback, distortion, and an array of innovative techniques that transformed rock, blues, and psychedelic music.\\n\\nHendrix rose to fame with his band, **The Jimi Hendrix Experience**, delivering classic albums such as *Are You Experienced* (1967) and *Electric Ladyland* (1968). His groundbreaking performances included a legendary rendition of \\\"The Star-Spangled Banner\\\" at Woodstock in 1969.\\n\\nDespite his career only spanning about four years, Hendrix's influence endures through his recordings and his impact on generations of musicians. He was posthumously inducted into the Rock and Roll Hall of Fame in 1992. Some of his most famous songs include \\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"Voodoo Child (Slight Return),\\\" and \\\"All Along the Watchtower.\\\" Hendrix died at the age of 27, becoming one of the most iconic members of the so-called \\\"27 Club.\\\"\"}}]} 3rd Request As expected, for a non-related request, the AI Gateway will hit the LLM to satisfy the query:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who was Joseph Conrad?\" } ] }' Expected response HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-Cache-Status: Miss openai-version: 2020-10-01 x-envoy-upstream-service-time: 4746 Date: Tue, 12 Aug 2025 14:49:35 GMT x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 CF-RAY: 96e0c7a088ac1b20-GRU x-ratelimit-remaining-tokens: 29992 alt-svc: h3=\":443\"; ma=86400 access-control-expose-headers: X-Request-ID X-Content-Type-Options: nosniff Server: cloudflare openai-organization: user-4qzstwunaw6d1dhwnga5bc5q Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-request-id: req_10dee9e2989e46cbb32a2125f774f446 openai-processing-ms: 4658 cf-cache-status: DYNAMIC openai-project: proj_r4KYFyenuGWthS5te4zaurNN x-ratelimit-reset-tokens: 16ms x-ratelimit-reset-requests: 120ms X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2515 X-Kong-Upstream-Latency: 4996 X-Kong-Proxy-Latency: 2222 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: b5fadb69431d1234d8bbd53a71abc559 { \"id\": \"chatcmpl-C3kbbMudkoKV6rrzvOHz0lQ8IH0Ci\", \"object\": \"chat.completion\", \"created\": 1755010171, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Joseph Conrad** (born Józef Teodor Konrad Korzeniowski; 1857–1924) was a Polish-British writer widely regarded as one of the great novelists writing in English, despite the fact that English was not his first language. He was born in Berdychiv, in the Russian Empire (now in Ukraine), to Polish parents.\\n\\n**Background:**\\n- **Early Life:** Conrad’s parents were exiled for their involvement in Polish independence movements. Orphaned at a young age, he spent much of his youth in Poland and later France.\\n- **Seafaring Career:** In his twenties, Conrad became a merchant marine, traveling around the world and eventually settling in England. He gained British citizenship in 1886.\\n\\n**Literary Career:**\\n- He began writing novels and short stories in English, starting with *Almayer’s Folly* (1895).\\n- **Notable works** include:\\n - *Heart of Darkness* (1899)\\n - *Lord Jim* (1900)\\n - *Nostromo* (1904)\\n - *The Secret Agent* (1907)\\n- His novels often deal with themes of isolation, existential doubt, imperialism, and the complexity of human nature.\\n\\n**Legacy:**\\n- Conrad’s innovative narrative techniques and psychological depth influenced modernist literature and writers such as Virginia Woolf, T.S. Eliot, and William Faulkner.\\n- *Heart of Darkness*, a novella about a journey into the Congo, is considered one of the most important works of 20th-century literature and has inspired many adaptations, including the film *Apocalypse Now*.\\n\\n**Summary:** \\nJoseph Conrad was a Polish-born novelist who wrote in English and became one of the leading literary figures of his time, celebrated for his adventure tales, deep psychological insight, and exploration of moral ambiguity.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 12, \"completion_tokens\": 383, \"total_tokens\": 395, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_799e4ca3f1\" } Check Redis again Redis database has two entries now:\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan Expected response \"kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36\" \"kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:42aa94b4bbbedce497e59e1fd0fc617683a43b58ac7e306a47feb46f502f1499\" Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.",
    "description": "Semantic caching enhances data retrieval efficiency by focusing on the meaning or context of queries rather than just exact matches. It stores responses based on the underlying intent and semantic similarities between different queries and can then retrieve those cached queries when a similar request is made.\nWhen a new request is made, the system can retrieve and reuse previously cached responses if they are contextually relevant, even if the phrasing is different. This method reduces redundant processing, speeds up response times, and ensures that answers are more relevant to the user’s intent, ultimately improving overall system performance and user experience.",
    "tags": [],
    "title": "AI Semantic Cache",
    "uri": "/16-ai-gateway/17-use-cases/156-semantic-cache/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "In this section, you will configure the Key-Auth plugin on the Kong Route to protect Amazon Bedrock.\nAdd Kong Key Authentication plugin and Kong Consumer Add a KongPlugin resource for authentication, specifically the Key-Auth plugin. Note that, besides describing the plugin configuration, the declaration also creates a Kong Consumer, named user1, with an API Key (123456) as its credential.\ncat \u003e ai-key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth-bedrock enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-key-auth.yaml Verify authentication is required New requests now require authentication\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expect response The response is a HTTP/1.1 401 Unauthorized, meaning the Kong Gateway Service requires authentication.\nHTTP/1.1 401 Unauthorized Date: Tue, 12 Aug 2025 14:53:42 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Key Content-Length: 96 X-Kong-Response-Latency: 1 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: fd1bc16647271a20b7245b0cc9eb5052 { \"message\":\"No API key found in request\", \"request_id\":\"fd1bc16647271a20b7245b0cc9eb5052\" } Send another request with an API key Use the apikey to pass authentication to access the services.\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' The request should now respond with a HTTP/1.1 200 OK.\nWhen submitting requests, the API Key name is defined, by default, apikey. You can change the plugin configuration, if you will.",
    "description": "In this section, you will configure the Key-Auth plugin on the Kong Route to protect Amazon Bedrock.\nAdd Kong Key Authentication plugin and Kong Consumer Add a KongPlugin resource for authentication, specifically the Key-Auth plugin. Note that, besides describing the plugin configuration, the declaration also creates a Kong Consumer, named user1, with an API Key (123456) as its credential.\ncat \u003e ai-key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth-bedrock enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Key Auth",
    "uri": "/16-ai-gateway/17-use-cases/157-apikey/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "For advanced and enterprise class requirements, OpenID Connect (OIDC) is the preferred option to implement API consumer Authentication. OIDC also provides mechanisms to implement Authorization. In fact, when applying OIDC to secure the APIs, we’re delegating the Authentication process to the external Identity Provider entity.\nOpenID Connect is an authentication protocol built on top of OAuth 2.0 and JWT - JSON Web Token to add login and profile information about the identity who is logged in.\nOAuth 2.0 defines grant types for different use cases. The most common ones are:\nAuthorization Code: for apps running on a web server, browser-based and mobile apps for user authentication. Client Credentials: for application authentication. PKCE - Proof Key for Code Exchange: an extension to the Authorization Code grant. Recommended for SPA or native applications, PKCE acts like a non hard-coded secret. OpenId Connect plugin Konnect provides an OIDC plugin that fully supports the OAuth 2.0 grants. The plugin allows the integration with a 3rd party identity provider (IdP) in a standardized way. This plugin can be used to implement Kong as a (proxying) OAuth 2.0 resource server (RS) and/or as an OpenID Connect relying party (RP) between the client, and the upstream service.\nAs an example, here’s the typical topology and the Authorization Code with PKCE grant:\nConsumer sends a request to Kong Data Plane. Since the API is being protected with the OIDC plugin, the Data Plane redirects the consumer to the IdP. Consumer provides credentials to the Identity Provide (IdP). IdP authenticates the consumer enforcing security policies previously defined. The policies might involve several database technologies (e.g. LDAP, etc.), MFA (Multi-Factor Authentication), etc. After user authentication, IdP redirects the consumer back to the Data Plane with the Authorization Code injected inside the request. Data Plane sends a request to the IdP’s token endpoint with the Authorization Code and gets an Access Token from the IdP. Data Plane routes the request to the upstream service along with the Access Token Once again, it’s important to notice that one of the main benefits provided by an architecture like this is to follow the Separation of Concerns principle:\nIdentity Provider: responsible for User and Application Authentication, Tokenization, MFA, multiples User Databases abstraction, etc. API Gateway: responsible for exposing the Upstream Services and controlling their consumption through an extensive list of policies besides Authentication including Rate Limiting, Caching, Log Processing, etc. In this module, we will configure this plugin to use Kong Identity.\nYou can now click Next to proceed further.",
    "description": "For advanced and enterprise class requirements, OpenID Connect (OIDC) is the preferred option to implement API consumer Authentication. OIDC also provides mechanisms to implement Authorization. In fact, when applying OIDC to secure the APIs, we’re delegating the Authentication process to the external Identity Provider entity.\nOpenID Connect is an authentication protocol built on top of OAuth 2.0 and JWT - JSON Web Token to add login and profile information about the identity who is logged in.",
    "tags": [],
    "title": "OpenID Connect",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "With the existing API Key policy, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIn this section, we are going to define specific Rate Limiting policies for each Consumer represented by its API Key.\nKong Consumer Policies It’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:\nconsumer1: apikey = 123456 rate limiting policy = 500 tokens per minute consumer2: apikey = 987654 rate limiting policy = 10000 tokens per minute Doing that, the Data Plane is capable to not just protect the Route but to identify the consumer based on the key injected to enforce specific policies to the consumer. Keep in mind that a Consumer might have other plugins also enabled such as TCP Log, etc.\nNew Consumer and AI Rate Limiting Advanced plugin Policies Then, create the second consumer2, just like you did with the first one, with the 987654 key. Both Kong Consumers have the AI Rate Limiting Advanced plugin enabled with specific configurations.\ncat \u003e ai-key-auth-rate-limiting-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth-bedrock enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 plugins: - name: ai-rate-limiting-advanced instance_name: ai-rate-limiting-advanced-consumer1 config: llm_providers: - name: openai window_size: - 60 limit: - 500 - keyauth_credentials: - key: \"987654\" username: user2 plugins: - name: ai-rate-limiting-advanced instance_name: ai-rate-limiting-advanced-consumer2 config: llm_providers: - name: openai window_size: - 60 limit: - 10000 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-key-auth-rate-limiting-advanced.yaml Use both Kong Consumers If you will, you can inject both keys to your requests.\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected output HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 500 X-AI-RateLimit-Remaining-minute-openai: 500 openai-version: 2020-10-01 x-envoy-upstream-service-time: 8059 Date: Tue, 12 Aug 2025 15:26:01 GMT x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 CF-RAY: 96e0fce49c204ee9-GRU x-ratelimit-remaining-tokens: 29993 alt-svc: h3=\":443\"; ma=86400 access-control-expose-headers: X-Request-ID X-Content-Type-Options: nosniff Server: cloudflare openai-organization: user-4qzstwunaw6d1dhwnga5bc5q Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-request-id: req_f230ff10e7374c7a8b2fc292a3cc0685 openai-processing-ms: 7964 cf-cache-status: DYNAMIC openai-project: proj_r4KYFyenuGWthS5te4zaurNN x-ratelimit-reset-tokens: 14ms x-ratelimit-reset-requests: 120ms X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2058 X-Kong-Upstream-Latency: 8798 X-Kong-Proxy-Latency: 4 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 379bd531f3411379f613083969a88c07 { \"id\": \"chatcmpl-C3lAneT4SCgIa50fe89CYPadtBdfQ\", \"object\": \"chat.completion\", \"created\": 1755012353, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Jimi Hendrix (born **James Marshall Hendrix**, November 27, 1942 – September 18, 1970) was an American guitarist, singer, and songwriter. He is widely regarded as one of the greatest and most influential electric guitarists in the history of popular music.\\n\\n**Career Highlights:**\\n- Hendrix gained fame in the late 1960s with his band, **The Jimi Hendrix Experience**.\\n- Some of his most famous songs include **\\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"The Wind Cries Mary,\\\"** and **\\\"All Along the Watchtower.\\\"**\\n- He was known for his innovative guitar techniques, including feedback, distortion, and wah-wah, and his energetic and theatrical performance style.\\n- Hendrix's legendary performances include his rendition of \\\"The Star-Spangled Banner\\\" at **Woodstock** in 1969.\\n\\n**Legacy:**\\n- Despite his short career (he died at age 27), Hendrix's music and style had a major impact on rock, blues, and modern guitar playing.\\n- He was posthumously inducted into the **Rock and Roll Hall of Fame** in 1992.\\n- Hendrix is consistently ranked among the greatest guitarists of all time by music publications and critics.\\n\\n**Fun Fact:** \\nJimi Hendrix is part of the so-called \\\"27 Club,\\\" a group of influential musicians who died at the age of 27.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 285, \"total_tokens\": 298, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_b3f1157249\" } or\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 987654' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected output HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 10000 X-AI-RateLimit-Remaining-minute-openai: 10000 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 Date: Tue, 12 Aug 2025 15:26:41 GMT x-ratelimit-remaining-tokens: 29993 access-control-expose-headers: X-Request-ID openai-organization: user-4qzstwunaw6d1dhwnga5bc5q openai-processing-ms: 6376 x-ratelimit-reset-requests: 120ms openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-cache-status: DYNAMIC openai-version: 2020-10-01 Server: cloudflare X-Content-Type-Options: nosniff x-envoy-upstream-service-time: 6463 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload CF-RAY: 96e0fdeec8b7ae57-GRU x-ratelimit-limit-requests: 500 x-request-id: req_f8c4559cbdac4d1bbff938b61a054f3d x-ratelimit-reset-tokens: 14ms alt-svc: h3=\":443\"; ma=86400 X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2324 X-Kong-Upstream-Latency: 6709 X-Kong-Proxy-Latency: 3 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 6e2daa0a80e95772fa46e15637278177 { \"id\": \"chatcmpl-C3lBT2cDacuuICQzugJ5CCMbWtCFV\", \"object\": \"chat.completion\", \"created\": 1755012395, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (born **James Marshall Hendrix**, November 27, 1942 – September 18, 1970) was an American guitarist, singer, and songwriter. Widely regarded as one of the greatest and most influential electric guitarists in the history of popular music, Hendrix is known for his innovative playing style, including groundbreaking use of guitar effects and techniques like feedback and distortion.\\n\\n### Brief Biography:\\n- **Early Life:** Born in Seattle, Washington; started playing guitar as a teenager.\\n- **Career:** Gained prominence in the mid-1960s after moving to England and forming the **Jimi Hendrix Experience** with bassist Noel Redding and drummer Mitch Mitchell.\\n- **Famous Albums:** \\n - *Are You Experienced* (1967)\\n - *Axis: Bold as Love* (1967)\\n - *Electric Ladyland* (1968)\\n- **Iconic Performances:** \\n - Monterey Pop Festival (1967)\\n - Woodstock Festival (1969), where he famously reinterpreted “The Star-Spangled Banner.”\\n\\n### Legacy:\\nHendrix’s innovative approach fused rock, blues, and psychedelia. His use of the wah-wah pedal, feedback, and studio effects transformed notions of what the electric guitar could do. Despite his death at the age of 27, his influence persists across genres and generations.\\n\\n### Key Songs:\\n- “Purple Haze”\\n- “Hey Joe”\\n- “All Along the Watchtower”\\n- “Voodoo Child (Slight Return)”\\n- “Little Wing”\\n\\nHendrix is a major figure in rock history and is frequently cited in “greatest guitarists of all time” lists.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 346, \"total_tokens\": 359, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_799e4ca3f1\" } Again, test the rate-limiting policy by executing the following command multiple times and observe the rate-limit headers in the response, specially, X-AI-RateLimit-Limit-minute-openai and X-AI-RateLimit-Remaining-minute-openai:\nNow, let’s consume it with the Consumer1’s API Key. As you can see the Data Plane is processing the Rate Limiting processes independently.\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 500 X-AI-RateLimit-Remaining-minute-openai: 110 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 Date: Tue, 12 Aug 2025 15:29:05 GMT x-ratelimit-remaining-tokens: 29993 access-control-expose-headers: X-Request-ID openai-organization: user-4qzstwunaw6d1dhwnga5bc5q openai-processing-ms: 4385 x-ratelimit-reset-requests: 120ms openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-cache-status: DYNAMIC openai-version: 2020-10-01 Server: cloudflare X-Content-Type-Options: nosniff x-envoy-upstream-service-time: 4484 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload CF-RAY: 96e10178faf000f1-GRU x-ratelimit-limit-requests: 500 x-request-id: req_b4611580cac4476288895c6315847b8b x-ratelimit-reset-tokens: 14ms alt-svc: h3=\":443\"; ma=86400 X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 1796 X-Kong-Upstream-Latency: 4881 X-Kong-Proxy-Latency: 1 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 8b4285448ad3b335f766d33c61a37851 If we keep sending requests using the first API Key, eventually, as expected, we’ll get an error code:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/1.1 429 Too Many Requests Date: Tue, 12 Aug 2025 15:29:52 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 500 X-AI-RateLimit-Remaining-minute-openai: 0 X-AI-RateLimit-Retry-After-minute-openai: 9 X-AI-RateLimit-Reset: 9 X-AI-RateLimit-Retry-After: 9 X-AI-RateLimit-Reset-minute-openai: 9 Content-Length: 66 X-Kong-Response-Latency: 1 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: c3a2a05b8f77b264230f041c527ade71 {\"message\":\"AI token rate limit exceeded for provider(s): openai\"} However, the second API Key is still allowed to consume the Kong Route:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 987654' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 10000 X-AI-RateLimit-Remaining-minute-openai: 10000 openai-version: 2020-10-01 x-envoy-upstream-service-time: 5053 Date: Tue, 12 Aug 2025 15:30:26 GMT x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 CF-RAY: 96e103739ca64292-VCP x-ratelimit-remaining-tokens: 29993 alt-svc: h3=\":443\"; ma=86400 access-control-expose-headers: X-Request-ID X-Content-Type-Options: nosniff Server: cloudflare openai-organization: user-4qzstwunaw6d1dhwnga5bc5q Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-request-id: req_379758f4fcd549acb57c7e6d911b5a89 openai-processing-ms: 5014 cf-cache-status: DYNAMIC openai-project: proj_r4KYFyenuGWthS5te4zaurNN x-ratelimit-reset-tokens: 14ms x-ratelimit-reset-requests: 120ms X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 1998 X-Kong-Upstream-Latency: 5602 X-Kong-Proxy-Latency: 2 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 118caf09b62c8f4f78771c182f736b00 { \"id\": \"chatcmpl-C3lF7AC6zeB8NOBFQn5Xeacn6Ntf7\", \"object\": \"chat.completion\", \"created\": 1755012621, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (born James Marshall Hendrix on November 27, 1942 – September 18, 1970) was an iconic American guitarist, singer, and songwriter. Widely regarded as one of the most influential electric guitarists in the history of popular music, Hendrix is celebrated for his innovative style, virtuosic playing, and groundbreaking use of guitar effects such as distortion, feedback, and wah-wah pedals.\\n\\nHendrix first gained fame in the mid-1960s after moving to London and forming the **Jimi Hendrix Experience** with bassist Noel Redding and drummer Mitch Mitchell. The band released classic albums such as *Are You Experienced* (1967), *Axis: Bold as Love* (1967), and *Electric Ladyland* (1968), featuring hit songs like \\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"The Wind Cries Mary,\\\" \\\"Voodoo Child (Slight Return),\\\" and his legendary rendition of \\\"The Star-Spangled Banner\\\" performed at Woodstock in 1969.\\n\\nTragically, Hendrix died at the young age of 27. Despite his short career, his influence continues to shape rock, blues, and popular music to this day. He is consistently ranked among the greatest guitarists of all time and has been inducted into the Rock and Roll Hall of Fame.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 271, \"total_tokens\": 284, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_51e1070cf2\" } Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.",
    "description": "With the existing API Key policy, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIn this section, we are going to define specific Rate Limiting policies for each Consumer represented by its API Key.\nKong Consumer Policies It’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:",
    "tags": [],
    "title": "AI Rate Limiting Advanced",
    "uri": "/16-ai-gateway/17-use-cases/158-rate-limiting/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.\nThe plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nLoad balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:\nLowest-usage Round-robin (weighted) Consistent-hashing (sticky-session on given header value) Semantic routing The AI Proxy Advanced plugin supports semantic routing, which enables distribution of requests based on the similarity between the prompt and the description of each model. This allows Kong to automatically select the model that is best suited for the given domain or use case.\nBy analyzing the content of the request, the plugin can match it to the most appropriate model that is known to perform better in similar contexts. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.\nAs a illustration here is the architecture where we are going to implement the multiple load balancing policies. AI Proxy Advanced will manage both LLMs:\ngpt-4.1 llama3.2:1b You can now click Next to proceed further.",
    "description": "The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.\nThe plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nLoad balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:",
    "tags": [],
    "title": "AI Proxy Advanced",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway",
    "content": "With the rapid emergence of multiple AI LLM providers, the AI technology landscape is fragmented and lacking in standards and controls. Kong AI Gateway is a powerful set of features built on top of Kong Gateway, designed to help developers and organizations effectively adopt AI capabilities quickly and securely\nWhile AI providers don’t conform to a standard API specification, the Kong AI Gateway provides a normalized API layer allowing clients to consume multiple AI services from the same client code base. The AI Gateway provides additional capabilities for credential management, AI usage observability, governance, and tuning through prompt engineering. Developers can use no-code AI Plugins to enrich existing API traffic, easily enhancing their existing application functionality.\nYou can enable the AI Gateway features through a set of specialized plugins, using the same model you use for any other Kong Gateway plugin.\nKong AI Gateway functional scope Universal API Kong’s AI Gateway Universal API, delivered through the AI Proxy and AI Proxy Advanced plugins, simplifies AI model integration by providing a single, standardized interface for interacting with models across multiple providers.\nEasy to use: Configure once and access any AI model with minimal integration effort.\nLoad balancing: Automatically distribute AI requests across multiple models or providers for optimal performance and cost efficiency.\nRetry and fallback: Optimize AI requests based on model performance, cost, or other factors.\nCross-plugin integration: Leverage AI in non-AI API workflows through other Kong Gateway plugins.\nHigh Level Tasks You will complete the following:\nSet up Kong AI Proxy for LLM Integration Implement Kong AI Plugins to secure prompt message You can now click Next to proceed further.",
    "description": "With the rapid emergence of multiple AI LLM providers, the AI technology landscape is fragmented and lacking in standards and controls. Kong AI Gateway is a powerful set of features built on top of Kong Gateway, designed to help developers and organizations effectively adopt AI capabilities quickly and securely\nWhile AI providers don’t conform to a standard API specification, the Kong AI Gateway provides a normalized API layer allowing clients to consume multiple AI services from the same client code base. The AI Gateway provides additional capabilities for credential management, AI usage observability, governance, and tuning through prompt engineering. Developers can use no-code AI Plugins to enrich existing API traffic, easily enhancing their existing application functionality.",
    "tags": [],
    "title": "Introduction",
    "uri": "/16-ai-gateway/159-ai-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "In this second part of the workshop we are going to explore the AI capabilities provides by Kong AI Gateway and the specific collection of plugins.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Kong AI Gateway",
    "description": "In this second part of the workshop we are going to explore the AI capabilities provides by Kong AI Gateway and the specific collection of plugins.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Kong AI Gateway",
    "tags": [],
    "title": "Kong AI Gateway",
    "uri": "/16-ai-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "What is RAG? RAG offers a solution to some of the limitations in traditional generative AI models, such as accuracy and hallucinations, allowing companies to create more contextually relevant AI applications.\nBasically, the RAG application comprises two main processes:\nData Preparation: External data sources, including all sorts of documents, are chunked and submitted to an Embedding Model which converts them into embeddings. The embeddings are stored in a Vector Database.\nQuery time: A consumer wants to send a query to the actual Prompt/Chat LLM model. However, the query should be enhanced with relevant data, taken from the Vector Database, and not available in the LLM model. The following steps are then performed:\nThe consumer builds a Prompt. The RAG application converts the Prompt into an embedding, calling the Embedding Model. Leveraging Semantic Search, RAG matches the Prompt Embedding with the most relevant information and retrieves that Vector Database. The Vector Database returns relevant data as a response to the search query. The RAG application sends a query to the Prompt/Chat LLM Model combining the Prompt with the Relevant Data returned by the Vector Database. The LLM Model returns a response. Implementation Architecture Data Preparation time During the preparation time, the following steps are executed:\nThe Document Loader script asks the AI RAG Injector plugin to provides the configuration regarding both Embedding Model and Vector Database. The Document Loader sends data chunks to the Embedding Model to gets the embeddings related to them The Document Loader stores the embeddings and content into the Vector Database. RAG time The following steps are performed during the execution time:\nThe API Consumer send a request with a prompt. The AI RAG Injector Plugin converts the prompt into embeddings calling the Embedding Model. The AI RAG Injector Plugin sends a KNN vector search query to the Vector Database to find the top “k-nearest neighbors” to a query vector. The AI Proxy Advanced Plugin sends a regular request to the LLM model adding the relevant data received from the Vector Database. Here’s the declaration including both plugins: AI RAG Injector and AI Proxy Advanced.\ncat \u003e ai-rag-injector.yaml \u003c\u003c 'EOF' _format_version: '3.0' _konnect: control_plane_name: kong-workshop services: - name: ai-proxy url: http://localhost:65535 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - logging: log_statistics: true route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-5 options: temperature: 1.0 - name: ai-rag-injector id: 3194f12e-60c9-4cb6-9cbc-c8fd7a00cff1 instance_name: ai-rag-injector1 config: inject_template: | Only use the following information surrounded by \u003cRAG\u003e\u003c/RAG\u003e to and your existing knowledge to provide the best possible answer to the user. \u003cRAG\u003e\u003cCONTEXT\u003e\u003c/RAG\u003e User's question: \u003cPROMPT\u003e fetch_chunks_count: 1 embeddings: auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: \"text-embedding-3-small\" vectordb: strategy: redis redis: host: redis-stack.redis port: 6379 distance_metric: cosine dimensions: 1024 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-rag-injector.yaml If you send a request with no context, we’ll see the LLM is not able to respond to it:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header \"Content-Type: application/json\" \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"What did Marco say about AI Gateways?\" } ] }' | jq '.choices[].message.content' Typical response: \"Could you clarify which Marco you mean and where he said it (e.g., a podcast, post, talk, or email)? If you can share a link, quote, or more context, I can find and summarize exactly what he said about AI Gateways.\" We are going to inject some context using an interview transcript snippet Marco Palladino, Kong’s co-founder and CTO, gave some time ago.\nThe transcript snippet is available in the following file.\ncurl 'https://raw.githubusercontent.com/Kong/workshop-kong-api-ai-gateway/refs/heads/main/content/static/code/SED1683-Kong-short.txt' --output ./SED1683-Kong-short.txt We have to copy the Document Loader script, algo available in a file, to the Data Plane:\ncurl 'https://raw.githubusercontent.com/Kong/workshop-kong-api-ai-gateway/refs/heads/main/content/static/code/ingest_update.lua' --output ./ingest_update.lua kubectl cp ./ingest_update.lua -n kong $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name'):/tmp/ingest_update.lua Now, execute the Document Loader passing the content as a parameter. Note that, to make to process a bit easier, we have created the AI RAG Injector plugin with a pre-defined id.\nkubectl exec -ti $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name') -n kong -- kong runner /tmp/ingest_update.lua 3194f12e-60c9-4cb6-9cbc-c8fd7a00cff1 \"'\"$(cat ./SED1683-Kong-short.txt)\"'\" You should be able to get a much better response from the LLM model this time:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header \"Content-Type: application/json\" \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"What did Marco say about AI Gateways?\" } ] }' | jq '.choices[].message.content' \"Marco said that for AI use cases you need an AI gateway to connect multiple LLMs and orchestrate them. He added that Kong can deploy its gateway in this AI gateway role (alongside edge gateway and service mesh) under a unified control plane, so you can see, manage, monitor, consume, and expose APIs consistently across these use cases.\" If you Redis, you’ll se there a new entry for RAG Injector\n% kubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan \"kong_rag_injector:7e9d1404-5cfb-4585-bada-132e6d6595c1\"",
    "description": "What is RAG? RAG offers a solution to some of the limitations in traditional generative AI models, such as accuracy and hallucinations, allowing companies to create more contextually relevant AI applications.\nBasically, the RAG application comprises two main processes:\nData Preparation: External data sources, including all sorts of documents, are chunked and submitted to an Embedding Model which converts them into embeddings. The embeddings are stored in a Vector Database.",
    "tags": [],
    "title": "RAG - Retrieval-Augmented Generation",
    "uri": "/16-ai-gateway/17-use-cases/170-rag/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway",
    "content": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nSimple AI Gateway Proxy Prompt Engineering AI Request and Response Tranformers AI Semantic Cache AI Rate Limiting AI Proxy Advanced and load balancing algoritms RAG These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong AI plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "description": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nSimple AI Gateway Proxy Prompt Engineering AI Request and Response Tranformers AI Semantic Cache AI Rate Limiting AI Proxy Advanced and load balancing algoritms RAG These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong AI plugins on the Plugin Hub.",
    "tags": [],
    "title": "Use Cases",
    "uri": "/16-ai-gateway/17-use-cases/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "Basically Konnect provides two main Builtin Observability services:\nKonnect Advanced Analytics: it is a real-time, contextual analytics platform that provides insights into API health, performance, and usage.\nKonnect Debugger: it provides a connected debugging experience and real-time trace-level visibility into API traffic.",
    "description": "Basically Konnect provides two main Builtin Observability services:\nKonnect Advanced Analytics: it is a real-time, contextual analytics platform that provides insights into API health, performance, and usage.\nKonnect Debugger: it provides a connected debugging experience and real-time trace-level visibility into API traffic.",
    "tags": [],
    "title": "Konnect Builtin Observability",
    "uri": "/20-observability/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Builtin Observability",
    "content": "Konnect Advanced Analytics is a real-time, highly contextual analytics platform that provides deep insights into API health, performance, and usage. It helps businesses optimize their API strategies and improve operational efficiency. This feature is offered as a premium service within Konnect.\nKey benefits:\nCentralized visibility: Gain insights across all APIs, Services, and data planes. Contextual API analytics: Analyze API requests, Routes, Consumers, and Services. Democratized data insights: Generate reports based on your needs. Fast time to insight: Retrieve critical API metrics in less than a second. Reduced cost of ownership: A turn-key analytics solution without third-party dependencies. Enabling data ingestion Manage data ingestion from any Control Plane Dashboard using the Advanced Analytics toggle. This toggle lets you enable or disable data collection for your API traffic per control plane.\nModes:\nOn: Both basic and advanced analytics data is collected, allowing in-depth insights and reporting. Off: Advanced analytics collection stops, but basic API metrics remain available in Gateway Manager, and can still be used for custom reports. Explorer Interface The Explorer interface displays API usage data gathered by Konnect Analytics from your Data Plane nodes. You can use this tool to:\nDiagnose performance issues Monitor LLM token consumption and costs Capture essential usage metrics The Analytics Explorer also lets you save the output as a custom report.\nCheck the Advanced Analytics Explorer documentation to learn more.\nDashboards The Summary Dashboard shows performance and health statistics of all your APIs across your organization on a single page and provides insights into your Service usage.\nCustom Dasboards Advanced Analytics includes the ability to build organization-specific views with Custom Dashboards. You can create them from scratch or use existing templates. The functionality is powered by a robust API, and Terraform integration.\nCreate a dashboard You can create custom dashboards either from scratch or from a template. In this tutorial, we’ll use a template.\nTo create a custom dashboard, do the following:\nIn Konnect, navigate to Dashboards in the sidebar. From the Create dashboard dropdown menu, select “Create from template”. Click Quick summary dashboard. Click Use template. This creates a new template with pre-configured tiles. Add a filter Filters help you narrow down the data shown in charts without modifying individual tiles.\nFor this example, let’s add a filter so that the data shown in the dashboard is scoped to only one control plane:\nFrom the dashboard, click Add filter. This brings up the configuration options. Select “Control plane” from the Filter by dropdown menu. Select “In” from the Operator dropdown menu. Select “kong-workshop from the Filter value dropdown menu. Select the Make this a preset for all viewers checkbox. Click Apply. This applies the filter to the dashboard. Anyone that views this dashboard will be viewing it scoped to the filter you created.\nCheck the Advanced Analytics Custom Dashboards documentation to learn more.\nRequests The Requests options shows all requests that have been processed by the Data Planes. For example, here’s the requests processed by the Data Planes created for the kong-workshop Control Plane.",
    "description": "Konnect Advanced Analytics is a real-time, highly contextual analytics platform that provides deep insights into API health, performance, and usage. It helps businesses optimize their API strategies and improve operational efficiency. This feature is offered as a premium service within Konnect.\nKey benefits:\nCentralized visibility: Gain insights across all APIs, Services, and data planes. Contextual API analytics: Analyze API requests, Routes, Consumers, and Services. Democratized data insights: Generate reports based on your needs. Fast time to insight: Retrieve critical API metrics in less than a second. Reduced cost of ownership: A turn-key analytics solution without third-party dependencies. Enabling data ingestion Manage data ingestion from any Control Plane Dashboard using the Advanced Analytics toggle. This toggle lets you enable or disable data collection for your API traffic per control plane.",
    "tags": [],
    "title": "Konnect Advanced Analytics",
    "uri": "/20-observability/21-builtin/211-advanced_analytics/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Builtin Observability",
    "content": "Konnect Debugger provides a connected debugging experience and real-time trace-level visibility into API traffic, enabling you to:\nTroubleshoot issues: Investigate and resolve problems during deployments or incidents with targeted, on-demand traces. Understand request lifecycle: Visualize exactly what happened during a specific request, including order and duration plugin execution. See Debugger spans for a list of spans captured. Improve performance and reliability: Use deep insights to fine-tune configurations and resolve bottlenecks. Capture traces and logs Konnect Debugger allows you to capture traces and logs.\nTraces Traces provide a visual representation of the request and response lifecycle, offering a comprehensive overview of Kong’s request processing pipeline.\nThe debugger helps capture OpenTelemetry-compatible traces for all requests matching the sampling criteria. The detailed spans are captured for the entire request/response lifecycle. These traces can be visualized with Konnect’s built-in span viewer with no additional instrumentation or telemetry tools. For a complete list of available spans and their meanings, see Debugger spans.\nKey Highlights Traces can be generated for a service or per route Refined traces can be generated for all requests matching a sampling criteria Sampling criteria can be defined with simple expressions language, for example: http.method == GET Trace sessions are retained for up to 7 days Traces can be visualized in Konnect’s built in trace viewer To ensure consistency and interoperability, tracing adheres to OpenTelemetry naming conventions for spans and attributes, wherever possible.\nLogs For deeper insights, logs can be captured along with traces. When initiating a debug session, administrators can choose to capture logs. Detailed Kong Gateway logs are captured for the duration of the session. These logs are then correlated with traces using trace_id and span_id providing a comprehensive and drill-down view of logs generated during specific trace or span.\nReading traces and logs Traces captured during a debug session can be visualized in debugger’s built-in trace viewer. The trace viewer displays Summary, Spans and Logs view. You can gain instant insights with the summary view while the spans and logs view help you to dive deeper.\nSummary view Summary view helps you visualize the entire API request-response flow in a single glance. This view provides a concise overview of critical latency metrics and a transaction map. The lifecycle map includes the different phases of Kong Gateway and the plugins executed by Kong Gateway on both the request and the response along with the times spent in each phase. Use the summary view to quickly understand the end-to-end API flow, identify performance bottlenecks, and optimize your API strategy.\nSpans view The span view gives you unparalleled visibility into Kong Gateway’s internal workings. This detailed view breaks down into individual spans, providing a comprehensive understanding of:\nKong Gateway’s internal processes and phases Plugin execution and performance Request and response handling For detailed definitions of each span, see Debugger spans. Use the span view to troubleshoot issues, optimize performance, and refine your configuration.\nLogs View A drill-down view of all the logs generated during specific debug session are shown in the logs tab. All the spans in the trace are correlated using trace_id and span_id. The logs can be filtered on log level and spans. Logs are displayed in reverse chronological order. Konnect encrypts all the logs that are ingested. You can further ensure complete privacy and control by using customer-managed encryption keys (CMEK). Use the logs view to quickly troubleshoot and pinpoint issues.\nData Security with Customer-Managed Encryption Keys (CMEK) By default, logs are automatically encrypted using encryption keys that are owned and managed by Konnect. However if you have a specific compliance and regulatory requirements related to the keys that protect your data, you can use the customer-managed encryption keys. This ensures that sensitive data are secured for each organization with their own key and nobody, including Konnect, has access to that data. For more information about how to create and manage CMEK keys, see Customer-Managed Encryption Keys (CMEK).\nStart your first debug session To begin using the Debugger, ensure the following requirements are met:\nYour data plane nodes are running Kong Gateway version 3.9.1 or later. Logs require Kong Gateway version 3.11.0 or later. Your Konnect data planes are hosted using self-managed hybrid, Dedicated Cloud Gateways, or serverless gateways. Kong Ingress Controller or Kong Native Event Proxy Gateways aren’t currently supported. In Gateway Manager, select the control plane that contains the data plane to be traced. In the left navigation menu, click Debugger. Click New session. Define the sampling criteria and click Start Session. Once the session starts, traces will be captured for requests that match the rule. Click a trace to view it in the span viewer.\nEach session can be configured to run for a time between 10 seconds and 30 minutes. Sessions are retained for up to 7 days.\nFor details on defining sampling rules, see Debugger sessions.\nSampling rules Sampling rules help you capture only relevant traffic. Requests that match the defined criteria are included in the session. There are two types:\nBasic sampling rules: Filter by Route or Service. Advanced sampling rules: Use expressions for fine-grained filtering.",
    "description": "Konnect Debugger provides a connected debugging experience and real-time trace-level visibility into API traffic, enabling you to:\nTroubleshoot issues: Investigate and resolve problems during deployments or incidents with targeted, on-demand traces. Understand request lifecycle: Visualize exactly what happened during a specific request, including order and duration plugin execution. See Debugger spans for a list of spans captured. Improve performance and reliability: Use deep insights to fine-tune configurations and resolve bottlenecks. Capture traces and logs Konnect Debugger allows you to capture traces and logs.",
    "tags": [],
    "title": "Konnect Debugger",
    "uri": "/20-observability/21-builtin/212-debugger/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases \u003e OpenID Connect",
    "content": "Kong Identity Kong Identity enables you to use Konnect to generate, authenticate, and authorize API access. You can use Kong Identity to:\nCreate authorization servers per region Issue and validate access tokens Integrate secure authentication into your Kong Gateway APIs Create a Kong Service and Route cat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /httpbin-route EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT httpbin.yaml Client Credentials Grant This next section describe the OAuth Client Credentials grants implemented by Kong Konnect and Kong Identity as the Identity Provider. Let’s start instantiating an Authentication Service in Kong Identity.\nCreate the Authentication Server in Kong Identity Before you can configure any authentication plugin, you must first create an auth server in Kong Identity. The auth server name is unique per each organization and each Konnect region.\nCreate an auth server using the /v1/auth-servers endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"AuthN_Server_1\", \"audience\": \"http://myhttpbin.dev\", \"description\": \"AuthN Server 1\" }' | jq You should get a response like this:\n{ \"audience\": \"http://myhttpbin.dev\", \"created_at\": \"2025-09-23T13:04:47.789958Z\", \"description\": \"AuthN Server 1\", \"id\": \"836fda4d-612c-4faf-9c45-284a0ecd637a\", \"issuer\": \"https://wt3fgfqb8r7fktwe.us.identity.konghq.com/auth\", \"labels\": {}, \"metadata_uri\": \"https://wt3fgfqb8r7fktwe.us.identity.konghq.com/auth/.well-known/openid-configuration\", \"name\": \"AuthN_Server_1\", \"signing_algorithm\": \"RS256\", \"updated_at\": \"2025-09-23T13:04:47.789958Z\" } Check your AuthN Server curl -sX GET \"https://us.api.konghq.com/v1/auth-servers\" \\ -H \"Authorization: Bearer $PAT\" | jq Get the AuthN Server Id:\nexport AUTHN_SERVER_ID=$(curl -sX GET \"https://us.api.konghq.com/v1/auth-servers\" -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].id') Get the Issuer URL:\nexport ISSUER_URL=$(curl -sX GET \"https://us.api.konghq.com/v1/auth-servers\" -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].issuer') Configure the auth server with scopes Configure a scope in your auth server using the /v1/auth-servers/$AUTHN_SERVER_ID/scopes endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/scopes\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"scope1\", \"description\": \"scope1\", \"default\": false, \"include_in_metadata\": false, \"enabled\": true }' | jq Expected response\n{ \"created_at\": \"2025-09-23T13:06:24.252827Z\", \"default\": false, \"description\": \"scope1\", \"enabled\": true, \"id\": \"b71c1192-6416-4933-b913-5358904dd234\", \"include_in_metadata\": false, \"name\": \"scope1\", \"updated_at\": \"2025-09-23T13:06:24.252827Z\" } Export your scope ID:\nexport SCOPE_ID=$(curl -sX GET \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/scopes\" -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].id') Configure the auth server with custom claims Configure a custom claim using the /v1/auth-servers/$AUTHN_SERVER_ID/claims endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/claims\" \\ -H \"Authorization: Bearer $PAT\" \\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"claim1\", \"value\": \"claim1\", \"include_in_token\": true, \"include_in_all_scopes\": false, \"include_in_scopes\": [ \"'$SCOPE_ID'\" ], \"enabled\": true }' | jq Expected output:\n{ \"created_at\": \"2025-09-23T13:06:56.096243Z\", \"enabled\": true, \"id\": \"9b149436-ce85-4fb9-9105-b887546e7b21\", \"include_in_all_scopes\": false, \"include_in_scopes\": [ \"b71c1192-6416-4933-b913-5358904dd234\" ], \"include_in_token\": true, \"name\": \"claim1\", \"updated_at\": \"2025-09-23T13:06:56.096243Z\", \"value\": \"claim1\" } Create a client in the AuthN Server The client is the machine-to-machine credential. In this tutorial, Konnect will autogenerate the client ID and secret, but you can alternatively specify one yourself.\nConfigure the client using the /v1/auth-servers/$AUTHN_SERVER_ID/clients endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/clients\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"client1\", \"grant_types\": [ \"client_credentials\" ], \"allow_all_scopes\": false, \"allow_scopes\": [ \"'$SCOPE_ID'\" ], \"access_token_duration\": 3600, \"id_token_duration\": 3600, \"response_types\": [ \"id_token\", \"token\" ] }' | jq Expected output:\n{ \"access_token_duration\": 3600, \"allow_all_scopes\": false, \"allow_scopes\": [ \"b71c1192-6416-4933-b913-5358904dd234\" ], \"client_secret\": \"8vbywkjyj1zxcgsujljnuge1\", \"created_at\": \"2025-09-23T13:07:23.691662Z\", \"grant_types\": [ \"client_credentials\" ], \"id\": \"fxsn74prsnrcyskm\", \"id_token_duration\": 3600, \"labels\": {}, \"login_uri\": null, \"name\": \"client1\", \"redirect_uris\": [], \"response_types\": [ \"id_token\", \"token\" ], \"token_endpoint_auth_method\": \"client_secret_post\", \"updated_at\": \"2025-09-23T13:07:23.691662Z\" } The Client Secret will not be shown again, so copy both ID and Secret:\nexport CLIENT_ID=\u003cYOUR_CLIENT_ID\u003e export CLIENT_SECRET=\u003cYOUR_CLIENT_SECRET\u003e Configure the OIDC plugin You can configure the OIDC plugin to use Kong Identity as the identity provider for your Gateway Services. In this example, you’ll apply the plugin to the control plane globally, but you can alternatively apply it to the Gateway Service.\nFirst, get the ID of the serverless-default control plane you configured in the prerequisites:\nexport CONTROL_PLANE_ID=$(curl -sX GET \"https://us.api.konghq.com/v2/control-planes?filter%5Bname%5D%5Bcontains%5D=serverless-default\" \\ -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].id') Enable the OIDC plugin globally:\ncurl -sX POST \"https://us.api.konghq.com/v2/control-planes/$CONTROL_PLANE_ID/core-entities/plugins/\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"openid-connect\", \"config\": { \"issuer\": \"'$ISSUER_URL'\", \"auth_methods\": [ \"bearer\" ], \"audience\": [ \"http://myhttpbin.dev\" ] } }' | jq In this example:\nissuer: Setting that connects the plugin to your IdP (in this case, Kong Identity). auth_methods: Specifies that the plugin should use bearer for authentication. Generate a token for the client The Gateway Service requires an access token from the client to access the Service. Generate a token for the client by making a call to the issuer URL:\nexport ACCESS_TOKEN=$(curl -sX POST \"$ISSUER_URL/oauth/token\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=client_credentials\" \\ -d \"client_id=$CLIENT_ID\" \\ -d \"client_secret=$CLIENT_SECRET\" \\ -d \"scope=scope1\" | jq -r '.access_token') Check the token\necho $ACCESS_TOKEN | jwt decode --json - Expected result\n{ \"header\": { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"a01feebd-5bed-45d7-9244-582010807705\" }, \"payload\": { \"aud\": [ \"http://myhttpbin.dev\" ], \"claim1\": \"claim1\", \"client_id\": \"fxsn74prsnrcyskm\", \"exp\": 1758636665, \"iat\": 1758633065, \"iss\": \"https://wt3fgfqb8r7fktwe.us.identity.konghq.com/auth\", \"jti\": \"370cab31-31f4-44da-b0dc-74577b8a5a81\", \"nbf\": 1758633065, \"scope\": \"scope1\", \"sub\": \"fxsn74prsnrcyskm\" } } Access the Gateway service using the token Access the httpbin Gateway Service using the short-lived token generated by the authorization server from Kong Identity:\ncurl -i -X GET \"$DATA_PLANE_URL/httpbin-route/get\" \\ -H \"Authorization: Bearer $ACCESS_TOKEN\" Check the token with:\ncurl -sX GET \"$DATA_PLANE_URL/httpbin-route/get\" \\ -H \"Authorization: Bearer $ACCESS_TOKEN\" | jq -r '.headers.Authorization' | cut -d ' ' -f 2 | jwt decode -j - Expected output\n{ \"header\": { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"a01feebd-5bed-45d7-9244-582010807705\" }, \"payload\": { \"aud\": [ \"http://myhttpbin.dev\" ], \"claim1\": \"claim1\", \"client_id\": \"fxsn74prsnrcyskm\", \"exp\": 1758636665, \"iat\": 1758633065, \"iss\": \"https://wt3fgfqb8r7fktwe.us.identity.konghq.com/auth\", \"jti\": \"370cab31-31f4-44da-b0dc-74577b8a5a81\", \"nbf\": 1758633065, \"scope\": \"scope1\", \"sub\": \"fxsn74prsnrcyskm\" } } Cleaning Up After testing the configuration, reset the Control Plane:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f Delete the AuthN Server:\ncurl -sX DELETE \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID\" \\ -H \"Authorization: Bearer $PAT\"",
    "description": "Kong Identity Kong Identity enables you to use Konnect to generate, authenticate, and authorize API access. You can use Kong Identity to:\nCreate authorization servers per region Issue and validate access tokens Integrate secure authentication into your Kong Gateway APIs Create a Kong Service and Route cat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /httpbin-route EOF Submit the declaration",
    "tags": [],
    "title": "Kong Identity",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/1573-kong-identity/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Introduction Kong Konnect is an API lifecycle management platform delivered as a service. The management plane is hosted in the cloud by Kong. The management plane enables customers to securely execute API management activities such as create API routes, define services etc. Runtime environments connect with the management plane using mutual transport layer authentication (mTLS), receive the updates and take customer facing API traffic.\nThe runtime environments can be:\nSelf-managed and deployed in your personal environment Managed by Kong in two options: Dedicated Cloud Gateway: it runs on isolated infrastructure within Kong-managed environments in AWS, Azure, or GCP offering the performance and security of dedicated infrastructure with the operational ease of SaaS. Serverless Gateway: it is a lightweight API gateways, ideal for developers who want to test or experiment in a pre-production environment. Learning Objectives In this workshop, you will:\nGet an architectural overview of Kong Konnect platform.\nSet up Konnect runtime as a Serverless Gateway.\nLearn what are services, routes and plugin.\nDeploy a sample microservice and access the application using the defined route.\nUse the platform to address the following API Gateway use cases\nProxy caching Authentication and Authorization Response Transformer Request Callout Rate limiting Observability And the following AI Gateway use cases\nPrompt Engineering LLM-based Request and Reponse transformation Semantic Caching Token-based Rate Limiting Semantic Routing RAG - Retrieval-Augmented Generation Expected Duration Workshop Introduction (15 minutes) Architectural Walkthrough (10 minutes) Sample Application and initial use case (15 minutes) Addressing API and AI Gateway use cases (90 minutes) Observability (30 minutes) Next Steps and Cleanup (5 min)",
    "description": "Introduction Kong Konnect is an API lifecycle management platform delivered as a service. The management plane is hosted in the cloud by Kong. The management plane enables customers to securely execute API management activities such as create API routes, define services etc. Runtime environments connect with the management plane using mutual transport layer authentication (mTLS), receive the updates and take customer facing API traffic.\nThe runtime environments can be:\nSelf-managed and deployed in your personal environment Managed by Kong in two options: Dedicated Cloud Gateway: it runs on isolated infrastructure within Kong-managed environments in AWS, Azure, or GCP offering the performance and security of dedicated infrastructure with the operational ease of SaaS. Serverless Gateway: it is a lightweight API gateways, ideal for developers who want to test or experiment in a pre-production environment. Learning Objectives In this workshop, you will:",
    "tags": [],
    "title": "API Management with Kong Konnect and Serverless API Gateway",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
