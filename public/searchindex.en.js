var relearn_searchindex = [
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Decorator plugin adds an array of llm/v1/chat messages to either the start or end of an LLM consumer’s chat history. This allows you to pre-engineer complex prompts, or steer (and guard) prompts in such a way that the modification to the consumer’s LLM message is completely transparent.\nYou can use this plugin to pre-set a system prompt, set up specific prompt history, add words and phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway.\ncat \u003e ai-prompt-decorator.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai options: temperature: 1.0 - name: ai-prompt-decorator instance_name: ai-prompt-decorator-openai config: prompts: prepend: - role: system content: \"You will always respond in the Portuguese (Brazil) language.\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-decorator.yaml Send a request now:\ncurl -s -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-4\" }' | jq You can now click Next to proceed further.",
    "description": "The AI Prompt Decorator plugin adds an array of llm/v1/chat messages to either the start or end of an LLM consumer’s chat history. This allows you to pre-engineer complex prompts, or steer (and guard) prompts in such a way that the modification to the consumer’s LLM message is completely transparent.\nYou can use this plugin to pre-set a system prompt, set up specific prompt history, add words and phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway.",
    "tags": [],
    "title": "AI Prompt Decorator",
    "uri": "/16-ai-gateway/18-use-cases/151-prompt-engineering/1-prompt-decorator/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Template plugin lets you provide tuned AI prompts to users. Users only need to fill in the blanks with variable placeholders in the following format: {{variable}}. This lets admins set up templates, which can be then be used by anyone in the organization. It also allows admins to present an LLM as an API in its own right - for example, a bot that can provide software class examples and/or suggestions.\nThis plugin also sanitizes string inputs to ensure that JSON control characters are escaped, preventing arbitrary prompt injection.\nWhen calling a template, simply replace the messages (llm/v1/chat) or prompt (llm/v1/completions) with a template reference, in the following format: {template://TEMPLATE_NAME}\nHere’s an example of template definition:\ncat \u003e ai-prompt-template.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-prompt-template instance_name: ai-prompt-template-openai enabled: true config: allow_untemplated_requests: true templates: - name: template1 template: |- { \"messages\": [ { \"role\": \"user\", \"content\": \"Explain to me what {{thing}} is.\" } ] } EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-template.yaml Now, send a request referring the template:\ncurl -s -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": \"{template://template1}\", \"properties\": { \"thing\": \"niilism\" } }' | jq Kong-gratulations! have now reached the end of this module by authenticating your API requests with AWS Cognito. You can now click Next to proceed with the next module.",
    "description": "The AI Prompt Template plugin lets you provide tuned AI prompts to users. Users only need to fill in the blanks with variable placeholders in the following format: {{variable}}. This lets admins set up templates, which can be then be used by anyone in the organization. It also allows admins to present an LLM as an API in its own right - for example, a bot that can provide software class examples and/or suggestions.",
    "tags": [],
    "title": "AI Prompt Template",
    "uri": "/16-ai-gateway/18-use-cases/151-prompt-engineering/2-prompt-template/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Let’s get started with a simple round-robin policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: serverless-default services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: round-robin targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: openai name: gpt-5 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name serveless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Send the request few times. Note we are going to receive responses from both LLMs, in a round-robin way.\ncurl -s -X POST \\ --url $DATA_PLANE_URL/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq Here’s the gpt-5 response:\n{ \"id\": \"chatcmpl-CJQJOSWpos9jzGMiYiXpeXcrMyGEN\", \"object\": \"chat.completion\", \"created\": 1758745410, \"model\": \"gpt-5-2025-08-07\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Adam Mickiewicz is most often considered the greatest Polish writer (and Poland’s national bard). Depending on criteria, others frequently cited include Henryk Sienkiewicz, Stanisław Lem, Czesław Miłosz, and Wisława Szymborska.\", \"refusal\": null, \"annotations\": [] }, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 14, \"completion_tokens\": 385, \"total_tokens\": 399, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 320, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": null } And here is gpt-4’s:\n{ \"id\": \"chatcmpl-CJQJYClEQQXwtNpnm6BiAuhLufLex\", \"object\": \"chat.completion\", \"created\": 1758745420, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"The title of **greatest Polish writer** is most often attributed to **Adam Mickiewicz** (1798–1855). Mickiewicz, a Romantic-era poet, is widely regarded as Poland’s national poet and a key figure in Polish literature. His epic poem **\\\"Pan Tadeusz\\\"** is considered the national epic of Poland, and his influence on Polish identity, language, and literature is profound.\\n\\nOther notable Polish writers who are sometimes mentioned in discussions of the greatest include:\\n\\n- **Henryk Sienkiewicz** (Nobel Prize in Literature 1905), known for the historical novel **\\\"Quo Vadis\\\"** and the \\\"Trilogy\\\" (“Trylogia”).\\n- **Wisława Szymborska** (Nobel Prize in Literature 1996), celebrated for her poetry.\\n- **Czesław Miłosz** (Nobel Prize in Literature 1980), a poet and essayist.\\n- **Bruno Schulz**, known for his unique prose.\\n- **Stanisław Lem**, a world-renowned science fiction author.\\n\\nHowever, in terms of cultural impact and universal recognition as the quintessential Polish writer, **Adam Mickiewicz** is most frequently given that distinction.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 15, \"completion_tokens\": 252, \"total_tokens\": 267, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_62bb2e3c55\" }",
    "description": "Let’s get started with a simple round-robin policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: serverless-default services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: round-robin targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: openai name: gpt-5 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Round Robin",
    "uri": "/16-ai-gateway/18-use-cases/159-ai-proxy-advanced/2-round-robin/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Guard plugin lets you to configure a series of PCRE-compatible regular expressions as allow or deny lists, to guard against misuse of llm/v1/chat or llm/v1/completions requests.\nYou can use this plugin to allow or block specific prompts, words, phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway. It does this by scanning all chat messages (where the role is user) for the specific expressions set. You can use a combination of allow and deny rules to preserve integrity and compliance when serving an LLM service using Kong Gateway.\nFor llm/v1/chat type models: You can optionally configure the plugin to ignore existing chat history, wherein it will only scan the trailing user message. For llm/v1/completions type models: There is only one prompt field, thus the whole prompt is scanned on every request. The plugin matches lists of regular expressions to requests through AI Proxy. The matching behavior is as follows:\nIf any deny expressions are set, and the request matches any regex pattern in the deny list, the caller receives a 400 response. If any allow expressions are set, but the request matches none of the allowed expressions, the caller also receives a 400 response. If any allow expressions are set, and the request matches one of the allow expressions, the request passes through to the LLM. If there are both deny and allow expressions set, the deny condition takes precedence over allow. Any request that matches an entry in the deny list will return a 400 response, even if it also matches an expression in the allow list. If the request does not match an expression in the deny list, then it must match an expression in the allow list to be passed through to the LLM Here’s an example to allow only valid credit cards numbers:\ncat \u003e ai-prompt-guard.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-prompt-guard instance_name: ai-prompt-guard-openai enabled: true config: allow_all_conversation_history: true allow_patterns: - \".*\\\\\\\"card\\\\\\\".*\\\\\\\"4[0-9]{3}\\\\*{12}\\\\\\\"\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-guard.yaml Send a request with a valid credit card pattern:\ncurl -s -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Validate this card: {\\\"card\\\": \\\"4111************\\\", \\\"cvv\\\": \\\"000\\\"}\" } ] }' | jq '.' Now, send a non-valid number:\ncurl -s -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Validate this card: {\\\"card\\\": \\\"4111xyz************\\\", \\\"cvv\\\": \\\"000\\\"}\" } ] }' | jq '.' The expect result is:\n{ \"error\": { \"message\": \"bad request\" } }",
    "description": "The AI Prompt Guard plugin lets you to configure a series of PCRE-compatible regular expressions as allow or deny lists, to guard against misuse of llm/v1/chat or llm/v1/completions requests.\nYou can use this plugin to allow or block specific prompts, words, phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway. It does this by scanning all chat messages (where the role is user) for the specific expressions set. You can use a combination of allow and deny rules to preserve integrity and compliance when serving an LLM service using Kong Gateway.",
    "tags": [],
    "title": "AI Prompt Guard",
    "uri": "/16-ai-gateway/18-use-cases/151-prompt-engineering/3-prompt-guard/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Now, let’s redirect 80% of the request to OpenAI’s gpt-4.1 with a weight based policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: serverless-default services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} weight: 80 - model: provider: openai name: gpt-5 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} weight: 20 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml",
    "description": "Now, let’s redirect 80% of the request to OpenAI’s gpt-4.1 with a weight based policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: serverless-default services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} weight: 80 - model: provider: openai name: gpt-5 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} weight: 20 EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Weight",
    "uri": "/16-ai-gateway/18-use-cases/159-ai-proxy-advanced/3-weight/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.\nCreate a file with the following declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: openai name: gpt-5 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Test the Route again.\ncurl -s -X POST \\ --url $DATA_PLANE_URL/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq Lowest Usage policy The lowest-usage algorithm in AI Proxy Advanced is based on the volume of usage for each model. It balances the load by distributing requests to models with the lowest usage, measured by factors such as prompt token counts, response token counts, or other resource metrics.\nReplace the declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-usage targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: openai name: gpt-5 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} EOF Apply the declaration:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml And test the Route again.\ncurl -s -X POST \\ --url $DATA_PLANE_URL/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq",
    "description": "Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.\nCreate a file with the following declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: openai name: gpt-5 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Lowest-Latency and Lowest-Usage",
    "uri": "/16-ai-gateway/18-use-cases/159-ai-proxy-advanced/4-lowest-latency-usage/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "Knowledge Attendees should have basic knowledge of containers and other products like Keycloak, Redis, Prometheus etc.\nAttendees should have basic knowledge of GenAI models, specially LLMs and Embedding models as well as providers like OpenAI, Mistral, Anthropic, AWS, GCP, Azure.\nKong Academy Complete two badges via Kong Academy: Kong Gateway Foundations and Kong Gateway Operations\nEnvironment 1. Kong Konnect Subscription You will need a Kong Konnect Subscription to execute this workshop.\n2. decK (Declarations for Kong) Please, make sure you have decK installed in your local environment.\n3. LLM Some AI use cases require a GenAI infracture. Please make sure you have a OpenAI API Key.\n4. Command Line Utilities In this workshop, we will use the following command line utilities\ncurl jq yq jwt-cli wget",
    "description": "Knowledge Attendees should have basic knowledge of containers and other products like Keycloak, Redis, Prometheus etc.\nAttendees should have basic knowledge of GenAI models, specially LLMs and Embedding models as well as providers like OpenAI, Mistral, Anthropic, AWS, GCP, Azure.\nKong Academy Complete two badges via Kong Academy: Kong Gateway Foundations and Kong Gateway Operations\nEnvironment 1. Kong Konnect Subscription You will need a Kong Konnect Subscription to execute this workshop.",
    "tags": [],
    "title": "Prerequisites",
    "uri": "/10-prerequisites/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Prerequisites",
    "content": "Konnect Plus Subscription You will need a Konnect subscription. Follow the steps below to obtain a Konnect Plus subscription. Initially $500 of credits are provided for up to 30 days, after the credits have expired or the time has finished Konnect Plus will charge based on Konnect Plus pricing. Following the workshop instructions will use less than $500 credits. After the workshop has finished the Konnect Plus environment can be deleted.\nClick on the Registration link and present your credentials.\nKonnect will send you an email to confirm the subscription. Click on the link in email to confirm your subscription.\nThe Konnect environment can be accessed via the Konnect log in page.\nAfter logging in create an organisation name, select a region, then answer a few questions.\nCredit available can be monitored though Plan and Usage page.",
    "description": "Konnect Plus Subscription You will need a Konnect subscription. Follow the steps below to obtain a Konnect Plus subscription. Initially $500 of credits are provided for up to 30 days, after the credits have expired or the time has finished Konnect Plus will charge based on Konnect Plus pricing. Following the workshop instructions will use less than $500 credits. After the workshop has finished the Konnect Plus environment can be deleted.",
    "tags": [],
    "title": "Konnect Subscription",
    "uri": "/10-prerequisites/konnect-subscription/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "The Kong Konnect platform provides a cloud control plane (CP), which manages all service configurations. It propagates those configurations to all Runtime control planes, which use in-memory storage. These nodes can be installed anywhere, on-premise or in AWS.\nFor today workshop, we will be focusing on Kong Gateway. Kong Gateway data plane listen for traffic on the proxy port 443 by default. The data plane evaluates incoming client API requests and routes them to the appropriate backend APIs. While routing requests and providing responses, policies can be applied with plugins as necessary.\nKonnect modules Kong Konnect Enterprise features are described in this section, including modules and plugins that extend and enhance the functionality of the Kong Konnect platform.\nControl Plane (Gateway Manager) Control Plane empowers your teams to securely collaborate and manage their own set of runtimes and services without the risk of impacting other teams and projects. Control Plane instantly provisions hosted Kong Gateway control planes and supports securely attaching Kong Gateway data planes from your cloud or hybrid environments.\nThrough the Control Plane, increase the security of your APIs with out-of-the-box enterprise and community plugins, including OpenID Connect, Open Policy Agent, Mutual TLS, and more.\nAI Manager Manage all of your LLMs in a single dashboard providing a unified control plane to create, manage, and monitor LLMs using the Konnect platform. With AI Manager you can assign Gateway Services and define how traffic is distributed across models, enable streaming responses and manage authentication through the AI Gateway, monitor request and token volumes, track error rates, and measure average latency with historical comparisons, etc.\nDev Portal Streamline developer onboarding with the Dev Portal, which offers a self-service developer experience to discover, register, and consume published services from your Service Hub catalog. This customizable experience can be used to match your own unique branding and highlights the documentation and interactive API specifications of your services. Enable application registration to automatically secure your APIs with a variety of authorization providers.\nAnalytics Use Analytics to gain deep insights into service, route, and application usage and health monitoring data. Keep your finger on the pulse of the health of your API products with custom reports and contextual dashboards. In addition, you can enhance the native monitoring and analytics capabilities with Kong Gateway plugins that enable streaming monitoring metrics to third-party analytics providers.\nTeams To help secure and govern your environment, Konnect provides the ability to manage authorization with teams. You can use Konnect’s predefined teams for a standard set of roles, or create custom teams with any roles you choose. Invite users and add them to these teams to manage user access. You can also map groups from your existing identity provider into Konnect teams.\nFurther Reading Gateway Manager AI Manager Dev Portal Analytics",
    "description": "The Kong Konnect platform provides a cloud control plane (CP), which manages all service configurations. It propagates those configurations to all Runtime control planes, which use in-memory storage. These nodes can be installed anywhere, on-premise or in AWS.\nFor today workshop, we will be focusing on Kong Gateway. Kong Gateway data plane listen for traffic on the proxy port 443 by default. The data plane evaluates incoming client API requests and routes them to the appropriate backend APIs. While routing requests and providing responses, policies can be applied with plugins as necessary.",
    "tags": [],
    "title": "Kong Konnect Architectural Overview",
    "uri": "/architecture/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "This chapter will walk you through\nKonnect Control Plane and Data Plane. Access Kong Data Plane. Here’s a Reference Architecture that will be implemented in this workshop:\nKong Konnect Control Plane: responsible for managing your APIs Kong Konnect Data Plane: connected to the Control Plane, it is responsible for processing all the incoming requests sent by the consumers. Kong provides a plugin framework, where each one of them is responsible for a specific functionality. As a can see, there are two main collections of plugins: On the left, the historic and regular API Gateway plugins, implementing all sort of policies including, for example, OIDC based Authentication processes with Keycloak, Amazon Cognito and Okta or Observability with Prometheus/Grafana and Dynatrace. On the right, another plugin collection for AI-based use cases. For example, the AI Rate Limiting plugin implements policies like this based on the number of tokens consumed be the requests. Or, as another example is the AI Semantic Cache plugin, which caches data based on the semantics related to the responses coming from the LLM models. Kong AI Gateway supports, out of the box, a variety of infrastructures, including not just OpenAI, but also Amazon Bedrock, Google Gemini, Mistral, Anthropic, etc. In order to deal with embeddings, the Gateway also supports also vector databases. Kong Gateway protects not just the LLM Models but also the upstream services, including your applications and microservices. Konnect Control Planes Kong Konnect is the main Kong offering for hybrid deployments, including:\nSelf-managed Data Planes Dedicated Cloud Gateways - Data Plane nodes that are fully managed by Kong Serverless gateways - lightweight API gateways, also managed by Kong Kong Ingress Controller which allows you to run Kong Gateway as a Kubernetes Ingress. To check all these options, click + New Gateway inside the API Gateway menu option:\nServerless Proxy URL Konnect trial creates, by default, a Serverless API Gateway, naming it serverless-default Log in to the Kong Konnect UI. Inside the “API Gateway” page you should see your your Serverless Control Plane:\nCopy the value of your Control Plane Proxy URL and keep it handy. That’s the URL you Data Plane is located.\nSend a request to your Serverless Data Plane Save your URL in an enviroment variable:\nexport DATA_PLANE_URL=\u003cYOUR_DATA_PLANE_URL\u003e You can use curl to send the first request to the Data Plane\ncurl $DATA_PLANE_URL Expected result\n{ \"message\":\"no Route matched with those values\", \"request_id\":\"84fac2649eb6ae01f4d920115a4df70d\" } Konnect Control Plane and Kong Objects There are multiple ways to create new Kong Objects in your Control Plane:\nKonnect User Interface. RESTful Admin API, a fundamental mechanism for administration purposes. Kong Gateway Operator (KGO) and Kubernetes CRDs. deck - Declarations for Kong. To get an easier and faster deployment, this workshop uses deck and Konnect UI.\nThis tutorial is intended to be used for labs and PoC only. There are many aspects and processes, typically implemented in production sites, not described here. For example: Digital Certificate issuing, Cluster monitoring, etc. For a production ready deployment, refer Kong on Terraform Constructs, available here\nYou can now click Next to begin the module.",
    "description": "This chapter will walk you through\nKonnect Control Plane and Data Plane. Access Kong Data Plane. Here’s a Reference Architecture that will be implemented in this workshop:\nKong Konnect Control Plane: responsible for managing your APIs Kong Konnect Data Plane: connected to the Control Plane, it is responsible for processing all the incoming requests sent by the consumers. Kong provides a plugin framework, where each one of them is responsible for a specific functionality. As a can see, there are two main collections of plugins: On the left, the historic and regular API Gateway plugins, implementing all sort of policies including, for example, OIDC based Authentication processes with Keycloak, Amazon Cognito and Okta or Observability with Prometheus/Grafana and Dynatrace. On the right, another plugin collection for AI-based use cases. For example, the AI Rate Limiting plugin implements policies like this based on the number of tokens consumed be the requests. Or, as another example is the AI Semantic Cache plugin, which caches data based on the semantics related to the responses coming from the LLM models. Kong AI Gateway supports, out of the box, a variety of infrastructures, including not just OpenAI, but also Amazon Bedrock, Google Gemini, Mistral, Anthropic, etc. In order to deal with embeddings, the Gateway also supports also vector databases. Kong Gateway protects not just the LLM Models but also the upstream services, including your applications and microservices. Konnect Control Planes Kong Konnect is the main Kong offering for hybrid deployments, including:",
    "tags": [],
    "title": "Konnect Setup",
    "uri": "/11-konnect-setup/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Setup",
    "content": "decK requires a Konnect Personal Access Token (PAT) to manage your Control Plane. To generate your PAT, click on your initials in the upper right corner of the Konnect home page, then select Personal Access Tokens. Click on + Generate Token, name your PAT, set its expiration time, and be sure to copy and save it in an evironment variable, as Konnect won’t display it again.\nNote Be sure to copy and save your PAT, as Konnect won’t display it again.\nKonnect PAT secret Save PAT in an environment variables export PAT=\u003cPASTE_THE_CONTENTS_OF_COPIED_PAT\u003e Test your PAT deck gateway ping --konnect-control-plane-name serverless-default --konnect-token $PAT You should get a response like this\nSuccessfully Konnected to the AcquaOrg organization!",
    "description": "decK requires a Konnect Personal Access Token (PAT) to manage your Control Plane. To generate your PAT, click on your initials in the upper right corner of the Konnect home page, then select Personal Access Tokens. Click on + Generate Token, name your PAT, set its expiration time, and be sure to copy and save it in an evironment variable, as Konnect won’t display it again.",
    "tags": [],
    "title": "PAT - Personal Access Token",
    "uri": "/11-konnect-setup/112-personal-access-token/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "With our Control Plane created and Data Plane layer deployed it’s time to create an API and expose an application. In this module, we will:\nUse Konnect UI to import an OpenAPI specification. Use decK to define a Kong Service based on an endpoint provided by the application and a Kong Route on top of the Kong Service to expose the application. Enable Kong Plugins to the Kong Route or Kong Service. Define Kong Consumers to represent the entities sending request to the Gateway and enable Kong Plugin to them. With decK (declarations for Kong) you can manage Kong Konnect configuration in a declaratively way.\ndecK operates on state files. decK state files describe the configuration of Kong API Gateway. State files encapsulate the complete configuration of Kong in a declarative format, including services, routes, plugins, consumers, and other entities that define how requests are processed and routed through Kong.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Konnect Gateway Manager",
    "description": "With our Control Plane created and Data Plane layer deployed it’s time to create an API and expose an application. In this module, we will:\nUse Konnect UI to import an OpenAPI specification. Use decK to define a Kong Service based on an endpoint provided by the application and a Kong Route on top of the Kong Service to expose the application. Enable Kong Plugins to the Kong Route or Kong Service. Define Kong Consumers to represent the entities sending request to the Gateway and enable Kong Plugin to them. With decK (declarations for Kong) you can manage Kong Konnect configuration in a declaratively way.",
    "tags": [],
    "title": "Kong API Gateway",
    "uri": "/12-api-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway",
    "content": "Kong Gateway can proxy:\nLayer 7 protocol, including: REST, GraphQL, gRPC, Websocket, SOAP, Kafka Layer 4 TCP and UDP Streaming Kong Gateway Service Gateway Services represent the upstream services in your system. These applications are the business logic components of your system responsible for responding to requests.\nThe configuration of a Gateway Service defines the connectivity details between the Kong Gateway and the upstream service, along with other metadata. Generally, you should map one Gateway Service to each upstream service.\nFor simple deployments, the upstream URL can be provided directly in the Gateway Service. For sophisticated traffic management needs, a Gateway Service can point at an Upstream.\nIn the following diagram, seven Kong Gateway Services objects should be defined in Kong.\nKong Route Gateway Services, in conjunction with Routes, let you expose your upstream services to clients with Kong Gateway, defining an entry point for client requests.\nA Kong Route defines rules that match client requests and associate them with a Kong Service. Routing can occur by PATH, URI, HEADERS, etc.\nA Kong Service can have many Kong Routes associated with it.\nKong Plugin Plugins can be attached to a Service, and will run against every request that triggers a request to the Service that they’re attached to.\nPlugins extend the functionality of Kong Gateway. They can be applied to different entities (i.e. Routes, Services, etc.).\nKong Gateway provides 90+ plugins out of the box and allows for Custom Plugins to be created",
    "description": "Kong Gateway can proxy:\nLayer 7 protocol, including: REST, GraphQL, gRPC, Websocket, SOAP, Kafka Layer 4 TCP and UDP Streaming Kong Gateway Service Gateway Services represent the upstream services in your system. These applications are the business logic components of your system responsible for responding to requests.\nThe configuration of a Gateway Service defines the connectivity details between the Kong Gateway and the upstream service, along with other metadata. Generally, you should map one Gateway Service to each upstream service.",
    "tags": [],
    "title": "Kong Gateway Service, Kong Route and Kong Plugin",
    "uri": "/12-api-gateway/121-kong-service-route/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway",
    "content": "For our first Kong Service, we are going to import an OpenAPI spec (OAS) to our Control Plane. The Control Plane will convert the spec into Kong Service and Kong Routes.\nDownload the bankong.yaml spec. In your Control Plane, click on Import via OAS spec. Choose the bankong.yaml spec and click Continue. Review the Import Summary and click Import Notice the Services and Routes that will be imported Notice declarative representation of this import as well (more on this later) You should see your new Kong Service and Routes:\nTest the API Deployment curl -i $DATA_PLANE_URL/transactions You should get a response from the API. Notice the x-kong-* headers like request id, proxy latency, upstream latency. Run the request again, what do you notice about the proxy latency now?\nHTTP/2 200 content-type: application/json; charset=utf-8 content-length: 517 x-kong-request-id: 79d76d16883197392033bd590536481b x-powered-by: Express vary: Origin, Accept-Encoding access-control-allow-credentials: true cache-control: no-cache pragma: no-cache expires: -1 x-content-type-options: nosniff etag: W/\"205-u4o2XSHOR6oYVCAyD/5BTbm6Xgk\" date: Tue, 23 Sep 2025 11:58:13 GMT server: kong/3.11.0.1-enterprise-edition x-kong-upstream-latency: 39 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.1-enterprise-edition, 1.1 kong/3.11.0.0-enterprise-edition [ { \"source\": \"DE8412325587359375895\", \"senderName\": \"Max Mustermann\", \"destination\": \"GR872659435350353\", \"amount\": 10.2, \"currency\": \"EUR\", \"subject\": \"The money we have talked about\", \"id\": \"b88f7029-fa93-41a5-9462-4884e544bf63\" }, { \"source\": \"UK8412325587359375895\", \"senderName\": \"Mister Smith\", \"destination\": \"GR872559435350353\", \"amount\": 10000, \"currency\": \"EUR\", \"subject\": \"Invoice #34078ja\", \"id\": \"143aadce-f995-4503-ba6e-01ed01c6af88\" } ]",
    "description": "For our first Kong Service, we are going to import an OpenAPI spec (OAS) to our Control Plane. The Control Plane will convert the spec into Kong Service and Kong Routes.\nDownload the bankong.yaml spec. In your Control Plane, click on Import via OAS spec. Choose the bankong.yaml spec and click Continue. Review the Import Summary and click Import Notice the Services and Routes that will be imported Notice declarative representation of this import as well (more on this later) You should see your new Kong Service and Routes:",
    "tags": [],
    "title": "Import an OpenAPI specification",
    "uri": "/12-api-gateway/122-oas-import/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway",
    "content": "Now, let’s use decK to create Kong Objects. This time, you’ll create and expose a service to the HTTPbin API. HTTPbin is an echo-type application that returns requests back to the requester as responses.\nPing Konnect with decK Before start using decK, you should ping Konnect to check if the connecting is up. Note we assume you have the PAT environment variable set. Please, refer to the previous section to learn how to issue a PAT.\ndeck gateway ping --konnect-control-plane-name serverless-default --konnect-token $PAT Expected Output\nSuccessfully Konnected to the AcquaOrg organization! Create a Kong Gateway Service and Kong Route Create the following declaration first. Remarks:\nNote the host and port refers to the HTTPbin’s endpoint http://httpbin.konghq.com:80. The declaration tags the objects so you can managing them apart from other ones. cat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /httpbin-route EOF Submit the declaration Before submiting the declaration, run the following deck command to reset your Control Plane and delete all Kong Objects you might have created previously.\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f Now, you can use the following command to sync your Konnect Control Plane with the declaration.\ndeck gateway sync --konnect-token $PAT httpbin.yaml Expected Output\ncreating service httpbin-service creating route httpbin-route Summary: Created: 2 Updated: 0 Deleted: 0 You should see your new service’s overview page.\nConsume the Route We are to use the same ELB provisioned during the Data Plane deployment:\ncurl -v $DATA_PLANE_URL/httpbin-route/get If successful, you should see the httpbin output:\n* Host kong-cceb6a93c9usmc2hk.kongcloud.dev:443 was resolved. * IPv6: (none) * IPv4: 66.51.127.198 * Trying 66.51.127.198:443... * Connected to kong-cceb6a93c9usmc2hk.kongcloud.dev (66.51.127.198) port 443 * ALPN: curl offers h2,http/1.1 * (304) (OUT), TLS handshake, Client hello (1): * CAfile: /etc/ssl/cert.pem * CApath: none * (304) (IN), TLS handshake, Server hello (2): * (304) (IN), TLS handshake, Unknown (8): * (304) (IN), TLS handshake, Certificate (11): * (304) (IN), TLS handshake, CERT verify (15): * (304) (IN), TLS handshake, Finished (20): * (304) (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / AEAD-CHACHA20-POLY1305-SHA256 / [blank] / UNDEF * ALPN: server accepted h2 * Server certificate: * subject: CN=*.kongcloud.dev * start date: Aug 15 23:28:00 2025 GMT * expire date: Nov 13 23:27:59 2025 GMT * subjectAltName: host \"kong-cceb6a93c9usmc2hk.kongcloud.dev\" matched cert's \"*.kongcloud.dev\" * issuer: C=US; O=Let's Encrypt; CN=E6 * SSL certificate verify ok. * using HTTP/2 * [HTTP/2] [1] OPENED stream for https://kong-cceb6a93c9usmc2hk.kongcloud.dev/httpbin-route/get * [HTTP/2] [1] [:method: GET] * [HTTP/2] [1] [:scheme: https] * [HTTP/2] [1] [:authority: kong-cceb6a93c9usmc2hk.kongcloud.dev] * [HTTP/2] [1] [:path: /httpbin-route/get] * [HTTP/2] [1] [user-agent: curl/8.7.1] * [HTTP/2] [1] [accept: */*] \u003e GET /httpbin-route/get HTTP/2 \u003e Host: kong-cceb6a93c9usmc2hk.kongcloud.dev \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/2 200 \u003c content-type: application/json \u003c content-length: 500 \u003c x-kong-request-id: 738fe4313578a34415154c6833df4e40 \u003c server: gunicorn/19.9.0 \u003c date: Tue, 23 Sep 2025 12:10:29 GMT \u003c access-control-allow-origin: * \u003c access-control-allow-credentials: true \u003c x-kong-upstream-latency: 12 \u003c x-kong-proxy-latency: 69 \u003c via: 1.1 kong/3.11.0.0-enterprise-edition \u003c { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Host\": \"httpbin.konghq.com\", \"User-Agent\": \"curl/8.7.1\", \"X-Forwarded-Host\": \"kong-cceb6a93c9usmc2hk.kongcloud.dev\", \"X-Forwarded-Path\": \"/httpbin-route/get\", \"X-Forwarded-Prefix\": \"/httpbin-route\", \"X-Kong-Request-Id\": \"738fe4313578a34415154c6833df4e40\" }, \"origin\": \"186.204.54.49, 66.51.127.198, 172.16.12.194\", \"url\": \"https://kong-cceb6a93c9usmc2hk.kongcloud.dev/get\" } * Connection #0 to host kong-cceb6a93c9usmc2hk.kongcloud.dev left intact```",
    "description": "Now, let’s use decK to create Kong Objects. This time, you’ll create and expose a service to the HTTPbin API. HTTPbin is an echo-type application that returns requests back to the requester as responses.\nPing Konnect with decK Before start using decK, you should ping Konnect to check if the connecting is up. Note we assume you have the PAT environment variable set. Please, refer to the previous section to learn how to issue a PAT.",
    "tags": [],
    "title": "Use decK to create Kong Objects",
    "uri": "/12-api-gateway/123-kong-service-route-deck/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "Concept APIOps applies DevOps principles (automation, version control, CI/CD) to API lifecycle management. Here’s a diagrama describing an APIOps lifecycle with Kong Technologies.\nThere are 3 main steps used to deliver APIs to Kong Gateway. These workflows are completed in sequence, with administrator approval between each step. Completion of final workflow results in a deployed API to Kong Konnect.\nOpenAPI to Kong: This step takes an OpenAPI specification and converts it to a decK file that can be used by Kong Gateway. Additionally, service specific patches are applied to the decK file allowing administrators to set overrides for specific APIs. Stage decK changes: This step takes the decK file generated by the OpenAPI to Kong step and compares it to the current state of the Kong Gateway. It generates a diff and stages the changes as a PR. decK sync: This step is triggered by the merging of deck configuration files to the main branch (by the previous workflow PR approval). The step triggers a deck gateway sync command on the configuration files and applies the changes to Kong Gateway. decK and APIOps With Kong Konnect and decK, APIs can be treated as code — versioned, tested, and deployed through pipelines.\nIn this section we are going to implement a simple flow starting with the OpenAPI specification, converting it to a Kong declaration and applying to the Control Plane.\nDownload the OpenAPI specification Download the kong-air-apis.yaml OpenAPI spec. The spec defines routes to consume APIs exposed by the KongAir API Server\nConvert the OpenAPI spec into Kong configuration deck file openapi2kong --spec ./kong-air-apis.yaml -o kong.yaml Add tags for traceability deck file add-tags kong-konnect-workshop -s kong.yaml -o kong.yaml Merge plugins configuration Create a plugin configuration file\ncat \u003e plugins.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - kong-konnect-workshop plugins: - name: proxy-cache enabled: true config: strategy: memory cache_ttl: 30 EOF Merge the files\ndeck file merge ./kong.yaml ./plugins.yaml -o kong.yaml Sync with Konnect deck gateway sync kong.yaml \\ --konnect-addr \"https://us.api.konghq.com\" \\ --konnect-control-plane-name serverless-default \\ --konnect-token $PAT \\ --select-tag kong-konnect-workshop Expected output:\ncreating service routes-service creating route routes-service_get-route creating route routes-service_health-check creating route routes-service_get-routes creating plugin proxy-cache (global) Summary: Created: 5 Updated: 0 Deleted: 0 Consume a Kong Route curl -s $DATA_PLANE_URL/routes | jq CI tool integration As a best practice, for a CI tool integration, these steps should be included in a script and integrated into a CI tool of your choice (e.g., Jenkins, GitHub Actions, GitLab CI).\nThe script should be triggered the script on every code push or pull request.\nLastly, you should store your Konnect token as secrets.\nKey Takeaways Treat APIs as code: store OpenAPI + Kong configs in version control. Automate API deployment with decK for repeatability and reliability. CI/CD tools can run Bash scripts to enable APIOps in real-world projects. Next Steps Extend the script with tests (linting, validation, diff before sync). Explore GitHub Actions or Jenkins examples for pipeline automation. Move towards full GitOps by integrating declarative API configs with source control workflows. Kong Konnect Automation Options Besides decK, Kong offers other options to implement APIOps automation:\nTerraform: Used for both Konnect platform and gateway configuration Konnect APIs: Used for both Konnect platform and gateway configuration Reset your Control Plane deck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f",
    "description": "Concept APIOps applies DevOps principles (automation, version control, CI/CD) to API lifecycle management. Here’s a diagrama describing an APIOps lifecycle with Kong Technologies.\nThere are 3 main steps used to deliver APIs to Kong Gateway. These workflows are completed in sequence, with administrator approval between each step. Completion of final workflow results in a deployed API to Kong Konnect.\nOpenAPI to Kong: This step takes an OpenAPI specification and converts it to a decK file that can be used by Kong Gateway. Additionally, service specific patches are applied to the decK file allowing administrators to set overrides for specific APIs. Stage decK changes: This step takes the decK file generated by the OpenAPI to Kong step and compares it to the current state of the Kong Gateway. It generates a diff and stages the changes as a PR. decK sync: This step is triggered by the merging of deck configuration files to the main branch (by the previous workflow PR approval). The step triggers a deck gateway sync command on the configuration files and applies the changes to Kong Gateway. decK and APIOps With Kong Konnect and decK, APIs can be treated as code — versioned, tested, and deployed through pipelines.",
    "tags": [],
    "title": "APIOps and decK",
    "uri": "/14-apiops-deck/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Proxy plugin is the fundamental AI Gateway component. It lets you transform and proxy requests to a number of AI providers and models. The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nThe following table describes which providers and requests the AI Proxy plugin supports:\nObs 1: OpenAI has marked Completions as legacy and recommends using the Chat Completions API for developing new applications.\nObs 2: Starting with Kong AI Gateway 3.11, new GenAI APIs are supported:\nGetting Started with Kong AI Gateway We are going to get started with a simple configuration. The following decK declaration enables the AI Proxy plugin to the Kong Gateway Service, to send requests to the LLM and consume the Mistral’s mistral-tiny FM with chat LLM requests.\nUpdate your ai-proxy.yaml file with that. Make sure you have the DECK_OPENAI_API_KEY environment variable set with your OpenAI’s API Key.\nSet an environment variable with your Mistral’s API Key.\nexport DECK_OPENAI_API_KEY=\u003cYOUR_OPENAI_API_KEY\u003e Create the declaration:\ncat \u003e ai-proxy.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-5 options: temperature: 1.0 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy.yaml OpenAI API Kong AI Gateway provides one API to access all of the LLMs it supports. To accomplish this, Kong AI Gateway has standardized on the OpenAI API specification. This will help developers to onboard more quickly by providing them with an API specification that they’re already familiar with. You can start using LLMs behind the AI Gateway simply by redirecting your requests to a URL that points to a route of the AI Gateway.\nSend a request to Kong AI Gateway Now, send a request to Kong AI Gateway following the OpenAI API Chat specification as a reference:\ncurl -s -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ] }' | jq Expected Output\nNote the response also complies to the OpenAI API spec:\n{ \"id\": \"chatcmpl-CJPuRRxO43loCR0KCL5OMoAQLlH8u\", \"object\": \"chat.completion\", \"created\": 1758743863, \"model\": \"gpt-5-2025-08-07\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Pi (π) is the mathematical constant equal to the ratio of a circle’s circumference to its diameter.\\n\\nKey points:\\n- Approximate value: 3.141592653589793…\\n- Properties: irrational and transcendental (its decimal expansion never ends or repeats).\\n- Common formulas: C = 2πr (circumference), A = πr² (area).\\n- Ubiquitous in math and physics; appears in trigonometry, calculus, probability, and identities like e^(iπ) + 1 = 0.\\n- Handy approximations: 3.14, 22/7, 355/113.\\n\\nIf you meant a different “pi” (e.g., principal investigator, Raspberry Pi), tell me which.\", \"refusal\": null, \"annotations\": [] }, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 10, \"completion_tokens\": 611, \"total_tokens\": 621, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 448, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": null } AI Proxy configuration parameters The AI Proxy plugin is responsible for a variety of topics. For example:\nRequest and response formats appropriate for the configured provider and route_type settings. provider can be set as anthropic, azure, bedrock, cohere, gemini, huggingface, llama2, mistral or openai. The route_type AI Proxy configuration parameter defines which kind of request the AI Gateway is going to perform. It must be one of: audio/v1/audio/speech audio/v1/audio/transcriptions audio/v1/audio/translations image/v1/images/edits image/v1/images/generations llm/v1/assistants llm/v1/batches llm/v1/chat llm/v1/completions llm/v1/embeddings llm/v1/files llm/v1/responses preserve realtime/v1/realtime Authentication on behalf of the Kong API consumer. Decorating the request with parameters from the config.model.options block, appropriate for the chosen provider. For our case, we tell the temperature we are going to use. Define the model to be consume when sending the request As you may have noticed our AI Proxy plugin defines the model it should consume. That is can be done for individual requests, if required. Change the ai-proxy.yaml file, removing the model’s name parameter and apply the declaration again:\ncat \u003e ai-proxy.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai options: temperature: 1.0 EOF Submit the declaration\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy.yaml Send the request specifing the model:\ncurl -i -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-5\" }' or\ncurl -i -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-4\" }' Note the Kong AI Proxy plugin adds a new x-kong-llm-model header with the model we consumed: mistral/mistral-tiny\nStreaming Normally, a request is processed and completely buffered by the LLM before being sent back to Kong AI Gateway and then to the caller in a single large JSON block. This process can be time-consuming, depending on the request parameters, and the complexity of the request sent to the LLM model. To avoid making the user wait for their chat response with a loading animation, most models can stream each word (or sets of words and tokens) back to the client. This allows the chat response to be rendered in real time.\nThe config AI Proxy configuration section has a response_streaming parameter to define the response streaming. By default is set as allow but it can be set with deny or always.\nAs an example, if you send the same request with the stream parameter as true you should see a response like this:\ncurl -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-4\", \"stream\": true }' data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\",\"refusal\":null},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"K3A23CoFUaAJu\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Pi\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"UoRzdjHDE8Evc\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" is\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"Dgg4gQxL4dvw\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" a\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"8g20TYPlKgxRc\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" mathematical\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"aH\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" constant\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"pQcMEi\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" that\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"OKAgP9M4dR\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" represents\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"V8SI\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" the\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"pamC3RsKfeU\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" ratio\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"sq0FRezS1\"} ... data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" repeating\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"NmF1W\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"qLrHCwlWsOhSFI\"} data: {\"id\":\"chatcmpl-CJPyYJDtqoBv2fMHjYyWn8vt9uVne\",\"object\":\"chat.completion.chunk\",\"created\":1758744118,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"obfuscation\":\"ABsseAiaw\"} data: [DONE] Extra Model Options The Kong AI Proxy provides other configuration options. For example:\nmax_tokens: defines the max_tokens, if using chat or completion models. temperature: it is a number between 0 and 5 and it defines the matching temperature, if using chat or completion models. top_p: a number between 0 and 1 defining the top-p probability mass, if supported. top_k: an integer between 0 and 500 defining the top-k most likely tokens, if supported. Kong-gratulations! have now reached the end of this module by caching API responses. You can now click Next to proceed with the next module.",
    "description": "The AI Proxy plugin is the fundamental AI Gateway component. It lets you transform and proxy requests to a number of AI providers and models. The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nThe following table describes which providers and requests the AI Proxy plugin supports:",
    "tags": [],
    "title": "AI Proxy",
    "uri": "/16-ai-gateway/18-use-cases/150-ai-proxy/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "The Konnect Developer Portal is a customizable website for developers to locate, access, and consume API services. It enables developers to browse and search API documentation, try API operations, and manage their own credentials. The Portal supports both internal and external APIs through flexible deployment options.\nDev Portal APIs allow you to publish APIs using OpenAPI or AsyncAPI specifications and Markdown documentation. You can provide highly customizable content at both the site and API level to offer additional context to developers.\nKonnect Developer Portal provides an extensive list of benefits to Developers as well as Organizations:\nDevelopers\nAccelerates Onboarding: Self-service and instant access to documentation of APIs and testing tools Empowers Innovation through discovery of APIs and usage instructions Build applications faster Foster culture of innovation Improve Developer Experience Discovery of APIs In browser testing and troubleshooting tools Organizations\nDrives API Adoption Public or Partner facing portal markets APIs, fueling innovation and new revenue streams Reduces Support Overhead Comprehensive and searchable documentation and self-service tools shifts the burden of support Ensures Governance and Security Portal acts as single source of truth Ensures developers are using the correct, approved versions of APIs and adhering to policies Enhances Collaboration This section is divided into two parts:\nCreate a Developer Portal and publish an API Use Developer self-service capability to create and consume Portal Applications.",
    "description": "The Konnect Developer Portal is a customizable website for developers to locate, access, and consume API services. It enables developers to browse and search API documentation, try API operations, and manage their own credentials. The Portal supports both internal and external APIs through flexible deployment options.\nDev Portal APIs allow you to publish APIs using OpenAPI or AsyncAPI specifications and Markdown documentation. You can provide highly customizable content at both the site and API level to offer additional context to developers.",
    "tags": [],
    "title": "Konnect Developer Portal",
    "uri": "/15-developer-portal/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "Proxy Caching provides a reverse proxy cache implementation for Kong. It caches response entities based on configurable response code and content type, as well as request method. It can cache per-Consumer or per-API. Cache entities are stored for a configurable period of time, after which subsequent requests to the same resource will re-fetch and re-store the resource. Cache entities can also be forcefully purged via the Admin API prior to their expiration time.\nKong Gateway Plugin list Before enabling the Proxy Caching, let’s check the list of plugins Konnect provides. Inside the serverless-default Control Plane, click on Plugins menu option and + New plugin. You should the following page with all plugins available:\nEnabling a Kong Plugin on a Kong Service Create another declaration with plugins option. With this option you can enable and configure the plugin on your Kong Service.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.konghq.com port: 80 plugins: - name: proxy-cache instance_name: proxy-cache1 config: strategy: memory cache_ttl: 30 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route EOF For the plugin configuration we used the following settings:\nstrategy with memory. The plugin will use the Runtime Instance’s memory to implement to cache. cache_ttl with 30, which means the plugin will clear all data that reached this time limit. All plugin configuration paramenters are described inside Kong Plugin Hub portal, in its specific documentation page.\nSubmit the new declaration deck gateway sync --konnect-token $PAT httpbin.yaml Expected Output\ncreating plugin proxy-cache for service httpbin-service Summary: Created: 1 Updated: 0 Deleted: 0 Consume the Service If you consume the service again, you’ll see some new headers describing the caching status:\ncurl -i $DATA_PLANE_URL/httpbin-route/get HTTP/2 200 content-type: application/json content-length: 500 x-kong-request-id: abd3f90c6ecbbb0a0939fb2edab2b40d x-cache-key: f44e43eff1a09eeb35e0436a117f95f9363267d79089b8bdd950f85ca6247e97 x-cache-status: Miss server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:16:15 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 13 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Host\": \"httpbin.konghq.com\", \"User-Agent\": \"curl/8.7.1\", \"X-Forwarded-Host\": \"kong-cceb6a93c9usmc2hk.kongcloud.dev\", \"X-Forwarded-Path\": \"/httpbin-route/get\", \"X-Forwarded-Prefix\": \"/httpbin-route\", \"X-Kong-Request-Id\": \"abd3f90c6ecbbb0a0939fb2edab2b40d\" }, \"origin\": \"186.204.54.49, 66.51.127.198, 172.16.12.194\", \"url\": \"https://kong-cceb6a93c9usmc2hk.kongcloud.dev/get\" } Notice that, for the first request we get Miss for the X-Cache-Status header, meaning that the Runtime Instance didn’t have any data avaialble in the cache and had to connect to the Upstream Service, httpbin.org.\nIf we send a new request, the Runtime Instance has all it needs to satify the request, therefore the status is Hit. Note that the latency time has dropped considerably.\n% curl -i $DATA_PLANE_URL/httpbin-route/get HTTP/2 200 content-type: application/json x-kong-request-id: 4ad5c907f84c167c3eb3f716200ae17c x-cache-key: f44e43eff1a09eeb35e0436a117f95f9363267d79089b8bdd950f85ca6247e97 date: Tue, 23 Sep 2025 12:16:46 GMT server: gunicorn/19.9.0 age: 3 x-cache-status: Hit access-control-allow-origin: * access-control-allow-credentials: true content-length: 500 x-kong-upstream-latency: 0 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Host\": \"httpbin.konghq.com\", \"User-Agent\": \"curl/8.7.1\", \"X-Forwarded-Host\": \"kong-cceb6a93c9usmc2hk.kongcloud.dev\", \"X-Forwarded-Path\": \"/httpbin-route/get\", \"X-Forwarded-Prefix\": \"/httpbin-route\", \"X-Kong-Request-Id\": \"70184e54f9235642a310362396089529\" }, \"origin\": \"186.204.54.49, 66.51.127.198, 172.16.12.194\", \"url\": \"https://kong-cceb6a93c9usmc2hk.kongcloud.dev/get\" } Enabling a Kong Plugin on a Kong Route Now, we are going to define a Rate Limiting policy for our Service. This time, you are going to enable the Rate Limiting plugin to the Kong Route, not to the Kong Gateway Service. In this sense, new Routes defined for the Service will not have the Rate Limiting plugin enabled, only the Proxy Caching.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.konghq.com port: 80 plugins: - name: proxy-cache config: strategy: memory cache_ttl: 30 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 3 EOF The configuration includes:\nminute as 3, which means the Route can be consumed only 3 times a given minute. Submit the declaration deck gateway sync --konnect-token $PAT httpbin.yaml Consume the Service If you consume the service again, you’ll see, besides the caching related headers, new ones describing the status of current rate limiting policy:\ncurl -i $DATA_PLANE_URL/httpbin-route/get HTTP/2 200 content-type: application/json content-length: 500 x-kong-request-id: dc73a70617cde444eada947550425656 ratelimit-limit: 3 ratelimit-remaining: 2 x-ratelimit-limit-minute: 3 x-ratelimit-remaining-minute: 2 ratelimit-reset: 29 x-cache-key: f44e43eff1a09eeb35e0436a117f95f9363267d79089b8bdd950f85ca6247e97 x-cache-status: Miss server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:19:31 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 8 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Host\": \"httpbin.konghq.com\", \"User-Agent\": \"curl/8.7.1\", \"X-Forwarded-Host\": \"kong-cceb6a93c9usmc2hk.kongcloud.dev\", \"X-Forwarded-Path\": \"/httpbin-route/get\", \"X-Forwarded-Prefix\": \"/httpbin-route\", \"X-Kong-Request-Id\": \"dc73a70617cde444eada947550425656\" }, \"origin\": \"186.204.54.49, 66.51.127.198, 172.16.12.194\", \"url\": \"https://kong-cceb6a93c9usmc2hk.kongcloud.dev/get\" } If you keep sending new requests to the Runtime Instance, eventually, you’ll get a 429 error code, meaning you have reached the consumption rate limiting policy for this Route.\n% curl -i $DATA_PLANE_URL/httpbin-route/get HTTP/2 429 date: Tue, 23 Sep 2025 12:19:37 GMT content-type: application/json; charset=utf-8 x-kong-request-id: bf61f204c78bc7404721310d6a35ec11 retry-after: 23 ratelimit-limit: 3 ratelimit-remaining: 0 x-ratelimit-limit-minute: 3 x-ratelimit-remaining-minute: 0 ratelimit-reset: 23 content-length: 92 x-kong-response-latency: 0 server: kong/3.11.0.0-enterprise-edition { \"message\":\"API rate limit exceeded\", \"request_id\":\"bf61f204c78bc7404721310d6a35ec11\" } Enabling a Kong Plugin globally Besides scoping a plugin to a Kong Service or Route, we can apply it globally also. When we do it so, all Services ans Routes will enforce the police described by the plugin.\nFor example, let’s apply the Proxy Caching plugin globally.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route plugins: - name: proxy-cache config: strategy: memory cache_ttl: 30 services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.konghq.com port: 80 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 3 EOF Submit the declaration deck gateway sync --konnect-token $PAT httpbin.yaml Expected output Note the first Proxy Cache instance is deleted to get the Control Plane state synced with the declaratio:\ncreating plugin proxy-cache (global) deleting plugin proxy-cache for service httpbin-service Summary: Created: 1 Updated: 0 Deleted: 1 After testing the configuration, reset the Control Plane:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f",
    "description": "Proxy Caching provides a reverse proxy cache implementation for Kong. It caches response entities based on configurable response code and content type, as well as request method. It can cache per-Consumer or per-API. Cache entities are stored for a configurable period of time, after which subsequent requests to the same resource will re-fetch and re-store the resource. Cache entities can also be forcefully purged via the Admin API prior to their expiration time.",
    "tags": [],
    "title": "Proxy Caching and Rate Limiting",
    "uri": "/12-api-gateway/15-use-cases/150-proxy-caching-rate-limiting/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway",
    "content": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nProxy caching Rate Limiting API Key with Kong Consumers Request Transformer Request Callout OpenID Connect with Kong Identity These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "description": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nProxy caching Rate Limiting API Key with Kong Consumers Request Transformer Request Callout OpenID Connect with Kong Identity These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "tags": [],
    "title": "Use Cases",
    "uri": "/12-api-gateway/15-use-cases/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Developer Portal",
    "content": "Basic Developer Portal implementation In this section we are going to cover the basic steps to get a Developer Portal deployed\nKong Service and Route Create a Kong Service based on http://httpbin.konghq.com Create a Kong Route with path / Globally enable the CORS plugin to your Control Plane. That’s needed to solve the relationship between the Dev Portal and the Upstream Service. Dev Portal creation Choose the Dev Portal menu option and click + Create a new portal. Create a Private Portal named portal1. In the New Portal page accept the default values and click Save Click Go to overview. You should see the Overview page of your portal Check the Portal configuration: User authentication: Konnect built-in - That defines the default mechanism the Dev Portal uses for the user authentication. Besides the build-in option, it can be configured as OIDC or SAML. Default authentication strategy: key-auth - That defines the mechanism to control the API consumption inside the Dev Portal. Test your Dev Portal Sign Up If you click on the Dev Portal URL, you will play the Developer role and see your new portal home page. Since there’s no developer created, click Sing up and register to the Portal. Type your name and use a real email since the Dev Portal will send a confirmation request to it.\nCheck your email and click the Confirm your email address. Still playing the developer role, you should get redirected to the Dev Portal to define your password.\nAfter creating the password, if you try to login, you’ll receive an error message saying “Your account is disabled or pending approval”. That’s because, by default, the Dev Portal was created with the Auto approved developers option disabled, meaning the administrator has to manually approve the new developers registrations.\nApprove the Developer registration Getting back to the Dev Portal Administrator role, return to the Dev Portal menu option and choose Access and approvals. You can approve the new developer registration in the page:\nLogin to the Dev Portal Playing the Developer role again, try to login to the Dev Portal one more time. You should get redirected to the Dev Portal home page. Click the API tab. You are supposed to get an empty page since we don’t have any API published.\nAPI creation Prepare your OpenAPI specification Download the httpbin_spec.yaml OpenAPI specification. From the Konnect Dev Portal perspective, the spec has two main configurations:\nThe servers section. Make sure the url parameters has your Proxy URL: - url: \u003cYOUR_PROXY_URL\u003e Note the spec has added specific DevPortal elements in the security \u0026 securitySchemes sections. That means the DevPortal will use the Key Auth plugin to control the API Consumption inside the Portal. ################################# # Kong DevPortal Security mechanism ################################# security: - ApiKeyAuth: [] ################################# components: securitySchemes: ################################# # Kong Gateway Key-Auth ################################# ApiKeyAuth: type: apiKey in: header name: apikey ################################# Create your API Choose the APIs menu option inside Dev Portal and click + New API. Upload your httpbin_orig.yaml and click Create. You should see your httpbin API page:\nTest your API Click the API Specification tab. Click try it! in the Returns Origin IP\nAdd a documentation Click the Documentation tab. Create a new and empty document page with both name and slug as doc1. Click edit and type some documentation. Click save and switch the Published toggle on.\nAPI Publication Go to the Portals tab and click Publish API. Choose portal1and make sure you have the API visibility set as Private. Click Publish API. You should see the portal1 listed inside the tab.\nYou are going to play the Developer role again. Click on the URL shown in the Portals tab to get redirected to the Dev Portal. Login to it, if needed. Inside the Dev Portal home page, click the APIs tab. You should see the API you’ve just published.\nClick View APIs. You should see the documentation page with the data you entered before. Click Overview to see the httpbin API specification rendered in the page. Click Try it for Returns Origin IP to send a request to the Data Plane and consume the API.",
    "description": "Basic Developer Portal implementation In this section we are going to cover the basic steps to get a Developer Portal deployed\nKong Service and Route Create a Kong Service based on http://httpbin.konghq.com Create a Kong Route with path / Globally enable the CORS plugin to your Control Plane. That’s needed to solve the relationship between the Dev Portal and the Upstream Service. Dev Portal creation Choose the Dev Portal menu option and click + Create a new portal. Create a Private Portal named portal1. In the New Portal page accept the default values and click Save Click Go to overview. You should see the Overview page of your portal Check the Portal configuration: User authentication: Konnect built-in - That defines the default mechanism the Dev Portal uses for the user authentication. Besides the build-in option, it can be configured as OIDC or SAML. Default authentication strategy: key-auth - That defines the mechanism to control the API consumption inside the Dev Portal. Test your Dev Portal Sign Up If you click on the Dev Portal URL, you will play the Developer role and see your new portal home page. Since there’s no developer created, click Sing up and register to the Portal. Type your name and use a real email since the Dev Portal will send a confirmation request to it.",
    "tags": [],
    "title": "Dev Portal creation and API publication",
    "uri": "/15-developer-portal/151-api-publication/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "For Prompt Engineering use cases, Kong AI Gateway provides:\nAI Prompt Decorator plugin: it injects messages at the start or end of a caller’s chat history. AI Prompt Guard plugin: it lets you configure a series of PCRE-compatible regular expressions to allow and block specific prompts, words, phrases, or otherwise and have more control over the LLM. AI Prompt Template plugin: it’s responsible for pre-configuring AI prompts to users All plugins extend the functionality of the AI Proxy plugin, and requires AI Proxy to be configured first.\nYou can now click Next to proceed further.",
    "description": "For Prompt Engineering use cases, Kong AI Gateway provides:\nAI Prompt Decorator plugin: it injects messages at the start or end of a caller’s chat history. AI Prompt Guard plugin: it lets you configure a series of PCRE-compatible regular expressions to allow and block specific prompts, words, phrases, or otherwise and have more control over the LLM. AI Prompt Template plugin: it’s responsible for pre-configuring AI prompts to users All plugins extend the functionality of the AI Proxy plugin, and requires AI Proxy to be configured first.",
    "tags": [],
    "title": "Prompt Engineering",
    "uri": "/16-ai-gateway/18-use-cases/151-prompt-engineering/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "To get started with API Authentication, let’s implement a basic Key Authentication mechanism. API Keys are one of the foundamental security mechanisms provided by Konnect. In order to consume an API, the consumer should inject a previously created API Key in the header of the request. The API consumption is allowed if the Gateway recognizes the API Key. Consumers add their API key either in a query string parameter, a header, or a request body to authenticate their requests and consume the application.\nA Kong Consumer represents a consumer (user or application) of a Service. A Kong Consumer is tightly coupled to an Authentication mechanism the Kong Gateway provides.\nPlease, check the Key-Auth plugin plugin and Kong Consumer documentation pages to learn more about them.\nEnable the Key Authentication Plugin on the Kong Route cat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 EOF deck gateway sync --konnect-token $PAT key-auth.yaml Consume the Route Now, if you try the Route, you’ll get a specific 401 error code meaning that, since you don’t have any API Key injected in your request, you are not allowd to consume it.\ncurl -i $DATA_PLANE_URL/key-auth-route/get HTTP/2 401 date: Tue, 23 Sep 2025 12:28:41 GMT content-type: application/json; charset=utf-8 x-kong-request-id: 5082dc799dcc913c3e27581f84eba120 www-authenticate: Key content-length: 96 x-kong-response-latency: 1 server: kong/3.11.0.0-enterprise-edition { \"message\":\"No API key found in request\", \"request_id\":\"5082dc799dcc913c3e27581f84eba120\" } Create a Kong Consumer In order to consume the Route we need to create a Kong Consumer. Here’s its declaration:\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT key-auth.yaml Consume the Route with the API Key Now, you need to inject the Key you’ve just created, as a header, in your requests. Using HTTPie, you can do it easily like this:\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:123456' HTTP/2 200 content-type: application/json content-length: 702 x-kong-request-id: b54df81da8dde905312cd55d5600f638 server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:29:57 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 10 x-kong-proxy-latency: 2 via: 1.1 kong/3.11.0.0-enterprise-edition Of course, if you inject a wrong key, you get a specific error like this:\n% curl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:12' HTTP/2 401 date: Tue, 23 Sep 2025 12:30:23 GMT content-type: application/json; charset=utf-8 x-kong-request-id: 31c73846e3a250366a42d805f0282b4b www-authenticate: Key content-length: 81 x-kong-response-latency: 1 server: kong/3.11.0.0-enterprise-edition NOTE\nThe header has to have the API Key name, which is, in our case, apikey. That was the default name provided by Konnect when you enabled the Key Authentication on the Kong Route. You can change the plugin configuration, if you will. Kong Consumer Policies With the API Key policy in place, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIt’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:\nconsumer1: apikey = 123456 rate limiting policy = 5 rpm consumer2: apikey = 987654 rate limiting policy = 8 rpm Doing that, the Data Plane is capable to not just protect the Route but to identify the consumer based on the key injected to enforce specific policies to the consumer.\nFor this section we’re implementing a Rate Limiting policy. Keep in mind that a Consumer might have other plugins also enabled such as Request Transformer, TCP Log, etc.\nNew Consumer Create the second consumer2, just like you did with the first one, with the 987654 key.\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 - keyauth_credentials: - key: \"987654\" username: consumer2 EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT key-auth.yaml If you will, you can inject both keys to your requests.\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:123456' or\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:987654' Consumers’ Policy Now let’s enhance the plugins declaration enabling the Rate Limiting plugin to each one of our consumers.\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 5 - keyauth_credentials: - key: \"987654\" username: consumer2 plugins: - name: rate-limiting instance_name: rate-limiting2 config: minute: 8 EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT key-auth.yaml Consumer the Route using different API Keys. First of all let’s consume the Route with the Consumer1’s API Key:\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:123456' Expected Output\nHTTP/2 200 content-type: application/json content-length: 702 x-kong-request-id: 863f8f3fc16dd32bdf8f045465857c7e ratelimit-limit: 5 ratelimit-remaining: 4 x-ratelimit-limit-minute: 5 x-ratelimit-remaining-minute: 4 ratelimit-reset: 35 server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:33:25 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 8 x-kong-proxy-latency: 2 via: 1.1 kong/3.11.0.0-enterprise-edition Now, let’s consume it with the Consumer2’s API Key. As you can see the Data Plane is processing the Rate Limiting processes independently.\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:987654' Expected Output\nHTTP/2 200 content-type: application/json content-length: 702 x-kong-request-id: 3c99310c94ed3157f1cdb4604e0a8c4e ratelimit-limit: 8 ratelimit-remaining: 7 x-ratelimit-limit-minute: 8 x-ratelimit-remaining-minute: 7 ratelimit-reset: 9 server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:33:51 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 9 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition If we keep sending requests using the first API Key, eventually, as expected, we’ll get an error code:\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:123456' Expected Output\nHTTP/2 429 date: Tue, 23 Sep 2025 12:34:21 GMT content-type: application/json; charset=utf-8 x-kong-request-id: 3e890328c589ee56b57a0171c4c89267 retry-after: 39 ratelimit-limit: 5 ratelimit-remaining: 0 x-ratelimit-limit-minute: 5 x-ratelimit-remaining-minute: 0 ratelimit-reset: 39 content-length: 92 x-kong-response-latency: 0 server: kong/3.11.0.0-enterprise-edition However, the second API Key is still allowed to consume the Kong Route:\ncurl --head $DATA_PLANE_URL/key-auth-route/get -H 'apikey:987654' Expected Output\nHTTP/2 200 content-type: application/json content-length: 702 x-kong-request-id: 86f7c102950a06452a4e0f66c290f30e ratelimit-limit: 8 ratelimit-remaining: 7 x-ratelimit-limit-minute: 8 x-ratelimit-remaining-minute: 7 ratelimit-reset: 26 server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:34:34 GMT access-control-allow-origin: * access-control-allow-credentials: true x-kong-upstream-latency: 8 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.\nOptional Reading Applying Kong Plugins on Services, Routes or Globally helps us to implement an extensive list of policies in the API Gateway layer. However, so far, we are not controlling who is sending the requests to the Data Plane. That is, anyone who has the Runtime Instance ELB address is capable to send requests to it and consumer the Services.\nAPI Gateway Authentication is an important way to control the data that is allowed to be transmitted using your APIs. Basically, it checks that a particular consumer has permission to access the API, using a predefined set of credentials.\nKong Gateway has a library of plugins that provide simple ways to implement the best known and most widely used methods of API gateway authentication. Here are some of the commonly used ones:\nBasic Authentication Key Authentication OAuth 2.0 Authentication LDAP Authentication OpenID Connect Kong Plugin Hub provides documentation about all Authentication based plugins. Refer to the following link to read more about API Gateway Authentication",
    "description": "To get started with API Authentication, let’s implement a basic Key Authentication mechanism. API Keys are one of the foundamental security mechanisms provided by Konnect. In order to consume an API, the consumer should inject a previously created API Key in the header of the request. The API consumption is allowed if the Gateway recognizes the API Key. Consumers add their API key either in a query string parameter, a header, or a request body to authenticate their requests and consume the application.",
    "tags": [],
    "title": "API Key Authentication and Rate Limiting",
    "uri": "/12-api-gateway/15-use-cases/152-key-authn-rate-limiting/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Developer Portal",
    "content": "This section will explore the Developer self-service and App registration capabilities provided by Konnect Developer Portal.\nSo far, you have your API published in the portal. However, there’s no control over the API consumption. Konnect Dev Portal provides flexible options for controlling access to content and APIs. When combined with a Gateway Service, developers visiting a Dev Portal can sign up, create an application, register it with an API, and retrieve API keys without intervention from Dev Portal administrators.\nDeveloper self-service consists of two main components:\nUser authentication: Allows users to access your Dev Portal by logging in. You can further customize what logged in users can see using RBAC. Application registration: Allows developers to use your APIs using credentials and create applications for them. Link the API to your Gateway Service. When you link a service with an API, Konnect automatically adds the Konnect Application Auth (KAA) plugin on that Service. The KAA plugin is responsible for applying authentication and authorization on the Service. The authentication strategy that you select for the API defines how clients authenticate. After linking to the Konnect Gateway Service, developers can create applications and generate credentials, e.g. API keys.\nPlay the Administrator role again and click on APIs inside the Dev Portal menu option. Choose your API. Click in the Gateway Service tab and link the API to your Kong Gateway Service, created in the serverless-default Control Plane.\nAs a developer, if you try to consume the API from the Dev Portal you are going to get a 401 error code, meaning the Dev Portal is controlling the Authentication mechanism which is, by default, based on API Keys.\nTurn RBAC on in your Portal In order to control the API consumption we are going to turn the RBAC security model in our portal. That will allow to define which developer can consume the API.\nAs an administrator, click the Access and approvals menu option inside your Dev Portal. Click on the Teams tab and create a team, named team1.\nInside your team, click Add developer and add your developer to your team.\nGo to the APIs tab and click + Add role. Choose your API and add the API Conusumer role.\nThat means your team has only one developer who has permissions to consumer your API.\nAs a developer, if you try to consume the API again, you will still get the 401 error code. Create a Portal Application. Play the developer role again. Inside the API page, click “Use this API” and create an application, named app1 (the Auth strategy is the default - API Key Auth). Click Create and use API. Copy the Credential (e.g. vuOeFHUiR9oSc2fDLRvJDrJvd8ZLJJbh) and click Copy and close Add your API Key in the Authentication box. You will still get the 401 error code if you try to consume the API again. Approve the Application As the administrator get back to the Access and approvals menu option inside your portal. Click the App Registration tab and approve the application. As the developer, you should be finally able to consume the API inside the Dev Portal. Check your Application Still as the developer you can check your applications through the self-services provided by the Konnect Dev Portal. Click on the user icon on the top right corner of the Dev Portal page.\nYou should see the applications you’ve created. In our case, there’s only one, app1.\nClick on the application. You will see three tabs avaiable. The first one, APIs, you can see all APIs defined for the application. In our case, only the httpbin API has been used.\nThe second tab, Analytics, provides observability data related to the API consumption.\nThe third tab, Credentials, you can manage your credentials, e.g. delete the existing ones, issue new ones, etc.\nKey Takeaways The Konnect Dev Portal enables you to publish, document, and manage APIs for internal, partner, or public consumption. APIs can be published to the Dev Portal with OpenAPI/AsyncAPI specs and Markdown documentation, and linked to Gateway Services for authentication and self-service. Developers can self-register, create applications, and generate credentials directly from the Dev Portal. All of these actions can be performed via the Konnect UI, providing a user-friendly, centralized management experience for your API ecosystem. Next Steps Explore advanced Dev Portal customization options (branding, custom pages, and layouts). Review analytics for your APIs and API Products in Konnect. Learn about automating API and Dev Portal management using Konnect APIs or Terraform. References:\nKonnect Dev Portal Overview Publish your API to Dev Portal Automate your API catalog with Dev Portal Developer Self-Service and App Registration",
    "description": "This section will explore the Developer self-service and App registration capabilities provided by Konnect Developer Portal.\nSo far, you have your API published in the portal. However, there’s no control over the API consumption. Konnect Dev Portal provides flexible options for controlling access to content and APIs. When combined with a Gateway Service, developers visiting a Dev Portal can sign up, create an application, register it with an API, and retrieve API keys without intervention from Dev Portal administrators.",
    "tags": [],
    "title": "Developer Self-Service and Application Registration",
    "uri": "/15-developer-portal/152-portal-application/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "The Response Transformer plugin modifies the upstream response (e.g. response from the server) before returning it to the client.\nIn this section, you will configure the Response Transformer plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\ncat \u003e response-transformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: response-transformer-route paths: - /response-transformer-route plugins: - name: response-transformer instance_name: response-transformer1 config: add: headers: - demo:injected-by-kong EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT response-transformer.yaml Verify Test to make sure Kong transforms the request to the echo server and httpbin server.\ncurl --head $DATA_PLANE_URL/response-transformer-route/get HTTP/2 200 content-type: application/json content-length: 526 x-kong-request-id: 29f8371c1b1cc446119b7f5df69a6128 server: gunicorn/19.9.0 date: Tue, 23 Sep 2025 12:40:54 GMT access-control-allow-origin: * access-control-allow-credentials: true demo: injected-by-kong x-kong-upstream-latency: 12 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition Expected Results Notice that demo: injected-by-kong is injected in the header.\nCleanup Reset the Control Plane to ensure that the plugins do not interfere with any other modules in the workshop for demo purposes and each workshop module code continues to function independently.\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f In real world scenario, you can enable as many plugins as you like depending on your use cases.\nKong-gratulations! have now reached the end of this module by configuring the Kong Route to include demo: injected-by-kong before responding to the client. You can now click Next to proceed with the next module.",
    "description": "The Response Transformer plugin modifies the upstream response (e.g. response from the server) before returning it to the client.\nIn this section, you will configure the Response Transformer plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\ncat \u003e response-transformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: response-transformer-route paths: - /response-transformer-route plugins: - name: response-transformer instance_name: response-transformer1 config: add: headers: - demo:injected-by-kong EOF Submit the declaration",
    "tags": [],
    "title": "Response Transformer",
    "uri": "/12-api-gateway/15-use-cases/153-response-transformer/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "The Request Callout plugin allows you to insert arbitrary API calls before proxying a request to the upstream service.\nIn this section, you will configure the Request Callout plugin on the Kong Route. Specifically, you will configure the plugin to do the following:\nCall Wikipedia using the “srsearch” header as a parameter. The number of hits found and returned by Wikipeadia is added as a new header to the request. The request is sent to httpbin application which echoes the number of hits. Hit Wikipedia Just to get an idea what the Wikipedia response, send the following request:\ncurl -s \"https://en.wikipedia.org/w/api.php?srsearch=Miles%20Davis\u0026action=query\u0026list=search\u0026format=json\" | jq '.query.searchinfo.totalhits' You should get a number like 43812, which represents the number of total hits related to Miles Davis\nCreate the Request Callout Plugin Take the plugins declaration and enable the Request Callout plugin to the Route.\ncat \u003e request-callout.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: request-callout-route paths: - /request-callout-route plugins: - name: request-callout instance_name: request-callout1 config: callouts: - name: wikipedia request: url: https://en.wikipedia.org/w/api.php method: GET query: forward: true by_lua: local srsearch = kong.request.get_header(\"srsearch\"); local srsearch_encoded = ngx.escape_uri(srsearch) query = \"srsearch=\" .. srsearch_encoded .. \"\u0026action=query\u0026list=search\u0026format=json\"; kong.log.inspect(query); kong.ctx.shared.callouts.wikipedia.request.params.query = query response: body: decode: true by_lua: kong.service.request.add_header(\"wikipedia-total-hits-header\", kong.ctx.shared.callouts.wikipedia.response.body.query.searchinfo.totalhits) EOF cat \u003e request-callout.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: request-callout-route paths: - /request-callout-route plugins: - name: request-callout instance_name: request-callout1 config: callouts: - name: dummyjson request: url: https://dummyjson.com/users/1 method: GET headers: forward: false custom: User-Agent: kong-request-callout-demo by_lua: | local user_id = kong.request.get_header(\"X-User-ID\") if not user_id then kong.log.err(\"Missing X-User-ID header\") return end kong.ctx.shared.callouts.dummyjson.request.params.url = \"https://dummyjson.com/users/\" .. tostring(user_id) response: body: decode: true by_lua: | local c = kong.ctx.shared.callouts.dummyjson if not c or not c.response or not c.response.body then kong.log.err(\"callout dummyjson failed or empty response\") return end local email = c.response.body.email if email then kong.service.request.add_header(\"user-email\", tostring(email)) else kong.log.err(\"email not found in dummyjson response\") end EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT request-callout.yaml Verify Send the request to Kong and check the response\ncurl -s \"$DATA_PLANE_URL/request-callout-route/get\" -H srsearch:\"Miles Davis\" | jq { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Content-Length\": \"0\", \"Host\": \"httpbin.kong.svc.cluster.local:8000\", \"Srsearch\": \"Miles Davis\", \"User-Agent\": \"curl/8.7.1\", \"Wipikedia-Total-Hits-Header\": \"43555\", \"X-Forwarded-Host\": \"127.0.0.1\", \"X-Forwarded-Path\": \"/request-callout-route/get\", \"X-Forwarded-Prefix\": \"/request-callout-route\", \"X-Kong-Request-Id\": \"6e4df528567f446630c6ae5c0b461c2e\" }, \"origin\": \"10.244.0.1\", \"url\": \"http://httpbin.kong.svc.cluster.local:8000/get\" } Expected Results Notice that new Wikipedia-Total-Hits-Header header is injected.\nCleanup Reset the Control Plane to ensure that the plugins do not interfere with any other modules in the workshop for demo purposes and each workshop module code continues to function independently.\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f In real world scenario, you can enable as many plugins as you like depending on your use cases.\nKong-gratulations! have now reached the end of this module. You can now click Next to proceed with the next module.",
    "description": "The Request Callout plugin allows you to insert arbitrary API calls before proxying a request to the upstream service.\nIn this section, you will configure the Request Callout plugin on the Kong Route. Specifically, you will configure the plugin to do the following:\nCall Wikipedia using the “srsearch” header as a parameter. The number of hits found and returned by Wikipeadia is added as a new header to the request. The request is sent to httpbin application which echoes the number of hits. Hit Wikipedia Just to get an idea what the Wikipedia response, send the following request:",
    "tags": [],
    "title": "Request Callout",
    "uri": "/12-api-gateway/15-use-cases/154-request-callout/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Request Transformer and AI Response Transformer plugins integrate with the LLM on Amazon Bedrock, enabling introspection and transformation of the request’s body before proxying it to the Upstream Service and prior to forwarding the response to the client.\nThe plugins support llm/v1/chat style requests for all of the same providers as the AI Proxy plugin. It also use all of the same configuration and tuning parameters as the AI Proxy plugin, under the config.llm block.\nThe AI Request Transformer plugin runs before all of the AI Prompt plugins and the AI Proxy plugin, allowing it to also introspect LLM requests against the same, or a different, LLM. On the other hand, the AI Response Transformer plugin runs after the AI Proxy plugin, and after proxying to the Upstream Service, allowing it to also introspect LLM responses against the same, or a different, LLM service.\nThe diagram shows the journey of a consumer’s request through Kong Gateway to the backend service, where it is transformed by both an AI LLM service and Kong’s AI Request Transformer and the AI Response Transformer plugins.\nFor each plugin the configuration and usage processes are:\nThe Kong Gateway admin sets up an llm: configuration block, following the same configuration format as the AI Proxy plugin, and the same driver capabilities. The Kong Gateway admin sets up a prompt for the request introspection. The prompt becomes the system message in the LLM chat request, and prepares the LLM with transformation instructions for the incoming user request body (for the AI Request Transformer plugin) and for the returning upstream response body (for the AI Response Transformer plugin) The user makes an HTTP(S) call. Before proxying the user’s request to the backend, Kong Gateway sets the entire request body as the user message in the LLM chat request, and then sends it to the configured LLM service. After receiving the response from the backend, Kong Gateway sets the entire response body as the user message in the LLM chat request, then sends it to the configured LLM service. The LLM service returns a response assistant message, which is subsequently set as the upstream request body. The following example is going to apply the plugins to transform both request and reponse when consuming the httpbin Upstream Service.\nNow, configure both plugins. Keep in mind that the plugins are totally independent from each other so, the configuration depends on your use case.\ncat \u003e ai-request-response-tranformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - llm services: - name: httpbin-service host: httpbin.konghq.com port: 8000 routes: - name: httpbin-route paths: - /httpbin-route plugins: - name: ai-request-transformer instance_name: ai-request-transformer enabled: true config: prompt: In my JSON message, anywhere there is a JSON tag for a \"city\", also add a \"country\" tag with the name of the country in which the city resides. Return me only the JSON message, no extra text.\" llm: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-response-transformer instance_name: ai-response-transformer enabled: true config: prompt: For any city name, add its current temperature, in brackets next to it. Reply with the JSON result only. llm: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-request-response-tranformer.yaml curl -s -X POST \\ --url $DATA_PLANE_URL/httpbin-route/post \\ --header 'Content-Type: application/json' \\ --data '{ \"user\": { \"name\": \"Kong User\", \"city\": \"Tokyo\" } }' | jq Expected output { \"user\": { \"name\": \"Kong User\", \"city\": \"Tokyo [12°C]\", \"country\": \"Japan\" } } Kong-gratulations! have now reached the end of this module by using Kong Gateway to invoke a AWS Lambda function. You can now click Next to proceed with the next chapter.",
    "description": "The AI Request Transformer and AI Response Transformer plugins integrate with the LLM on Amazon Bedrock, enabling introspection and transformation of the request’s body before proxying it to the Upstream Service and prior to forwarding the response to the client.\nThe plugins support llm/v1/chat style requests for all of the same providers as the AI Proxy plugin. It also use all of the same configuration and tuning parameters as the AI Proxy plugin, under the config.llm block.",
    "tags": [],
    "title": "AI Request and Response Transfomers",
    "uri": "/16-ai-gateway/18-use-cases/155-request-response-transformer/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "In this section, you will configure the Key-Auth plugin on the Kong Route to protect Amazon Bedrock.\nAdd Kong Key Authentication plugin and Kong Consumer Add a KongPlugin resource for authentication, specifically the Key-Auth plugin. Note that, besides describing the plugin configuration, the declaration also creates a Kong Consumer, named user1, with an API Key (123456) as its credential.\ncat \u003e ai-key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth1 enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-key-auth.yaml Verify authentication is required New requests now require authentication\ncurl -i -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expect response The response is a HTTP/1.1 401 Unauthorized, meaning the Kong Gateway Service requires authentication.\nHTTP/2 401 date: Wed, 24 Sep 2025 20:14:07 GMT content-type: application/json; charset=utf-8 x-kong-request-id: 95b133433cd56510d863734de2c1a16f www-authenticate: Key content-length: 96 x-kong-response-latency: 1 server: kong/3.11.0.0-enterprise-edition { \"message\":\"No API key found in request\", \"request_id\":\"95b133433cd56510d863734de2c1a16f\" } Send another request with an API key Use the apikey to pass authentication to access the services.\ncurl -i -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' The request should now respond with a HTTP/1.1 200 OK.\nWhen submitting requests, the API Key name is defined, by default, apikey. You can change the plugin configuration, if you will.",
    "description": "In this section, you will configure the Key-Auth plugin on the Kong Route to protect Amazon Bedrock.\nAdd Kong Key Authentication plugin and Kong Consumer Add a KongPlugin resource for authentication, specifically the Key-Auth plugin. Note that, besides describing the plugin configuration, the declaration also creates a Kong Consumer, named user1, with an API Key (123456) as its credential.\ncat \u003e ai-key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth1 enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Key Auth",
    "uri": "/16-ai-gateway/18-use-cases/157-apikey/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "For advanced and enterprise class requirements, OpenID Connect (OIDC) is the preferred option to implement API consumer Authentication. OIDC also provides mechanisms to implement Authorization. In fact, when applying OIDC to secure the APIs, we’re delegating the Authentication process to the external Identity Provider entity.\nOpenID Connect is an authentication protocol built on top of OAuth 2.0 and JWT - JSON Web Token to add login and profile information about the identity who is logged in.\nOAuth 2.0 defines grant types for different use cases. The most common ones are:\nAuthorization Code: for apps running on a web server, browser-based and mobile apps for user authentication. Client Credentials: for application authentication. PKCE - Proof Key for Code Exchange: an extension to the Authorization Code grant. Recommended for SPA or native applications, PKCE acts like a non hard-coded secret. OpenId Connect plugin Konnect provides an OIDC plugin that fully supports the OAuth 2.0 grants. The plugin allows the integration with a 3rd party identity provider (IdP) in a standardized way. This plugin can be used to implement Kong as a (proxying) OAuth 2.0 resource server (RS) and/or as an OpenID Connect relying party (RP) between the client, and the upstream service.\nAs an example, here’s the typical topology and the Authorization Code with PKCE grant:\nConsumer sends a request to Kong Data Plane. Since the API is being protected with the OIDC plugin, the Data Plane redirects the consumer to the IdP. Consumer provides credentials to the Identity Provide (IdP). IdP authenticates the consumer enforcing security policies previously defined. The policies might involve several database technologies (e.g. LDAP, etc.), MFA (Multi-Factor Authentication), etc. After user authentication, IdP redirects the consumer back to the Data Plane with the Authorization Code injected inside the request. Data Plane sends a request to the IdP’s token endpoint with the Authorization Code and gets an Access Token from the IdP. Data Plane routes the request to the upstream service along with the Access Token Once again, it’s important to notice that one of the main benefits provided by an architecture like this is to follow the Separation of Concerns principle:\nIdentity Provider: responsible for User and Application Authentication, Tokenization, MFA, multiples User Databases abstraction, etc. API Gateway: responsible for exposing the Upstream Services and controlling their consumption through an extensive list of policies besides Authentication including Rate Limiting, Caching, Log Processing, etc. In this module, we will configure this plugin to use Kong Identity.\nYou can now click Next to proceed further.",
    "description": "For advanced and enterprise class requirements, OpenID Connect (OIDC) is the preferred option to implement API consumer Authentication. OIDC also provides mechanisms to implement Authorization. In fact, when applying OIDC to secure the APIs, we’re delegating the Authentication process to the external Identity Provider entity.\nOpenID Connect is an authentication protocol built on top of OAuth 2.0 and JWT - JSON Web Token to add login and profile information about the identity who is logged in.",
    "tags": [],
    "title": "OpenID Connect",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "With the existing API Key policy, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIn this section, we are going to define specific Rate Limiting policies for each Consumer represented by its API Key.\nKong Consumer Policies It’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:\nconsumer1: apikey = 123456 rate limiting policy = 500 tokens per minute consumer2: apikey = 987654 rate limiting policy = 10000 tokens per minute Doing that, the Data Plane is capable to not just protect the Route but to identify the consumer based on the key injected to enforce specific policies to the consumer. Keep in mind that a Consumer might have other plugins also enabled such as TCP Log, etc.\nNew Consumer and AI Rate Limiting Advanced plugin Policies Then, create the second consumer2, just like you did with the first one, with the 987654 key. Both Kong Consumers have the AI Rate Limiting Advanced plugin enabled with specific configurations.\ncat \u003e ai-key-auth-rate-limiting-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth1 enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 plugins: - name: ai-rate-limiting-advanced instance_name: ai-rate-limiting-advanced-consumer1 config: llm_providers: - name: openai window_size: - 60 limit: - 500 - keyauth_credentials: - key: \"987654\" username: user2 plugins: - name: ai-rate-limiting-advanced instance_name: ai-rate-limiting-advanced-consumer2 config: llm_providers: - name: openai window_size: - 60 limit: - 10000 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-key-auth-rate-limiting-advanced.yaml Use both Kong Consumers If you will, you can inject both keys to your requests.\ncurl -i -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected output HTTP/2 200 content-type: application/json x-kong-request-id: e3526809fc3549492e2463168ecae109 x-ai-ratelimit-limit-minute-openai: 500 x-ai-ratelimit-remaining-minute-openai: 500 x-ratelimit-remaining-tokens: 29993 x-ratelimit-reset-requests: 120ms x-ratelimit-reset-tokens: 14ms x-request-id: req_1713c9930dba475a83c14e825e8c4c03 x-openai-proxy-wasm: v0.1 strict-transport-security: max-age=31536000; includeSubDomains; preload server: cloudflare x-content-type-options: nosniff access-control-expose-headers: X-Request-ID alt-svc: h3=\":443\"; ma=86400 openai-organization: user-4qzstwunaw6d1dhwnga5bc5q date: Wed, 24 Sep 2025 20:17:41 GMT openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-ray: 9844f7583ca8447a-EWR openai-version: 2020-10-01 cf-cache-status: DYNAMIC openai-processing-ms: 4861 x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-envoy-upstream-service-time: 5001 x-ratelimit-remaining-requests: 499 x-kong-llm-model: openai/gpt-4.1 content-length: 2087 x-kong-upstream-latency: 5155 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition { \"id\": \"chatcmpl-CJQDgFMHEtVk3h2VotEk31bNLi0am\", \"object\": \"chat.completion\", \"created\": 1758745056, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (born Johnny Allen Hendrix, later renamed James Marshall Hendrix; November 27, 1942 – September 18, 1970) was an American guitarist, singer, and songwriter who is widely regarded as one of the most influential electric guitarists in the history of popular music. \\n\\nHendrix gained fame in the mid-1960s with his band, **The Jimi Hendrix Experience**, and is celebrated for his innovative and experimental use of the electric guitar, pioneering the use of distortion, overdrive, and feedback. Songs like **\\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"The Wind Cries Mary,\\\" and \\\"All Along the Watchtower\\\"** are among his most famous recordings.\\n\\nHis stage presence, technical ability, and creative use of guitar effects revolutionized rock music. Hendrix's performance at the 1969 Woodstock Festival—particularly his iconic rendition of \\\"The Star-Spangled Banner\\\"—has become legendary.\\n\\nHe released only three studio albums during his lifetime: \\n- **Are You Experienced** (1967) \\n- **Axis: Bold as Love** (1967) \\n- **Electric Ladyland** (1968) \\n\\nDespite his brief career—he died at the age of 27—Hendrix's music and style have had a lasting impact, inspiring generations of musicians. He is frequently ranked among the greatest guitarists of all time.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 284, \"total_tokens\": 297, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_3502f4eb73\" } or\ncurl -i -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 987654' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected output HTTP/2 200 content-type: application/json x-kong-request-id: 52a6a066cc4e210a67ab850612ced730 x-ai-ratelimit-limit-minute-openai: 10000 x-ai-ratelimit-remaining-minute-openai: 10000 x-ratelimit-remaining-tokens: 29993 x-ratelimit-reset-requests: 120ms x-ratelimit-reset-tokens: 14ms x-request-id: req_99bf604880844d7c8ca5c47ffd8210ea x-openai-proxy-wasm: v0.1 strict-transport-security: max-age=31536000; includeSubDomains; preload server: cloudflare x-content-type-options: nosniff access-control-expose-headers: X-Request-ID alt-svc: h3=\":443\"; ma=86400 openai-organization: user-4qzstwunaw6d1dhwnga5bc5q date: Wed, 24 Sep 2025 20:18:24 GMT openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-ray: 9844f860ff07447a-EWR openai-version: 2020-10-01 cf-cache-status: DYNAMIC openai-processing-ms: 5501 x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-envoy-upstream-service-time: 5532 x-ratelimit-remaining-requests: 499 x-kong-llm-model: openai/gpt-4.1 content-length: 2003 x-kong-upstream-latency: 5641 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition { \"id\": \"chatcmpl-CJQEMCDo5VIyVDRMI9Q7B951HlWxt\", \"object\": \"chat.completion\", \"created\": 1758745098, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (full name: **James Marshall Hendrix**; born November 27, 1942 – died September 18, 1970) was an American guitarist, singer, and songwriter. He is widely regarded as one of the most influential electric guitarists in the history of popular music and one of the most celebrated musicians of the 20th century.\\n\\nHendrix combined blues, rock, soul, and psychedelia in his innovative guitar style, known for his creative use of feedback, distortion, and other effects. He rose to fame in the 1960s with his band **The Jimi Hendrix Experience**, releasing iconic albums such as **\\\"Are You Experienced\\\"** (1967), **\\\"Axis: Bold as Love\\\"** (1967), and **\\\"Electric Ladyland\\\"** (1968).\\n\\nSome of his most famous songs include:\\n- \\\"Purple Haze\\\"\\n- \\\"All Along the Watchtower\\\" (a Bob Dylan cover)\\n- \\\"Foxy Lady\\\"\\n- \\\"Voodoo Child (Slight Return)\\\"\\n- \\\"The Wind Cries Mary\\\"\\n- \\\"Hey Joe\\\"\\n\\nHendrix's legendary performances, including his rendition of \\\"The Star-Spangled Banner\\\" at **Woodstock** in 1969, helped solidify his status as a rock icon. Despite his short career—he died at age 27—Hendrix's influence on rock, blues, and popular music continues to be profound.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 296, \"total_tokens\": 309, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_3502f4eb73\" } Again, test the rate-limiting policy by executing the following command multiple times and observe the rate-limit headers in the response, specially, X-AI-RateLimit-Limit-minute-openai and X-AI-RateLimit-Remaining-minute-openai:\nNow, let’s consume it with the Consumer1’s API Key. As you can see the Data Plane is processing the Rate Limiting processes independently.\ncurl -i -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/2 200 content-type: application/json x-kong-request-id: 90665c528c6845101ac2eb3b85937540 x-ai-ratelimit-limit-minute-openai: 500 x-ai-ratelimit-remaining-minute-openai: 478 x-ratelimit-remaining-tokens: 29993 x-ratelimit-reset-requests: 120ms x-ratelimit-reset-tokens: 14ms x-request-id: req_2333577fbb0a4041aa6535ceba2e6102 x-openai-proxy-wasm: v0.1 strict-transport-security: max-age=31536000; includeSubDomains; preload server: cloudflare x-content-type-options: nosniff access-control-expose-headers: X-Request-ID alt-svc: h3=\":443\"; ma=86400 openai-organization: user-4qzstwunaw6d1dhwnga5bc5q date: Wed, 24 Sep 2025 20:19:04 GMT openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-ray: 9844f94a4821447a-EWR openai-version: 2020-10-01 cf-cache-status: DYNAMIC openai-processing-ms: 8492 x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-envoy-upstream-service-time: 8543 x-ratelimit-remaining-requests: 499 x-kong-llm-model: openai/gpt-4.1 content-length: 2309 x-kong-upstream-latency: 8627 x-kong-proxy-latency: 1 via: 1.1 kong/3.11.0.0-enterprise-edition If we keep sending requests using the first API Key, eventually, as expected, we’ll get an error code:\ncurl -i -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/2 429 date: Wed, 24 Sep 2025 20:19:45 GMT content-type: application/json; charset=utf-8 x-kong-request-id: 16d3d8047c19485e57837e36e9f2191f x-ai-ratelimit-reset-minute-openai: 29 x-ai-ratelimit-retry-after: 29 x-ai-ratelimit-reset: 29 x-ai-ratelimit-limit-minute-openai: 500 x-ai-ratelimit-remaining-minute-openai: 0 x-ai-ratelimit-retry-after-minute-openai: 29 content-length: 66 x-kong-response-latency: 1 server: kong/3.11.0.0-enterprise-edition {\"message\":\"AI token rate limit exceeded for provider(s): openai\"} However, the second API Key is still allowed to consume the Kong Route:\ncurl -i -X POST \\ --url $DATA_PLANE_URL/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 987654' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/2 200 content-type: application/json x-kong-request-id: e6ee574fbc371d4c54b745507ba08b12 x-ai-ratelimit-limit-minute-openai: 10000 x-ai-ratelimit-remaining-minute-openai: 10000 x-ratelimit-remaining-tokens: 29993 x-ratelimit-reset-requests: 120ms x-ratelimit-reset-tokens: 14ms x-request-id: req_ee7712a38e284ab1a06ddf85f37742da x-openai-proxy-wasm: v0.1 strict-transport-security: max-age=31536000; includeSubDomains; preload server: cloudflare x-content-type-options: nosniff access-control-expose-headers: X-Request-ID alt-svc: h3=\":443\"; ma=86400 openai-organization: user-4qzstwunaw6d1dhwnga5bc5q date: Wed, 24 Sep 2025 20:20:14 GMT openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-ray: 9844fb1a48a2eef5-EWR openai-version: 2020-10-01 cf-cache-status: DYNAMIC openai-processing-ms: 4642 x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-envoy-upstream-service-time: 4669 x-ratelimit-remaining-requests: 499 x-kong-llm-model: openai/gpt-4.1 content-length: 1870 x-kong-upstream-latency: 4784 x-kong-proxy-latency: 2 via: 1.1 kong/3.11.0.0-enterprise-edition { \"id\": \"chatcmpl-CJQGAUQK485uaDMZvu5fqtFWGG6KL\", \"object\": \"chat.completion\", \"created\": 1758745210, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (born **James Marshall Hendrix** on November 27, 1942 – died September 18, 1970) was an American guitarist, singer, and songwriter. Widely regarded as **one of the greatest and most influential electric guitarists in the history of popular music**, Hendrix revolutionized the way the instrument was played, using innovative techniques such as feedback, distortion, and controlled noise.\\n\\nHe first gained fame in the UK after forming the **Jimi Hendrix Experience** in 1966, releasing hit songs like “Hey Joe,” “Purple Haze,” and “The Wind Cries Mary.” His groundbreaking performances at the **Monterey Pop Festival** in 1967 and **Woodstock** in 1969 are legendary, with his rendition of “The Star-Spangled Banner” at Woodstock being especially iconic.\\n\\nHendrix’s music blended rock, blues, psychedelia, and funk, influencing countless musicians. His career was tragically short; he died at age 27. Today, Jimi Hendrix is remembered as a cultural icon and is frequently cited on lists of the greatest guitarists of all time.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 232, \"total_tokens\": 245, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_3502f4eb73\" } Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.",
    "description": "With the existing API Key policy, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIn this section, we are going to define specific Rate Limiting policies for each Consumer represented by its API Key.\nKong Consumer Policies It’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:",
    "tags": [],
    "title": "AI Rate Limiting Advanced",
    "uri": "/16-ai-gateway/18-use-cases/158-rate-limiting/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.\nThe plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nLoad balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:\nLowest-usage Round-robin (weighted) Consistent-hashing (sticky-session on given header value) Semantic routing The AI Proxy Advanced plugin supports semantic routing, which enables distribution of requests based on the similarity between the prompt and the description of each model. This allows Kong to automatically select the model that is best suited for the given domain or use case.\nBy analyzing the content of the request, the plugin can match it to the most appropriate model that is known to perform better in similar contexts. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.\nAs a illustration here is the architecture where we are going to implement the multiple load balancing policies. AI Proxy Advanced will manage both LLMs:\ngpt-4.1 gpt-5 You can now click Next to proceed further.",
    "description": "The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.\nThe plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nLoad balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:",
    "tags": [],
    "title": "AI Proxy Advanced",
    "uri": "/16-ai-gateway/18-use-cases/159-ai-proxy-advanced/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "Introduction With the rapid emergence of multiple AI LLM providers, the AI technology landscape is fragmented and lacking in standards and controls. Kong AI Gateway is a powerful set of features built on top of Kong Gateway, designed to help developers and organizations effectively adopt AI capabilities quickly and securely\nWhile AI providers don’t conform to a standard API specification, the Kong AI Gateway provides a normalized API layer allowing clients to consume multiple AI services from the same client code base. The AI Gateway provides additional capabilities for credential management, AI usage observability, governance, and tuning through prompt engineering. Developers can use no-code AI Plugins to enrich existing API traffic, easily enhancing their existing application functionality.\nYou can enable the AI Gateway features through a set of specialized plugins, using the same model you use for any other Kong Gateway plugin.\nKong AI Gateway functional scope Universal API Kong’s AI Gateway Universal API, delivered through the AI Proxy and AI Proxy Advanced plugins, simplifies AI model integration by providing a single, standardized interface for interacting with models across multiple providers.\nEasy to use: Configure once and access any AI model with minimal integration effort.\nLoad balancing: Automatically distribute AI requests across multiple models or providers for optimal performance and cost efficiency.\nRetry and fallback: Optimize AI requests based on model performance, cost, or other factors.\nCross-plugin integration: Leverage AI in non-AI API workflows through other Kong Gateway plugins.\nHigh Level Tasks You will complete the following:\nSet up Kong AI Proxy for LLM Integration Implement Kong AI Plugins to secure prompt message You can now click Next to proceed further.\nOptional Reading Learn more about Kong AI Gateway\nYou can now click Next to begin the module.",
    "description": "Introduction With the rapid emergence of multiple AI LLM providers, the AI technology landscape is fragmented and lacking in standards and controls. Kong AI Gateway is a powerful set of features built on top of Kong Gateway, designed to help developers and organizations effectively adopt AI capabilities quickly and securely\nWhile AI providers don’t conform to a standard API specification, the Kong AI Gateway provides a normalized API layer allowing clients to consume multiple AI services from the same client code base. The AI Gateway provides additional capabilities for credential management, AI usage observability, governance, and tuning through prompt engineering. Developers can use no-code AI Plugins to enrich existing API traffic, easily enhancing their existing application functionality.",
    "tags": [],
    "title": "Kong AI Gateway",
    "uri": "/16-ai-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway",
    "content": "AI Manager in Konnect provides a unified control plane to create, manage, and monitor LLMs using the Konnect platform.\nKey features include:\nRouting and load balancing: Assign Gateway Services and define how traffic is distributed across models. Streaming and authentication: Enable streaming responses and manage authentication through the AI Gateway. Access control: Create and apply access tiers to control how clients interact with LLMs. Usage analytics: Monitor request and token volumes, track error rates, and measure average latency with historical comparisons. Visual traffic maps: Explore interactive maps that show how requests flow between clients and models in real time. You can now click Next to begin the module.",
    "description": "AI Manager in Konnect provides a unified control plane to create, manage, and monitor LLMs using the Konnect platform.\nKey features include:\nRouting and load balancing: Assign Gateway Services and define how traffic is distributed across models. Streaming and authentication: Enable streaming responses and manage authentication through the AI Gateway. Access control: Create and apply access tiers to control how clients interact with LLMs. Usage analytics: Monitor request and token volumes, track error rates, and measure average latency with historical comparisons. Visual traffic maps: Explore interactive maps that show how requests flow between clients and models in real time. You can now click Next to begin the module.",
    "tags": [],
    "title": "Konnect AI Gateway User Interface",
    "uri": "/16-ai-gateway/17-konnect-ui/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Konnect AI Gateway User Interface",
    "content": "Create the AI Gateway The AI Gateway menu option lets you to expose an existing LLM. Click on the option and choose Start from scratch option inside the New AI Gateway button.\nInside the New AI Gateway page do the following:\nName your new AI Gateway as ai_gateway_1. Every AI Gateway has to be related to an existing Control Plane, so, for the Select gateway box, choose your serverless-default Control Plane. Define a basic Route with /openai-route as its Path. Keep all other parameter with their default values and click Save Connect LLM Inside the Overview page click Connect LLM* and do the following:\nFor the Connect to LLM popup box leave the LLM Provider box with its default values, OpenAI. Open the box if you want to check all LLMs supported by Kong AI Gateway. We are going to expose the OpenAI’s gpt-4 model, so, for the Enter a model box, also leave its default value. Finally, for the API Key box, paste your OpenAI’s API Key. Note you can store you API Key in you vault and leverage the vault support provided by Konnect. Click Save. You should get redirected to the Overview with the new AI Gateway configured:\nConsume the AI Gateway For now, we are not going to apply any AI based plugin so, inside the Overview page, click Test your setup.\nUse the Copy button to copy the request, paste it into your terminal and send it to your Data Plane:\ncurl -X POST https://kong-cceb6a93c9usmc2hk.kongcloud.dev/openai-route \\ -H 'Content-Type: application/json' \\ -d '{ \"messages\": [ { \"role\": \"user\", \"content\": \"How does Kong AI Gateway work?\" } ] }' Behind the scenes The AI Gateway Konnect UI is the easiest way to configure your Control Plane with new AI Gateway and plugins. Behind the scenes, Konnect creates all Kong Objects required to implement the AI Gateway. For example, if you get back to your API Gateway menu option, you’ll see you have three Kong Object defined:\nKong Gateway Service Kong Route, with the your Route configuration, including the /openai-route path. Kong Plugin with the Kong AI Proxy Advanced plugin with the necessary configuration to hit OpenAI and consume its gpt-4 model. For example, here’s your Kong AI Proxy Advanced configuration page:\nThat means you can use the same mechanisms you did for the API Gateway plugins, including the same Konnect UI and decK declarations, to configure the AI based plugins.\nWe are going to explore the AI Proxy and AI Proxy Advanced and other AI based plugins in the next sections.\nClick Next to proceed with the next module.",
    "description": "Create the AI Gateway The AI Gateway menu option lets you to expose an existing LLM. Click on the option and choose Start from scratch option inside the New AI Gateway button.\nInside the New AI Gateway page do the following:\nName your new AI Gateway as ai_gateway_1. Every AI Gateway has to be related to an existing Control Plane, so, for the Select gateway box, choose your serverless-default Control Plane. Define a basic Route with /openai-route as its Path. Keep all other parameter with their default values and click Save",
    "tags": [],
    "title": "AI Manager",
    "uri": "/16-ai-gateway/17-konnect-ui/171-ai-manager/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Konnect AI Gateway User Interface",
    "content": "As a simple example, let’s implement a Prompt Engineering policy adding a Decorator to our Kong Gateway Service.\nChoose your AI Gateway your created in the previous section.\nUsing the Konnect AI Manager UI we have two options:\nInside the Overview page, there’s a Add plugin option. If you click on it you see a popup window to configure the Decorator.\nInside the AI Gateway menu, there’s a Prompt Engineering option. If you click on it you’ll be able to configure two kinds of Prompt Engineering policies: Templates and Decorators\nWe are going to use the option #2. So, click the Decorators tab and + Add decorator.\nInside the New prompt decorator page, choose Start from scratch and type You will always respond in Italian. Click Save\nCopy the request again and send it to the Data Plane. You should receive something like:\n{ \"id\": \"chatcmpl-CK2oV3pSAWxXXzClbHOLEkCiXb8Zx\", \"object\": \"chat.completion\", \"created\": 1758893411, \"model\": \"gpt-4-0613\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Kong AI Gateway funziona come un intermediario tra le applicazioni client e dei servizi di backend. Funziona gestendo e dirigendo il traffico di rete a questi servizi. Inoltre, offre funzionalità come autenticazione, rate limiting, logging, trasformazioni di richieste e risposte, e molte altre funzioni utili. Kong AI Gateway può essere personalizzato per adattarsi alle diverse esigenze dell'azienda attraverso l'uso di plugin.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 24, \"completion_tokens\": 110, \"total_tokens\": 134, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": null } Click Next to proceed with the next module.",
    "description": "As a simple example, let’s implement a Prompt Engineering policy adding a Decorator to our Kong Gateway Service.\nChoose your AI Gateway your created in the previous section.\nUsing the Konnect AI Manager UI we have two options:\nInside the Overview page, there’s a Add plugin option. If you click on it you see a popup window to configure the Decorator.\nInside the AI Gateway menu, there’s a Prompt Engineering option. If you click on it you’ll be able to configure two kinds of Prompt Engineering policies: Templates and Decorators",
    "tags": [],
    "title": "Prompt Engineering - Decorator",
    "uri": "/16-ai-gateway/17-konnect-ui/172-decorator/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway",
    "content": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer. We are going to use decK declarations to configure the Gateway and Plugins.\nSimple AI Gateway Proxy Prompt Engineering AI Request and Response Tranformers AI Rate Limiting AI Proxy Advanced and load balancing algoritms These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong AI plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "description": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer. We are going to use decK declarations to configure the Gateway and Plugins.\nSimple AI Gateway Proxy Prompt Engineering AI Request and Response Tranformers AI Rate Limiting AI Proxy Advanced and load balancing algoritms These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong AI plugins on the Plugin Hub.",
    "tags": [],
    "title": "Use Cases",
    "uri": "/16-ai-gateway/18-use-cases/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "Basically Konnect provides two main Builtin Observability services:\nKonnect Advanced Analytics: it is a real-time, contextual analytics platform that provides insights into API health, performance, and usage.\nKonnect Debugger: it provides a connected debugging experience and real-time trace-level visibility into API traffic.",
    "description": "Basically Konnect provides two main Builtin Observability services:\nKonnect Advanced Analytics: it is a real-time, contextual analytics platform that provides insights into API health, performance, and usage.\nKonnect Debugger: it provides a connected debugging experience and real-time trace-level visibility into API traffic.",
    "tags": [],
    "title": "Konnect Builtin Observability",
    "uri": "/20-observability/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Builtin Observability",
    "content": "Konnect Advanced Analytics is a real-time, highly contextual analytics platform that provides deep insights into API health, performance, and usage. It helps businesses optimize their API strategies and improve operational efficiency. This feature is offered as a premium service within Konnect.\nKey benefits:\nCentralized visibility: Gain insights across all APIs, Services, and data planes. Contextual API analytics: Analyze API requests, Routes, Consumers, and Services. Democratized data insights: Generate reports based on your needs. Fast time to insight: Retrieve critical API metrics in less than a second. Reduced cost of ownership: A turn-key analytics solution without third-party dependencies. Enabling data ingestion Manage data ingestion from any Control Plane Dashboard using the Advanced Analytics toggle. This toggle lets you enable or disable data collection for your API traffic per control plane.\nModes:\nOn: Both basic and advanced analytics data is collected, allowing in-depth insights and reporting. Off: Advanced analytics collection stops, but basic API metrics remain available in Gateway Manager, and can still be used for custom reports. Explorer Interface The Explorer interface displays API usage data gathered by Konnect Analytics from your Data Plane nodes. You can use this tool to:\nDiagnose performance issues Monitor LLM token consumption and costs Capture essential usage metrics The Analytics Explorer also lets you save the output as a custom report.\nCheck the Advanced Analytics Explorer documentation to learn more.\nDashboards The Summary Dashboard shows performance and health statistics of all your APIs across your organization on a single page and provides insights into your Service usage.\nCustom Dasboards Advanced Analytics includes the ability to build organization-specific views with Custom Dashboards. You can create them from scratch or use existing templates. The functionality is powered by a robust API, and Terraform integration.\nCreate a dashboard You can create custom dashboards either from scratch or from a template. In this tutorial, we’ll use a template.\nTo create a custom dashboard, do the following:\nIn Konnect, navigate to Dashboards in the sidebar. From the Create dashboard dropdown menu, select “Create from template”. Click Quick summary dashboard. Click Use template. This creates a new template with pre-configured tiles. Add a filter Filters help you narrow down the data shown in charts without modifying individual tiles.\nFor this example, let’s add a filter so that the data shown in the dashboard is scoped to only one control plane:\nFrom the dashboard, click Add filter. This brings up the configuration options. Select “Control plane” from the Filter by dropdown menu. Select “In” from the Operator dropdown menu. Select “kong-workshop from the Filter value dropdown menu. Select the Make this a preset for all viewers checkbox. Click Apply. This applies the filter to the dashboard. Anyone that views this dashboard will be viewing it scoped to the filter you created.\nCheck the Advanced Analytics Custom Dashboards documentation to learn more.\nRequests The Requests options shows all requests that have been processed by the Data Planes. For example, here’s the requests processed by the Data Planes created for the kong-workshop Control Plane.",
    "description": "Konnect Advanced Analytics is a real-time, highly contextual analytics platform that provides deep insights into API health, performance, and usage. It helps businesses optimize their API strategies and improve operational efficiency. This feature is offered as a premium service within Konnect.\nKey benefits:\nCentralized visibility: Gain insights across all APIs, Services, and data planes. Contextual API analytics: Analyze API requests, Routes, Consumers, and Services. Democratized data insights: Generate reports based on your needs. Fast time to insight: Retrieve critical API metrics in less than a second. Reduced cost of ownership: A turn-key analytics solution without third-party dependencies. Enabling data ingestion Manage data ingestion from any Control Plane Dashboard using the Advanced Analytics toggle. This toggle lets you enable or disable data collection for your API traffic per control plane.",
    "tags": [],
    "title": "Konnect Advanced Analytics",
    "uri": "/20-observability/21-builtin/211-advanced_analytics/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Builtin Observability",
    "content": "Konnect Debugger provides a connected debugging experience and real-time trace-level visibility into API traffic, enabling you to:\nTroubleshoot issues: Investigate and resolve problems during deployments or incidents with targeted, on-demand traces. Understand request lifecycle: Visualize exactly what happened during a specific request, including order and duration plugin execution. See Debugger spans for a list of spans captured. Improve performance and reliability: Use deep insights to fine-tune configurations and resolve bottlenecks. Capture traces and logs Konnect Debugger allows you to capture traces and logs.\nTraces Traces provide a visual representation of the request and response lifecycle, offering a comprehensive overview of Kong’s request processing pipeline.\nThe debugger helps capture OpenTelemetry-compatible traces for all requests matching the sampling criteria. The detailed spans are captured for the entire request/response lifecycle. These traces can be visualized with Konnect’s built-in span viewer with no additional instrumentation or telemetry tools. For a complete list of available spans and their meanings, see Debugger spans.\nKey Highlights Traces can be generated for a service or per route Refined traces can be generated for all requests matching a sampling criteria Sampling criteria can be defined with simple expressions language, for example: http.method == GET Trace sessions are retained for up to 7 days Traces can be visualized in Konnect’s built in trace viewer To ensure consistency and interoperability, tracing adheres to OpenTelemetry naming conventions for spans and attributes, wherever possible.\nLogs For deeper insights, logs can be captured along with traces. When initiating a debug session, administrators can choose to capture logs. Detailed Kong Gateway logs are captured for the duration of the session. These logs are then correlated with traces using trace_id and span_id providing a comprehensive and drill-down view of logs generated during specific trace or span.\nReading traces and logs Traces captured during a debug session can be visualized in debugger’s built-in trace viewer. The trace viewer displays Summary, Spans and Logs view. You can gain instant insights with the summary view while the spans and logs view help you to dive deeper.\nSummary view Summary view helps you visualize the entire API request-response flow in a single glance. This view provides a concise overview of critical latency metrics and a transaction map. The lifecycle map includes the different phases of Kong Gateway and the plugins executed by Kong Gateway on both the request and the response along with the times spent in each phase. Use the summary view to quickly understand the end-to-end API flow, identify performance bottlenecks, and optimize your API strategy.\nSpans view The span view gives you unparalleled visibility into Kong Gateway’s internal workings. This detailed view breaks down into individual spans, providing a comprehensive understanding of:\nKong Gateway’s internal processes and phases Plugin execution and performance Request and response handling For detailed definitions of each span, see Debugger spans. Use the span view to troubleshoot issues, optimize performance, and refine your configuration.\nLogs View A drill-down view of all the logs generated during specific debug session are shown in the logs tab. All the spans in the trace are correlated using trace_id and span_id. The logs can be filtered on log level and spans. Logs are displayed in reverse chronological order. Konnect encrypts all the logs that are ingested. You can further ensure complete privacy and control by using customer-managed encryption keys (CMEK). Use the logs view to quickly troubleshoot and pinpoint issues.\nData Security with Customer-Managed Encryption Keys (CMEK) By default, logs are automatically encrypted using encryption keys that are owned and managed by Konnect. However if you have a specific compliance and regulatory requirements related to the keys that protect your data, you can use the customer-managed encryption keys. This ensures that sensitive data are secured for each organization with their own key and nobody, including Konnect, has access to that data. For more information about how to create and manage CMEK keys, see Customer-Managed Encryption Keys (CMEK).\nStart your first debug session To begin using the Debugger, ensure the following requirements are met:\nYour data plane nodes are running Kong Gateway version 3.9.1 or later. Logs require Kong Gateway version 3.11.0 or later. Your Konnect data planes are hosted using self-managed hybrid, Dedicated Cloud Gateways, or serverless gateways. Kong Ingress Controller or Kong Native Event Proxy Gateways aren’t currently supported. In Gateway Manager, select the control plane that contains the data plane to be traced. In the left navigation menu, click Debugger. Click New session. Define the sampling criteria and click Start Session. Once the session starts, traces will be captured for requests that match the rule. Click a trace to view it in the span viewer.\nEach session can be configured to run for a time between 10 seconds and 30 minutes. Sessions are retained for up to 7 days.\nFor details on defining sampling rules, see Debugger sessions.\nSampling rules Sampling rules help you capture only relevant traffic. Requests that match the defined criteria are included in the session. There are two types:\nBasic sampling rules: Filter by Route or Service. Advanced sampling rules: Use expressions for fine-grained filtering.",
    "description": "Konnect Debugger provides a connected debugging experience and real-time trace-level visibility into API traffic, enabling you to:\nTroubleshoot issues: Investigate and resolve problems during deployments or incidents with targeted, on-demand traces. Understand request lifecycle: Visualize exactly what happened during a specific request, including order and duration plugin execution. See Debugger spans for a list of spans captured. Improve performance and reliability: Use deep insights to fine-tune configurations and resolve bottlenecks. Capture traces and logs Konnect Debugger allows you to capture traces and logs.",
    "tags": [],
    "title": "Konnect Debugger",
    "uri": "/20-observability/21-builtin/212-debugger/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases \u003e OpenID Connect",
    "content": "Kong Identity Kong Identity enables you to use Konnect to generate, authenticate, and authorize API access. You can use Kong Identity to:\nCreate authorization servers per region Issue and validate access tokens Integrate secure authentication into your Kong Gateway APIs Create a Kong Service and Route cat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /httpbin-route EOF Submit the declaration\ndeck gateway sync --konnect-token $PAT httpbin.yaml Client Credentials Grant This next section describe the OAuth Client Credentials grants implemented by Kong Konnect and Kong Identity as the Identity Provider. Let’s start instantiating an Authentication Service in Kong Identity.\nCreate the Authentication Server in Kong Identity Before you can configure any authentication plugin, you must first create an auth server in Kong Identity. The auth server name is unique per each organization and each Konnect region.\nCreate an auth server using the /v1/auth-servers endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"AuthN_Server_1\", \"audience\": \"http://myhttpbin.dev\", \"description\": \"AuthN Server 1\" }' | jq You should get a response like this:\n{ \"audience\": \"http://myhttpbin.dev\", \"created_at\": \"2025-09-23T13:04:47.789958Z\", \"description\": \"AuthN Server 1\", \"id\": \"836fda4d-612c-4faf-9c45-284a0ecd637a\", \"issuer\": \"https://wt3fgfqb8r7fktwe.us.identity.konghq.com/auth\", \"labels\": {}, \"metadata_uri\": \"https://wt3fgfqb8r7fktwe.us.identity.konghq.com/auth/.well-known/openid-configuration\", \"name\": \"AuthN_Server_1\", \"signing_algorithm\": \"RS256\", \"updated_at\": \"2025-09-23T13:04:47.789958Z\" } Check your AuthN Server curl -sX GET \"https://us.api.konghq.com/v1/auth-servers\" \\ -H \"Authorization: Bearer $PAT\" | jq Get the AuthN Server Id:\nexport AUTHN_SERVER_ID=$(curl -sX GET \"https://us.api.konghq.com/v1/auth-servers\" -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].id') Get the Issuer URL:\nexport ISSUER_URL=$(curl -sX GET \"https://us.api.konghq.com/v1/auth-servers\" -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].issuer') Configure the auth server with scopes Configure a scope in your auth server using the /v1/auth-servers/$AUTHN_SERVER_ID/scopes endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/scopes\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"scope1\", \"description\": \"scope1\", \"default\": false, \"include_in_metadata\": false, \"enabled\": true }' | jq Expected response\n{ \"created_at\": \"2025-09-23T13:06:24.252827Z\", \"default\": false, \"description\": \"scope1\", \"enabled\": true, \"id\": \"b71c1192-6416-4933-b913-5358904dd234\", \"include_in_metadata\": false, \"name\": \"scope1\", \"updated_at\": \"2025-09-23T13:06:24.252827Z\" } Export your scope ID:\nexport SCOPE_ID=$(curl -sX GET \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/scopes\" -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].id') Configure the auth server with custom claims Configure a custom claim using the /v1/auth-servers/$AUTHN_SERVER_ID/claims endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/claims\" \\ -H \"Authorization: Bearer $PAT\" \\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"claim1\", \"value\": \"claim1\", \"include_in_token\": true, \"include_in_all_scopes\": false, \"include_in_scopes\": [ \"'$SCOPE_ID'\" ], \"enabled\": true }' | jq Expected output:\n{ \"created_at\": \"2025-09-23T13:06:56.096243Z\", \"enabled\": true, \"id\": \"9b149436-ce85-4fb9-9105-b887546e7b21\", \"include_in_all_scopes\": false, \"include_in_scopes\": [ \"b71c1192-6416-4933-b913-5358904dd234\" ], \"include_in_token\": true, \"name\": \"claim1\", \"updated_at\": \"2025-09-23T13:06:56.096243Z\", \"value\": \"claim1\" } Create a client in the AuthN Server The client is the machine-to-machine credential. In this tutorial, Konnect will autogenerate the client ID and secret, but you can alternatively specify one yourself.\nConfigure the client using the /v1/auth-servers/$AUTHN_SERVER_ID/clients endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/clients\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"client1\", \"grant_types\": [ \"client_credentials\" ], \"allow_all_scopes\": false, \"allow_scopes\": [ \"'$SCOPE_ID'\" ], \"access_token_duration\": 3600, \"id_token_duration\": 3600, \"response_types\": [ \"id_token\", \"token\" ] }' | jq Expected output:\n{ \"access_token_duration\": 3600, \"allow_all_scopes\": false, \"allow_scopes\": [ \"b71c1192-6416-4933-b913-5358904dd234\" ], \"client_secret\": \"8vbywkjyj1zxcgsujljnuge1\", \"created_at\": \"2025-09-23T13:07:23.691662Z\", \"grant_types\": [ \"client_credentials\" ], \"id\": \"fxsn74prsnrcyskm\", \"id_token_duration\": 3600, \"labels\": {}, \"login_uri\": null, \"name\": \"client1\", \"redirect_uris\": [], \"response_types\": [ \"id_token\", \"token\" ], \"token_endpoint_auth_method\": \"client_secret_post\", \"updated_at\": \"2025-09-23T13:07:23.691662Z\" } The Client Secret will not be shown again, so copy both ID and Secret:\nexport CLIENT_ID=\u003cYOUR_CLIENT_ID\u003e export CLIENT_SECRET=\u003cYOUR_CLIENT_SECRET\u003e Configure the OIDC plugin You can configure the OIDC plugin to use Kong Identity as the identity provider for your Gateway Services. In this example, you’ll apply the plugin to the control plane globally, but you can alternatively apply it to the Gateway Service.\nFirst, get the ID of the serverless-default control plane you configured in the prerequisites:\nexport CONTROL_PLANE_ID=$(curl -sX GET \"https://us.api.konghq.com/v2/control-planes?filter%5Bname%5D%5Bcontains%5D=serverless-default\" \\ -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].id') Enable the OIDC plugin globally:\ncurl -sX POST \"https://us.api.konghq.com/v2/control-planes/$CONTROL_PLANE_ID/core-entities/plugins/\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"openid-connect\", \"config\": { \"issuer\": \"'$ISSUER_URL'\", \"auth_methods\": [ \"bearer\" ], \"audience\": [ \"http://myhttpbin.dev\" ] } }' | jq In this example:\nissuer: Setting that connects the plugin to your IdP (in this case, Kong Identity). auth_methods: Specifies that the plugin should use bearer for authentication. Generate a token for the client The Gateway Service requires an access token from the client to access the Service. Generate a token for the client by making a call to the issuer URL:\nexport ACCESS_TOKEN=$(curl -sX POST \"$ISSUER_URL/oauth/token\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=client_credentials\" \\ -d \"client_id=$CLIENT_ID\" \\ -d \"client_secret=$CLIENT_SECRET\" \\ -d \"scope=scope1\" | jq -r '.access_token') Check the token\necho $ACCESS_TOKEN | jwt decode --json - Expected result\n{ \"header\": { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"a01feebd-5bed-45d7-9244-582010807705\" }, \"payload\": { \"aud\": [ \"http://myhttpbin.dev\" ], \"claim1\": \"claim1\", \"client_id\": \"fxsn74prsnrcyskm\", \"exp\": 1758636665, \"iat\": 1758633065, \"iss\": \"https://wt3fgfqb8r7fktwe.us.identity.konghq.com/auth\", \"jti\": \"370cab31-31f4-44da-b0dc-74577b8a5a81\", \"nbf\": 1758633065, \"scope\": \"scope1\", \"sub\": \"fxsn74prsnrcyskm\" } } Access the Gateway service using the token Access the httpbin Gateway Service using the short-lived token generated by the authorization server from Kong Identity:\ncurl -i -X GET \"$DATA_PLANE_URL/httpbin-route/get\" \\ -H \"Authorization: Bearer $ACCESS_TOKEN\" Check the token with:\ncurl -sX GET \"$DATA_PLANE_URL/httpbin-route/get\" \\ -H \"Authorization: Bearer $ACCESS_TOKEN\" | jq -r '.headers.Authorization' | cut -d ' ' -f 2 | jwt decode -j - Expected output\n{ \"header\": { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"a01feebd-5bed-45d7-9244-582010807705\" }, \"payload\": { \"aud\": [ \"http://myhttpbin.dev\" ], \"claim1\": \"claim1\", \"client_id\": \"fxsn74prsnrcyskm\", \"exp\": 1758636665, \"iat\": 1758633065, \"iss\": \"https://wt3fgfqb8r7fktwe.us.identity.konghq.com/auth\", \"jti\": \"370cab31-31f4-44da-b0dc-74577b8a5a81\", \"nbf\": 1758633065, \"scope\": \"scope1\", \"sub\": \"fxsn74prsnrcyskm\" } } Cleaning Up After testing the configuration, reset the Control Plane:\ndeck gateway reset --konnect-control-plane-name serverless-default --konnect-token $PAT -f Delete the AuthN Server:\ncurl -sX DELETE \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID\" \\ -H \"Authorization: Bearer $PAT\"",
    "description": "Kong Identity Kong Identity enables you to use Konnect to generate, authenticate, and authorize API access. You can use Kong Identity to:\nCreate authorization servers per region Issue and validate access tokens Integrate secure authentication into your Kong Gateway APIs Create a Kong Service and Route cat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: serverless-default _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.konghq.com port: 80 routes: - name: httpbin-route paths: - /httpbin-route EOF Submit the declaration",
    "tags": [],
    "title": "Kong Identity",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/1573-kong-identity/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Introduction Kong Konnect is an API lifecycle management platform delivered as a service. The management plane is hosted in the cloud by Kong. The management plane enables customers to securely execute API management activities such as create API routes, define services etc. Runtime environments connect with the management plane using mutual transport layer authentication (mTLS), receive the updates and take customer facing API traffic.\nThe runtime environments can be:\nSelf-managed and deployed in your personal environment Managed by Kong in two options: Dedicated Cloud Gateway: it runs on isolated infrastructure within Kong-managed environments in AWS, Azure, or GCP offering the performance and security of dedicated infrastructure with the operational ease of SaaS. Serverless Gateway: it is a lightweight API gateways, ideal for developers who want to test or experiment in a pre-production environment. Learning Objectives In this workshop, you will:\nGet an architectural overview of Kong Konnect platform.\nSet up Konnect runtime as a Serverless Gateway.\nLearn what are services, routes and plugin.\nDeploy a sample microservice and access the application using the defined route.\nUse the platform to address the following API Gateway use cases\nProxy caching Authentication and Authorization Response Transformer Request Callout Rate limiting Observability And the following AI Gateway use cases\nPrompt Engineering LLM-based Request and Reponse transformation Semantic Caching Token-based Rate Limiting Semantic Routing RAG - Retrieval-Augmented Generation Expected Duration Workshop Introduction (15 minutes) Architectural Walkthrough (10 minutes) Sample Application and initial use case (15 minutes) Addressing API and AI Gateway use cases (90 minutes) Observability (30 minutes) Next Steps and Cleanup (5 min)",
    "description": "Introduction Kong Konnect is an API lifecycle management platform delivered as a service. The management plane is hosted in the cloud by Kong. The management plane enables customers to securely execute API management activities such as create API routes, define services etc. Runtime environments connect with the management plane using mutual transport layer authentication (mTLS), receive the updates and take customer facing API traffic.\nThe runtime environments can be:\nSelf-managed and deployed in your personal environment Managed by Kong in two options: Dedicated Cloud Gateway: it runs on isolated infrastructure within Kong-managed environments in AWS, Azure, or GCP offering the performance and security of dedicated infrastructure with the operational ease of SaaS. Serverless Gateway: it is a lightweight API gateways, ideal for developers who want to test or experiment in a pre-production environment. Learning Objectives In this workshop, you will:",
    "tags": [],
    "title": "API Management with Kong Konnect and Serverless API Gateway",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
