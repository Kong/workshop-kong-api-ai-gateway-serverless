var relearn_searchindex = [
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Decorator plugin adds an array of llm/v1/chat messages to either the start or end of an LLM consumer’s chat history. This allows you to pre-engineer complex prompts, or steer (and guard) prompts in such a way that the modification to the consumer’s LLM message is completely transparent.\nYou can use this plugin to pre-set a system prompt, set up specific prompt history, add words and phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway.\ncat \u003e ai-prompt-decorator.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai options: temperature: 1.0 - name: ai-prompt-decorator instance_name: ai-prompt-decorator-openai config: prompts: prepend: - role: system content: \"You will always respond in the Portuguese (Brazil) language.\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-decorator.yaml Send a request now:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-5\" }' | jq You can now click Next to proceed further.",
    "description": "The AI Prompt Decorator plugin adds an array of llm/v1/chat messages to either the start or end of an LLM consumer’s chat history. This allows you to pre-engineer complex prompts, or steer (and guard) prompts in such a way that the modification to the consumer’s LLM message is completely transparent.\nYou can use this plugin to pre-set a system prompt, set up specific prompt history, add words and phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway.",
    "tags": [],
    "title": "AI Prompt Decorator",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/1-prompt-decorator/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Template plugin lets you provide tuned AI prompts to users. Users only need to fill in the blanks with variable placeholders in the following format: {{variable}}. This lets admins set up templates, which can be then be used by anyone in the organization. It also allows admins to present an LLM as an API in its own right - for example, a bot that can provide software class examples and/or suggestions.\nThis plugin also sanitizes string inputs to ensure that JSON control characters are escaped, preventing arbitrary prompt injection.\nWhen calling a template, simply replace the messages (llm/v1/chat) or prompt (llm/v1/completions) with a template reference, in the following format: {template://TEMPLATE_NAME}\nHere’s an example of template definition:\ncat \u003e ai-prompt-template.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-prompt-template instance_name: ai-prompt-template-openai enabled: true config: allow_untemplated_requests: true templates: - name: template1 template: |- { \"messages\": [ { \"role\": \"user\", \"content\": \"Explain to me what {{thing}} is.\" } ] } EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-template.yaml Now, send a request referring the template:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": \"{template://template1}\", \"properties\": { \"thing\": \"niilism\" } }' | jq Kong-gratulations! have now reached the end of this module by authenticating your API requests with AWS Cognito. You can now click Next to proceed with the next module.",
    "description": "The AI Prompt Template plugin lets you provide tuned AI prompts to users. Users only need to fill in the blanks with variable placeholders in the following format: {{variable}}. This lets admins set up templates, which can be then be used by anyone in the organization. It also allows admins to present an LLM as an API in its own right - for example, a bot that can provide software class examples and/or suggestions.",
    "tags": [],
    "title": "AI Prompt Template",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/2-prompt-template/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Let’s get started with a simple round-robin policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: round-robin targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Send the request few times. Note we are going to receive responses from both LLMs, in a round-robin way.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq Here’s the OpenAI’s gpt-4.1 response:\n{ \"id\": \"chatcmpl-C3lbQFtYTrEWwq7N6zKWh4CHl3idX\", \"object\": \"chat.completion\", \"created\": 1755014004, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"The title of \\\"greatest Polish writer\\\" is somewhat subjective and can depend on whom you ask and which literary genre or era is in focus. However, several names consistently rise to the top in critical consensus and cultural significance:\\n\\n1. **Adam Mickiewicz (1798–1855):** Often considered the national poet of Poland and a key figure in Romanticism, Mickiewicz is renowned for works such as *Pan Tadeusz* and *Dziady* (Forefathers' Eve). His impact on Polish identity and literature is unparalleled.\\n\\n2. **Henryk Sienkiewicz (1846–1916):** Winner of the Nobel Prize for Literature in 1905, Sienkiewicz is famous for historical novels such as *Quo Vadis* and his \\\"Trilogy\\\" (*With Fire and Sword*, *The Deluge*, *Fire in the Steppe*).\\n\\n3. **Wisława Szymborska (1923–2012):** One of the most celebrated modern poets, Szymborska received the Nobel Prize in 1996. Her poetry is noted for its wit, irony, and philosophical depth.\\n\\n4. **Czesław Miłosz (1911–2004):** A Nobel Prize-winning poet (1980), essayist, and thinker, Miłosz's impact on world literature and Polish thought is immense.\\n\\nGiven the foundational role Adam Mickiewicz played in defining Polish literature and his enduring influence, **Mickiewicz is most frequently regarded as the greatest Polish writer, particularly in terms of cultural and historical significance**.\\n\\nHowever, for different periods or genres, Sienkiewicz, Szymborska, or Miłosz may be regarded as the greatest by some readers or scholars.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 15, \"completion_tokens\": 364, \"total_tokens\": 379, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_799e4ca3f1\" } And here is llama3.2:1b’s:\n{ \"object\": \"chat.completion\", \"model\": \"llama3.2:1b\", \"choices\": [ { \"index\": 0, \"finish_reason\": \"stop\", \"message\": { \"content\": \"The greatest Polish writer is often debated among scholars and literary enthusiasts. However, some of the most highly regarded Polish writers include:\\n\\n1. Juliusz Słowacki (1795-1861): Considered one of the greatest Polish writers, Słowacki was a poet, playwright, and translator. He is known for his lyric poetry and plays that often explored themes of love, nature, and social issues.\\n2. Adam Mickiewicz (1809-1849): A leading figure in Poland's national revival movement, Mickiewicz was a poet, composer, and politician. He wrote the epic poem \\\"Pan Tadeusz,\\\" which is considered one of the greatest Polish works of literature.\\n3. Józef Piłsudski (1867-1935): While not traditionally considered a writer, Piłsudski was a prominent statesman and military leader who played a key role in Poland's struggle for independence. He also wrote two literary novels, \\\"The Tragedy of Maksimilian\\\" and \\\"The Trial of Maximilian.\\\"\\n4. Cyprian Kamil Norwid (1821-1883): A poet, playwright, and critic, Norwid was one of the most influential Polish writers of his time. He is known for his innovative style and exploration of themes such as love, nature, and social justice.\\n5. Henryk Sienkiewicz (1846-1916): Although primarily a novelist and playwright, Sienkiewicz's works often explored themes of politics, history, and morality. His novel \\\"Quo Vadis\\\" is considered one of the greatest Polish novels of all time.\\n\\nHowever, if I had to choose one writer who is widely regarded as the greatest Polish writer, it would be Juliusz Słowacki.\", \"role\": \"assistant\" } } ], \"usage\": { \"completion_tokens\": 370, \"total_tokens\": 403, \"prompt_tokens\": 33 }, \"created\": 1755014233 }",
    "description": "Let’s get started with a simple round-robin policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: round-robin targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Round Robin",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/2-round-robin/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Guard plugin lets you to configure a series of PCRE-compatible regular expressions as allow or deny lists, to guard against misuse of llm/v1/chat or llm/v1/completions requests.\nYou can use this plugin to allow or block specific prompts, words, phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway. It does this by scanning all chat messages (where the role is user) for the specific expressions set. You can use a combination of allow and deny rules to preserve integrity and compliance when serving an LLM service using Kong Gateway.\nFor llm/v1/chat type models: You can optionally configure the plugin to ignore existing chat history, wherein it will only scan the trailing user message. For llm/v1/completions type models: There is only one prompt field, thus the whole prompt is scanned on every request. The plugin matches lists of regular expressions to requests through AI Proxy. The matching behavior is as follows:\nIf any deny expressions are set, and the request matches any regex pattern in the deny list, the caller receives a 400 response. If any allow expressions are set, but the request matches none of the allowed expressions, the caller also receives a 400 response. If any allow expressions are set, and the request matches one of the allow expressions, the request passes through to the LLM. If there are both deny and allow expressions set, the deny condition takes precedence over allow. Any request that matches an entry in the deny list will return a 400 response, even if it also matches an expression in the allow list. If the request does not match an expression in the deny list, then it must match an expression in the allow list to be passed through to the LLM Here’s an example to allow only valid credit cards numbers:\ncat \u003e ai-prompt-guard.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-prompt-guard instance_name: ai-prompt-guard-openai enabled: true config: allow_all_conversation_history: true allow_patterns: - \".*\\\\\\\"card\\\\\\\".*\\\\\\\"4[0-9]{3}\\\\*{12}\\\\\\\"\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-guard.yaml Send a request with a valid credit card pattern:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Validate this card: {\\\"card\\\": \\\"4111************\\\", \\\"cvv\\\": \\\"000\\\"}\" } ] }' | jq '.' Now, send a non-valid number:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Validate this card: {\\\"card\\\": \\\"4111xyz************\\\", \\\"cvv\\\": \\\"000\\\"}\" } ] }' | jq '.' The expect result is:\n{ \"error\": { \"message\": \"bad request\" } }",
    "description": "The AI Prompt Guard plugin lets you to configure a series of PCRE-compatible regular expressions as allow or deny lists, to guard against misuse of llm/v1/chat or llm/v1/completions requests.\nYou can use this plugin to allow or block specific prompts, words, phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway. It does this by scanning all chat messages (where the role is user) for the specific expressions set. You can use a combination of allow and deny rules to preserve integrity and compliance when serving an LLM service using Kong Gateway.",
    "tags": [],
    "title": "AI Prompt Guard",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/3-prompt-guard/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Now, let’s redirect 80% of the request to OpenAI’s gpt-4.1 with a weight based policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} weight: 80 - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" weight: 20 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml",
    "description": "Now, let’s redirect 80% of the request to OpenAI’s gpt-4.1 with a weight based policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} weight: 80 - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" weight: 20 EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Weight",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/3-weight/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.\nCreate a file with the following declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Test the Route again.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq Lowest Usage policy The lowest-usage algorithm in AI Proxy Advanced is based on the volume of usage for each model. It balances the load by distributing requests to models with the lowest usage, measured by factors such as prompt token counts, response token counts, or other resource metrics.\nReplace the declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-usage targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration:\ndeck gateway reset --konnect-control-plane-name kong-aws --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml And test the Route again.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq",
    "description": "Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.\nCreate a file with the following declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Lowest-Latency and Lowest-Usage",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/4-lowest-latency-usage/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Semantic The semantic algorithm distributes requests to different models based on the similarity between the prompt in the request and the description provided in the model configuration. This allows Kong to automatically select the model that is best suited for the given domain or use case. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: semantic embeddings: auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: \"text-embedding-3-small\" vectordb: dimensions: 1024 distance_metric: cosine strategy: redis threshold: 1.0 redis: host: \"redis-stack.redis.svc.cluster.local\" port: 6379 database: 0 targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} description: \"mathematics, algebra, calculus, trigonometry\" - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" description: \"piano, orchestra, liszt, classical music\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Send a request related to Mathematics. The response should come from OpenAI’s gpt-4.1\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Tell me about the last theorem of Fermat\" } ] }' | jq On the other hand, Llama3.1 should be responsible for requests related to Classical Music.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who wrote the Hungarian Rhapsodies piano pieces?\" } ] }' | jq curl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Tell me a contemporaty pianist of Chopin\" } ] }' | jq If you check Redis, you’ll se there are two entries, related to the models\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan Expected output \"ai_proxy_advanced_semantic:01c84f59-b7c3-418b-818d-4369ef3e55ef:8f74aeaab95482bb37fbd69cd42154dcd6d321e1631ffdfd1802e1609d4c2481\" \"ai_proxy_advanced_semantic:01c84f59-b7c3-418b-818d-4369ef3e55ef:72a33ce9079fd34f6fb3624c3a4ba1a0df0c1aad267986db2249dc26a8808a41\"",
    "description": "Semantic The semantic algorithm distributes requests to different models based on the similarity between the prompt in the request and the description provided in the model configuration. This allows Kong to automatically select the model that is best suited for the given domain or use case. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.",
    "tags": [],
    "title": "Semantic Routing",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/5-semantic-routing/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "Knowledge Attendees should have basic knowledge of containers and other products like Keycloak, Redis, Prometheus etc.\nAttendees should have basic knowledge of GenAI models, specially LLMs and Embedding models as well as providers like OpenAI, Mistral, Anthropic, AWS, GCP, Azure.\nKong Academy Complete two badges via Kong Academy: Kong Gateway Foundations and Kong Gateway Operations\nKong Konnect Subscription You will need a Kong Konnect Subscription to execute this workshop.\nEnvironment Please, make sure you have decK installed in your local environment.\nCommand Line Utilities In this workshop, we will use the following command line utilities\ncurl jq yq jwt-cli wget Redis Redis is used in some use cases, including Rate Limiting, Caching, Semantic Routing and RAG.\nLLM Ollama The Kong AI Gateway use cases consume and protect LLMs running on Ollama\nOpenAI Some AI use cases also use OpenAI’s Embeddings and LLMs. Please make sure you have an OpenAI API key.\nRecommended hardware not including Ollama CPU: 4-6 vCPUs Memory: 8-16GB Disk: 30–50GB Recommended hardware including Ollama CPU: 6-8 vCPUs Memory: 12-24GB Disk:\t50-100GB",
    "description": "Knowledge Attendees should have basic knowledge of containers and other products like Keycloak, Redis, Prometheus etc.\nAttendees should have basic knowledge of GenAI models, specially LLMs and Embedding models as well as providers like OpenAI, Mistral, Anthropic, AWS, GCP, Azure.\nKong Academy Complete two badges via Kong Academy: Kong Gateway Foundations and Kong Gateway Operations\nKong Konnect Subscription You will need a Kong Konnect Subscription to execute this workshop.\nEnvironment Please, make sure you have decK installed in your local environment.",
    "tags": [],
    "title": "Prerequisites",
    "uri": "/10-prerequisites/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Prerequisites",
    "content": "Konnect Plus Subscription You will need a Konnect subscription. Follow the steps below to obtain a Konnect Plus subscription. Initially $500 of credits are provided for up to 30 days, after the credits have expired or the time has finished Konnect Plus will charge based on Konnect Plus pricing. Following the workshop instructions will use less than $500 credits. After the workshop has finished the Konnect Plus environment can be deleted.\nClick on the Registration link and present your credentials.\nKonnect will send you an email to confirm the subscription. Click on the link in email to confirm your subscription.\nThe Konnect environment can be accessed via the Konnect log in page.\nAfter logging in create an organisation name, select a region, then answer a few questions.\nCredit available can be monitored though Plan and Usage page.",
    "description": "Konnect Plus Subscription You will need a Konnect subscription. Follow the steps below to obtain a Konnect Plus subscription. Initially $500 of credits are provided for up to 30 days, after the credits have expired or the time has finished Konnect Plus will charge based on Konnect Plus pricing. Following the workshop instructions will use less than $500 credits. After the workshop has finished the Konnect Plus environment can be deleted.",
    "tags": [],
    "title": "Konnect Subscription",
    "uri": "/10-prerequisites/konnect-subscription/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "The Kong Konnect platform provides a cloud control plane (CP), which manages all service configurations. It propagates those configurations to all Runtime control planes, which use in-memory storage. These nodes can be installed anywhere, on-premise or in AWS.\nFor today workshop, we will be focusing on Kong Gateway. Kong Gateway data plane listen for traffic on the proxy port 443 by default. The data plane evaluates incoming client API requests and routes them to the appropriate backend APIs. While routing requests and providing responses, policies can be applied with plugins as necessary.\nKonnect modules Kong Konnect Enterprise features are described in this section, including modules and plugins that extend and enhance the functionality of the Kong Konnect platform.\nControl Plane (Gateway Manager) Control Plane empowers your teams to securely collaborate and manage their own set of runtimes and services without the risk of impacting other teams and projects. Control Plane instantly provisions hosted Kong Gateway control planes and supports securely attaching Kong Gateway data planes from your cloud or hybrid environments.\nThrough the Control Plane, increase the security of your APIs with out-of-the-box enterprise and community plugins, including OpenID Connect, Open Policy Agent, Mutual TLS, and more.\nAI Manager Manage all of your LLMs in a single dashboard providing a unified control plane to create, manage, and monitor LLMs using the Konnect platform. With AI Manager you can assign Gateway Services and define how traffic is distributed across models, enable streaming responses and manage authentication through the AI Gateway, monitor request and token volumes, track error rates, and measure average latency with historical comparisons, etc.\nDev Portal Streamline developer onboarding with the Dev Portal, which offers a self-service developer experience to discover, register, and consume published services from your Service Hub catalog. This customizable experience can be used to match your own unique branding and highlights the documentation and interactive API specifications of your services. Enable application registration to automatically secure your APIs with a variety of authorization providers.\nAnalytics Use Analytics to gain deep insights into service, route, and application usage and health monitoring data. Keep your finger on the pulse of the health of your API products with custom reports and contextual dashboards. In addition, you can enhance the native monitoring and analytics capabilities with Kong Gateway plugins that enable streaming monitoring metrics to third-party analytics providers.\nTeams To help secure and govern your environment, Konnect provides the ability to manage authorization with teams. You can use Konnect’s predefined teams for a standard set of roles, or create custom teams with any roles you choose. Invite users and add them to these teams to manage user access. You can also map groups from your existing identity provider into Konnect teams.\nFurther Reading Gateway Manager AI Manager Dev Portal Analytics",
    "description": "The Kong Konnect platform provides a cloud control plane (CP), which manages all service configurations. It propagates those configurations to all Runtime control planes, which use in-memory storage. These nodes can be installed anywhere, on-premise or in AWS.\nFor today workshop, we will be focusing on Kong Gateway. Kong Gateway data plane listen for traffic on the proxy port 443 by default. The data plane evaluates incoming client API requests and routes them to the appropriate backend APIs. While routing requests and providing responses, policies can be applied with plugins as necessary.",
    "tags": [],
    "title": "Kong Konnect Architectural Overview",
    "uri": "/architecture/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Prerequisites",
    "content": "Install Redis Use the redis-stack Helm Charts to install Redis as our vector database.\nhelm repo add redis-stack https://redis-stack.github.io/helm-redis-stack helm repo update helm install redis-stack redis-stack/redis-stack -n redis --create-namespace Check the installation:\n$ kubectl exec $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-server --version Redis server v=7.4.5 sha=00000000:0 malloc=libc bits=64 build=d2e5921793838dd If you want to uninstall it:\nhelm uninstall redis-stack -n redis kubectl delete namespace redis Install Ollama As our Embedding model, we’re going to consume the “mxbai-embed-large:latest” model handled locally by Ollama. Use the Ollama Helm Charts to install it.\nhelm repo add ollama-helm https://otwld.github.io/ollama-helm/ helm repo update helm install ollama ollama-helm/ollama \\ -n ollama \\ --create-namespace \\ --set ollama.models.pull[0]=\"mxbai-embed-large:latest\" \\ --set ollama.models.pull[1]=\"llama3.2:1b\" \\ --set service.type=LoadBalancer Check the version and models\n$ kubectl exec -it $(kubectl get pod -n ollama -o json | jq -r '.items[].metadata.name') -n ollama -- ollama --version ollama version is 0.11.2 $ kubectl exec -it $(kubectl get pod -n ollama -o json | jq -r '.items[].metadata.name') -n ollama -- ollama list NAME ID SIZE MODIFIED llama3.2:1b baf6a787fdff 1.3 GB 31 minutes ago mxbai-embed-large:latest 468836162de7 669 MB 32 minutes ago Send request to test it:\ncurl -sX POST http://localhost:11434/api/generate -d '{ \"model\": \"llama3.2:1b\", \"prompt\": \"Tell me about Miles Davis\", \"stream\": false }' | jq '.response' Expected response:\n\"Miles Davis (1926-1991) was an American jazz trumpeter, bandleader, and composer. He is widely regarded as one of the most influential musicians in the history of jazz.\\n\\nEarly Life and Career:\\n\\nBorn in Alton, Illinois, Davis grew up in a musical family and began playing trumpet at age five. He studied music theory and composition at the Juilliard School in New York City before serving in the United States Army during World War II.\\n\\nAfter the war, Davis moved to New York City's lower East Side, where he formed his first jazz group with guitarist Red Garland and pianist Cannonball Adderley. In 1954, he joined the cool jazz group Chirps, later renamed Art Bayou's Jazz Experience.\\n\\nThe \\\"Cool\\\" Period:\\n\\nIn 1956, Davis moved to Los Angeles to form a new band with pianist John Coltrane, bassist Charles Mingus, and drummer Bill Evans. This group was known as Miles Davis Quintet and released several critically acclaimed albums, including \\\"Birth of the Cool\\\" (1957) and \\\"Kind of Blue\\\" (1959). The quintet's music marked a turning point in jazz history, with its emphasis on cool, introspective, and expressive playing.\\n\\nIn the early 1960s, Davis began to experiment with more avant-garde and experimental approaches to jazz. He collaborated with pianist Herbie Hancock on the album \\\"Milestones\\\" (1960), which featured a more complex and electronic approach to jazz.\\n\\nLater Years:\\n\\nIn the late 1960s, Davis's playing became increasingly introspective and personal. He released several critically acclaimed albums, including \\\"Bitches Brew\\\" (1970) and \\\"A Tribute to Jack Johnson\\\" (1971). His later work was marked by a more relaxed and improvisational approach, as he explored new musical territories and collaborated with artists from other genres.\\n\\nPersonal Life:\\n\\nDavis's personal life was marked by periods of great creativity and introspection. He had several high-profile relationships, including with actress Joanna Glenn and fashion designer Carole King. In the 1970s, Davis became increasingly interested in Eastern spirituality and meditation, which influenced his later music.\\n\\nDeath:\\n\\nMiles Davis died on September 28, 1991, at the age of 65, due to complications from heart failure. He was buried in the Forest Lawn Memorial Park Cemetery in Glendale, California.\\n\\nLegacy:\\n\\nMiles Davis's legacy is profound and far-reaching. He helped shape the development of cool jazz and improvisational music, and his influence can be heard in countless artists across multiple genres. His innovative approach to jazz has inspired generations of musicians, from John Coltrane to Wayne Shorter and beyond.\\n\\nDavis's music continues to be celebrated for its complexity, depth, and emotional resonance. He remains one of the most beloved and respected figures in jazz history, and his impact on modern music is immeasurable.\" If you want to uninstall it:\nhelm uninstall ollama -n ollama kubectl delete namespace ollama Enable Metrics Server minikube addons enable metrics-server minikube addons list Keycloak and OPA The installation procedures for both servers are available in the OpenID Connect and OPA (Open Policy Agent) sections",
    "description": "Install Redis Use the redis-stack Helm Charts to install Redis as our vector database.\nhelm repo add redis-stack https://redis-stack.github.io/helm-redis-stack helm repo update helm install redis-stack redis-stack/redis-stack -n redis --create-namespace Check the installation:\n$ kubectl exec $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-server --version Redis server v=7.4.5 sha=00000000:0 malloc=libc bits=64 build=d2e5921793838dd If you want to uninstall it:\nhelm uninstall redis-stack -n redis kubectl delete namespace redis Install Ollama As our Embedding model, we’re going to consume the “mxbai-embed-large:latest” model handled locally by Ollama. Use the Ollama Helm Charts to install it.",
    "tags": [],
    "title": "Serverless API Gateway",
    "uri": "/10-prerequisites/redis-ollama-keycloak-opa/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "This chapter will walk you through\nKonnect Control Plane and Data Plane creation. Access Kong Data Plane. Here’s a Reference Architecture that will be implemented in this workshop:\nKong Konnect Control Plane: responsible for managing your APIs Kong Konnect Data Plane: connected to the Control Plane, it is responsible for processing all the incoming requests sent by the consumers. Kong provides a plugin framework, where each one of them is responsible for a specific functionality. As a can see, there are two main collections of plugins: On the left, the historic and regular API Gateway plugins, implementing all sort of policies including, for example, OIDC based Authentication processes with Keycloak, Amazon Cognito and Okta or Observability with Prometheus/Grafana and Dynatrace. On the right, another plugin collection for AI-based use cases. For example, the AI Rate Limiting plugin implements policies like this based on the number of tokens consumed be the requests. Or, as another example is the AI Semantic Cache plugin, which caches data based on the semantics related to the responses coming from the LLM models. Kong AI Gateway supports, out of the box, a variety of infrastructures, including not just OpenAI, but also Amazon Bedrock, Google Gemini, Mistral, Anthropic, etc. In order to deal with embeddings, the Gateway also supports also vector databases. Kong Gateway protects not just the LLM Models but also the upstream services, including your application micros surfaces or services. Konnect Control Plane After Konnect registration, you need to create your first Control Plane. There are multiple ways to do it:\nKonnect User Interface. RESTful Admin API, a fundamental mechanism for administration purposes. Kong Gateway Operator (KGO) and Kubernetes CRDs To get an easier and faster deployment, this workshop uses Konnect User Interface.\nThis tutorial is intended to be used for labs and PoC only. There are many aspects and processes, typically implemented in production sites, not described here. For example: Digital Certificate issuing, Cluster monitoring, etc. For a production ready deployment, refer Kong on Terraform Constructs, available here\nYou can now click Next to begin the module.",
    "description": "This chapter will walk you through\nKonnect Control Plane and Data Plane creation. Access Kong Data Plane. Here’s a Reference Architecture that will be implemented in this workshop:\nKong Konnect Control Plane: responsible for managing your APIs Kong Konnect Data Plane: connected to the Control Plane, it is responsible for processing all the incoming requests sent by the consumers. Kong provides a plugin framework, where each one of them is responsible for a specific functionality. As a can see, there are two main collections of plugins: On the left, the historic and regular API Gateway plugins, implementing all sort of policies including, for example, OIDC based Authentication processes with Keycloak, Amazon Cognito and Okta or Observability with Prometheus/Grafana and Dynatrace. On the right, another plugin collection for AI-based use cases. For example, the AI Rate Limiting plugin implements policies like this based on the number of tokens consumed be the requests. Or, as another example is the AI Semantic Cache plugin, which caches data based on the semantics related to the responses coming from the LLM models. Kong AI Gateway supports, out of the box, a variety of infrastructures, including not just OpenAI, but also Amazon Bedrock, Google Gemini, Mistral, Anthropic, etc. In order to deal with embeddings, the Gateway also supports also vector databases. Kong Gateway protects not just the LLM Models but also the upstream services, including your application micros surfaces or services. Konnect Control Plane After Konnect registration, you need to create your first Control Plane. There are multiple ways to do it:",
    "tags": [],
    "title": "Konnect Setup",
    "uri": "/11-konnect-setup/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Setup",
    "content": "Install the Operator To get started let’s install the Operator:\nhelm repo add kong https://charts.konghq.com helm repo update kong helm upgrade --install kgo kong/gateway-operator \\ -n kong-system \\ --create-namespace \\ --set image.tag=1.6.2 \\ --set kubernetes-configuration-crds.enabled=true \\ --set env.ENABLE_CONTROLLER_KONNECT=true You can check the Operator’s log with: kubectl logs -f $(kubectl get pod -n kong-system -o json | jq -r '.items[].metadata | select(.name | startswith(\"kgo-gateway-operator\"))' | jq -r '.name') -n kong-system Delete KGO If you want to delete KGO run:\nhelm uninstall kgo -n kong-system kubectl delete namespace kong-system Kong-gratulations! have now reached the end of this module by creating a Kong Operator. You can now click Next to proceed with the next module.",
    "description": "Install the Operator To get started let’s install the Operator:\nhelm repo add kong https://charts.konghq.com helm repo update kong helm upgrade --install kgo kong/gateway-operator \\ -n kong-system \\ --create-namespace \\ --set image.tag=1.6.2 \\ --set kubernetes-configuration-crds.enabled=true \\ --set env.ENABLE_CONTROLLER_KONNECT=true You can check the Operator’s log with: kubectl logs -f $(kubectl get pod -n kong-system -o json | jq -r '.items[].metadata | select(.name | startswith(\"kgo-gateway-operator\"))' | jq -r '.name') -n kong-system Delete KGO If you want to delete KGO run:",
    "tags": [],
    "title": "Serverless API Gateway",
    "uri": "/11-konnect-setup/111-serverless-api-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Setup",
    "content": "KGO requires a Konnect Personal Access Token (PAT) for creating the Control Plane. To generate your PAT, click on your initials in the upper right corner of the Konnect home page, then select Personal Access Tokens. Click on + Generate Token, name your PAT, set its expiration time, and be sure to copy and save it, as Konnect won’t display it again.\nNote Be sure to copy and save your PAT, as Konnect won’t display it again.\nKonnect PAT secret Create a Kubernetes (K8) Secret with your PAT in the kong namespace. KGO requires the secret to be labeled.\nSave PAT in an environment variables export PAT=PASTE_THE_CONTENTS_OF_COPIED_PAT Create the namespace kubectl create namespace kong Create K8s Secret with PAT Note Don’t forget to replace PASTE_THE_CONTENTS_OF_COPIED_PAT in the command above with the copied PAT from Kong UI.\nkubectl create secret generic konnect-pat -n kong --from-literal=token=$(echo $PAT) kubectl label secret konnect-pat -n kong \"konghq.com/credential=konnect\" Check your Secret. You should your PAT. kubectl get secret konnect-pat -n kong -o jsonpath='{.data.*}' | base64 -d You can now click Next to install the operator.",
    "description": "KGO requires a Konnect Personal Access Token (PAT) for creating the Control Plane. To generate your PAT, click on your initials in the upper right corner of the Konnect home page, then select Personal Access Tokens. Click on + Generate Token, name your PAT, set its expiration time, and be sure to copy and save it, as Konnect won’t display it again.",
    "tags": [],
    "title": "PAT - Personal Access Token",
    "uri": "/11-konnect-setup/112-personal-access-token/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Setup",
    "content": "Control Plane Deployment The following declaration defines an Authentication Configuration, based on the Kubernetes Secret and referring to a Konnect API URL, and the actual Konnect Control Plane.\ncat \u003c\u003cEOF | kubectl apply -f - kind: KonnectAPIAuthConfiguration apiVersion: konnect.konghq.com/v1alpha1 metadata: name: konnect-api-auth-conf namespace: kong spec: type: secretRef secretRef: name: konnect-pat namespace: kong serverURL: us.api.konghq.com --- kind: KonnectGatewayControlPlane apiVersion: konnect.konghq.com/v1alpha1 metadata: name: kong-workshop namespace: kong spec: name: kong-workshop konnect: authRef: name: konnect-api-auth-conf EOF If you go to Konnect UI \u003e Gateway manager, you should see a new control plane named kong-workshop getting created.\nData Plane deployment The next declaration instantiates a Data Plane connected to your Control Plane. It creates a KonnectExtension, asking KGO to manage the certificate and private key provisioning automatically, and the actual Data Plane. The Data Plane declaration specifies the Docker image, in our case 3.11, as well as how the Kubernetes Service, related to the Data Plane, should be created. Also, we use the the Data Plane deployment refers to the Kubernetes Service Account we created before.\ncat \u003c\u003cEOF | kubectl apply -f - kind: KonnectExtension apiVersion: konnect.konghq.com/v1alpha1 metadata: name: konnect-config1 namespace: kong spec: clientAuth: certificateSecret: provisioning: Automatic konnect: controlPlane: ref: type: konnectNamespacedRef konnectNamespacedRef: name: kong-workshop --- apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 network: services: ingress: name: proxy1 type: LoadBalancer EOF It takes some minutes to get the Load Balancer provisioned and avaiable. Get its domain name with:\nexport DATA_PLANE_LB=$(kubectl get svc -n kong proxy1 --output=jsonpath='{.status.loadBalancer.ingress[].ip}') View the load balancer DNS as\necho $DATA_PLANE_LB Try calling it as\ncurl -w '\\n' $DATA_PLANE_LB Expected Output\n{ \"message\":\"no Route matched with those values\", \"request_id\":\"d364362a60b32142fed73712a9ea1948\" } You can check the Data Plane logs with kubectl logs -f $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name') -n kong Control Plane and Data Plane deletion If you want to delete the DP run run:\nkubectl delete dataplane dataplane1 -n kong kubectl delete konnectextensions.konnect.konghq.com konnect-config1 -n kong If you want to delete the CP run:\nkubectl delete konnectgatewaycontrolplane kong-workshop -n kong kubectl delete konnectapiauthconfiguration konnect-api-auth-conf -n kong If you want to delete the PAT and namespace run:\nkubectl delete secret konnect-pat -n kong kubectl delete namespace kong Further Reading Kong Konnect API auth configuration Kong-gratulations! have now reached the end of this module by creating control plane and data plane. You can now click Next to proceed with the next module.",
    "description": "Control Plane Deployment The following declaration defines an Authentication Configuration, based on the Kubernetes Secret and referring to a Konnect API URL, and the actual Konnect Control Plane.\ncat \u003c\u003cEOF | kubectl apply -f - kind: KonnectAPIAuthConfiguration apiVersion: konnect.konghq.com/v1alpha1 metadata: name: konnect-api-auth-conf namespace: kong spec: type: secretRef secretRef: name: konnect-pat namespace: kong serverURL: us.api.konghq.com --- kind: KonnectGatewayControlPlane apiVersion: konnect.konghq.com/v1alpha1 metadata: name: kong-workshop namespace: kong spec: name: kong-workshop konnect: authRef: name: konnect-api-auth-conf EOF If you go to Konnect UI \u003e Gateway manager, you should see a new control plane named kong-workshop getting created.",
    "tags": [],
    "title": "Control Plane and Data Plane",
    "uri": "/11-konnect-setup/113-cp-dp/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "With our Control Plane created and Data Plane layer deployed it’s time to create an API and expose an application. In this module, we will:\nUse decK to define a Kong Service based on an endpoint provided by the application and a Kong Route on top of the Kong Service to expose the application. Enable Kong Plugins to the Kong Route or Kong Service. Define Kong Consumers to represent the entities sending request to the Gateway and enable Kong Plugin to them. With decK (declarations for Kong) you can manage Kong Konnect configuration in a declaratively way.\ndecK operates on state files. decK state files describe the configuration of Kong API Gateway. State files encapsulate the complete configuration of Kong in a declarative format, including services, routes, plugins, consumers, and other entities that define how requests are processed and routed through Kong.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Konnect Gateway Manager",
    "description": "With our Control Plane created and Data Plane layer deployed it’s time to create an API and expose an application. In this module, we will:\nUse decK to define a Kong Service based on an endpoint provided by the application and a Kong Route on top of the Kong Service to expose the application. Enable Kong Plugins to the Kong Route or Kong Service. Define Kong Consumers to represent the entities sending request to the Gateway and enable Kong Plugin to them. With decK (declarations for Kong) you can manage Kong Konnect configuration in a declaratively way.",
    "tags": [],
    "title": "Kong API Gateway",
    "uri": "/12-api-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway",
    "content": "Gateway Services represent the upstream services in your system. These applications are the business logic components of your system responsible for responding to requests.\nThe configuration of a Gateway Service defines the connectivity details between the Kong Gateway and the upstream service, along with other metadata. Generally, you should map one Gateway Service to each upstream service.\nFor simple deployments, the upstream URL can be provided directly in the Gateway Service. For sophisticated traffic management needs, a Gateway Service can point at an Upstream.\nGateway Services, in conjunction with Routes, let you expose your upstream services to clients with Kong Gateway.\nPlugins can be attached to a Service, and will run against every request that triggers a request to the Service that they’re attached to.\nFor the purpose of this workshop, you’ll create and expose a service to the HTTPbin API. HTTPbin is an echo-type application that returns requests back to the requester as responses.\nPing Konnect with decK Before start using decK, you should ping Konnect to check if the connecting is up. Note we assume you have the PAT environment variable set. Please, refer to the previous section to learn how to issue a PAT.\ndeck gateway ping --konnect-control-plane-name kong-workshop --konnect-token $PAT Expected Output\nSuccessfully Konnected to the Example-Name organization! Create a Kong Gateway Service and Kong Route Create the following declaration first. Remarks:\nNote the host and port refers to the HTTPbin’s Kubernetes Service FQDN (Fully Qualified Domain Name), in our case http://httpbin.kong.svc.cluster.local:8000. The declaration tags the objects so you can managing them apart from other ones. cat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /httpbin-route EOF Submit the declaration Now, you can use the following command to sync your Konnect Control Plane with the declaration. Note that all other existing objects will be deleted.\ndeck gateway sync --konnect-token $PAT httpbin.yaml Expected Output\ncreating service httpbin-service creating route httpbin-route Summary: Created: 2 Updated: 0 Deleted: 0 You should see your new service’s overview page.\nConsume the Route We are to use the same ELB provisioned during the Data Plane deployment:\ncurl -v $DATA_PLANE_LB/httpbin-route/get If successful, you should see the httpbin output:\n* Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Content-Length: 377 \u003c Connection: keep-alive \u003c Server: gunicorn \u003c Date: Wed, 06 Aug 2025 16:19:15 GMT \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Credentials: true \u003c X-Kong-Upstream-Latency: 6 \u003c X-Kong-Proxy-Latency: 3 \u003c Via: 1.1 kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: 0cbe555eefb4f14bb43f9b511435bd5c \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"127.0.0.1\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"0cbe555eefb4f14bb43f9b511435bd5c\"},\"origin\":\"10.244.0.1\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host 127.0.0.1 left intact Kong-gratulations! have now reached the end of this module by having your first service set up, running, and routing traffic proxied through a Kong data plane. You can now click Next to proceed with the next module.",
    "description": "Gateway Services represent the upstream services in your system. These applications are the business logic components of your system responsible for responding to requests.\nThe configuration of a Gateway Service defines the connectivity details between the Kong Gateway and the upstream service, along with other metadata. Generally, you should map one Gateway Service to each upstream service.\nFor simple deployments, the upstream URL can be provided directly in the Gateway Service. For sophisticated traffic management needs, a Gateway Service can point at an Upstream.",
    "tags": [],
    "title": "Kong Gateway Service and Kong Route",
    "uri": "/12-api-gateway/122-kong-service-route/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Proxy plugin is the fundamental AI Gateway component. It lets you transform and proxy requests to a number of AI providers and models. The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nThe following table describes which providers and requests the AI Proxy plugin supports:\nObs 1: OpenAI has marked Completions as legacy and recommends using the Chat Completions API for developing new applications.\nObs 2: Starting with Kong AI Gateway 3.11, new GenAI APIs are supported:\nGetting Started with Kong AI Gateway We are going to get started with a simple configuration. The following decK declaration enables the AI Proxy plugin to the Kong Gateway Service, to send requests to the LLM and consume the Ollama’s lamma3.2:1b FM and OpenAI’s gpt-5 FM with chat LLM requests.\nUpdate your ai-proxy.yaml file with that. Make sure you have the DECK_OPENAI_API_KEY environment variable set with your OpenAI’s API Key.\ncat \u003e ai-proxy.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-5 options: temperature: 1.0 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy.yaml OpenAI API Kong AI Gateway provides one API to access all of the LLMs it supports. To accomplish this, Kong AI Gateway has standardized on the OpenAI API specification. This will help developers to onboard more quickly by providing them with an API specification that they’re already familiar with. You can start using LLMs behind the AI Gateway simply by redirecting your requests to a URL that points to a route of the AI Gateway.\nSend a request to Kong AI Gateway Now, send a request to Kong AI Gateway following the OpenAI API Chat specification as a reference:\ncurl -s -X POST \\ --url http://$DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ] }' | jq Expected Output\nNote the response also complies to the OpenAI API spec:\n{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ] }' | jq { \"id\": \"chatcmpl-C3jWHoMI65rb0Ojkai1NjBq0JoRMG\", \"object\": \"chat.completion\", \"created\": 1755005997, \"model\": \"gpt-5-2025-08-07\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Pi (π) is the mathematical constant equal to the ratio of a circle’s circumference to its diameter. It’s the same for all circles.\\n\\n- Approximate value: 3.141592653589793…\\n- Nature: irrational (non-terminating, non-repeating) and transcendental.\\n- Common formulas:\\n - Circumference: C = 2πr\\n - Area of a circle: A = πr²\\n - Appears widely, e.g., e^(iπ) + 1 = 0, normal distribution, waves/Fourier analysis.\\n- Handy approximations: 22/7 ≈ 3.142857, 355/113 ≈ 3.14159292.\\n\\nIf you want more digits or historical background, say the word.\", \"refusal\": null, \"annotations\": [] }, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 10, \"completion_tokens\": 621, \"total_tokens\": 631, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 448, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": null } You can also consume the Ollama’s route\ncurl -s -X POST \\ --url http://$DATA_PLANE_LB/ollama-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ] }' | jq AI Proxy configuration parameters The AI Proxy plugin is responsible for a variety of topics. For example:\nRequest and response formats appropriate for the configured provider and route_type settings. provider can be set as anthropic, azure, bedrock, cohere, gemini, huggingface, llama2, mistral or openai. The route_type AI Proxy configuration parameter defines which kind of request the AI Gateway is going to perform. It must be one of: audio/v1/audio/speech audio/v1/audio/transcriptions audio/v1/audio/translations image/v1/images/edits image/v1/images/generations llm/v1/assistants llm/v1/batches llm/v1/chat llm/v1/completions llm/v1/embeddings llm/v1/files llm/v1/responses preserve realtime/v1/realtime Authentication on behalf of the Kong API consumer. Decorating the request with parameters from the config.model.options block, appropriate for the chosen provider. For our case, we tell the temperature we are going to use. Define the model to be consume when sending the request As you may have noticed our AI Proxy plugin defines the model it should consume. That is can be done for individual requests, if required. Change the ai-proxy.yaml file, removing the model’s name parameter and apply the declaration again:\ncat \u003e ai-proxy.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai options: temperature: 1.0 EOF deck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy.yaml Send the request specifing the model:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-5\" }' or\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-4\" }' Note the Kong AI Proxy plugin adds a new X-Kong-LLM-Model header with the model we consumer: openai/gpt-5 or openai/gpt-4\nStreaming Normally, a request is processed and completely buffered by the LLM before being sent back to Kong AI Gateway and then to the caller in a single large JSON block. This process can be time-consuming, depending on the request parameters, and the complexity of the request sent to the LLM model. To avoid making the user wait for their chat response with a loading animation, most models can stream each word (or sets of words and tokens) back to the client. This allows the chat response to be rendered in real time.\nThe config AI Proxy configuration section has a response_streaming parameter to define the response streaming. By default is set as allow but it can be set with deny or always.\nAs an example, if you send the same request with the stream parameter as true you should see a response like this:\ncurl -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-4\", \"stream\": true }' data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\",\"refusal\":null},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"D5jIQAiER0kD2\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Pi\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"3S9RmT4NS9k3b\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" (\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"3ARtgUA4COqRA\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\"π\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"IS99TImGO4SoLp\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\")\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"8jpC3eE7bQvh7b\"} ... data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"YHL01GZcTNF1Wh\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"obfuscation\":\"vf2t9C6t3\"} data: [DONE] Extra Model Options The Kong AI Proxy provides other configuration options. For example:\nmax_tokens: defines the max_tokens, if using chat or completion models. temperature: it is a number between 0 and 5 and it defines the matching temperature, if using chat or completion models. top_p: a number between 0 and 1 defining the top-p probability mass, if supported. top_k: an integer between 0 and 500 defining the top-k most likely tokens, if supported. Kong-gratulations! have now reached the end of this module by caching API responses. You can now click Next to proceed with the next module.",
    "description": "The AI Proxy plugin is the fundamental AI Gateway component. It lets you transform and proxy requests to a number of AI providers and models. The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nThe following table describes which providers and requests the AI Proxy plugin supports:",
    "tags": [],
    "title": "AI Proxy",
    "uri": "/16-ai-gateway/17-use-cases/150-ai-proxy/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "Proxy Caching provides a reverse proxy cache implementation for Kong. It caches response entities based on configurable response code and content type, as well as request method. It can cache per-Consumer or per-API. Cache entities are stored for a configurable period of time, after which subsequent requests to the same resource will re-fetch and re-store the resource. Cache entities can also be forcefully purged via the Admin API prior to their expiration time.\nKong Gateway Plugin list Before enabling the Proxy Caching, let’s check the list of plugins Konnect provides. Inside the kong-workshop Control Plane, click on Plugins menu option and + New plugin. You should the following page with all plugins available:\nEnabling a Kong Plugin on a Kong Service Create another declaration with plugins option. With this option you can enable and configure the plugin on your Kong Service.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 plugins: - name: proxy-cache instance_name: proxy-cache1 config: strategy: memory cache_ttl: 30 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route EOF For the plugin configuration we used the following settings:\nstrategy with memory. The plugin will use the Runtime Instance’s memory to implement to cache. cache_ttl with 30, which means the plugin will clear all data that reached this time limit. All plugin configuration paramenters are described inside Kong Plugin Hub portal, in its specific documentation page.\nSubmit the new declaration deck gateway sync --konnect-token $PAT httpbin.yaml Expected Output\ncreating plugin proxy-cache for service httpbin-service Summary: Created: 1 Updated: 0 Deleted: 0 Consume the Service If you consume the service again, you’ll see some new headers describing the caching status:\ncurl -v $DATA_PLANE_LB/httpbin-route/get * Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Content-Length: 377 \u003c Connection: keep-alive \u003c X-Cache-Key: a00008105a989fd0fa8a1eeeee08924b7205d24ed1adee71698926c12a31f2b7 \u003c X-Cache-Status: Miss \u003c Server: gunicorn \u003c Date: Mon, 11 Aug 2025 14:39:46 GMT \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Credentials: true \u003c X-Kong-Upstream-Latency: 8 \u003c X-Kong-Proxy-Latency: 6 \u003c Via: 1.1 kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: 4501cc0fa798cf08435edc01bb2b1a40 \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"127.0.0.1\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"4501cc0fa798cf08435edc01bb2b1a40\"},\"origin\":\"10.244.0.1\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host 127.0.0.1 left intact Notice that, for the first request we get Miss for the X-Cache-Status header, meaning that the Runtime Instance didn’t have any data avaialble in the cache and had to connect to the Upstream Service, httpbin.org.\nIf we send a new request, the Runtime Instance has all it needs to satify the request, therefore the status is Hit. Note that the latency time has dropped considerably.\n# curl -v $DATA_PLANE_LB/httpbin-route/get * Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Connection: keep-alive \u003c X-Cache-Key: a00008105a989fd0fa8a1eeeee08924b7205d24ed1adee71698926c12a31f2b7 \u003c Access-Control-Allow-Credentials: true \u003c X-Cache-Status: Hit \u003c Access-Control-Allow-Origin: * \u003c Date: Mon, 11 Aug 2025 14:40:17 GMT \u003c age: 3 \u003c Server: gunicorn \u003c Content-Length: 377 \u003c X-Kong-Upstream-Latency: 0 \u003c X-Kong-Proxy-Latency: 1 \u003c Via: 1.1 kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: 97cc6027e33f240a67d8930161b44e57 \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"127.0.0.1\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"2228de44dadd2e6126d82c4fb2e43961\"},\"origin\":\"10.244.0.1\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host 127.0.0.1 left intact Enabling a Kong Plugin on a Kong Route Now, we are going to define a Rate Limiting policy for our Service. This time, you are going to enable the Rate Limiting plugin to the Kong Route, not to the Kong Gateway Service. In this sense, new Routes defined for the Service will not have the Rate Limiting plugin enabled, only the Proxy Caching.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 plugins: - name: proxy-cache config: strategy: memory cache_ttl: 30 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 3 EOF The configuration includes:\nminute as 3, which means the Route can be consumed only 3 times a given minute. Submit the declaration deck gateway sync --konnect-token $PAT httpbin.yaml Consume the Service If you consume the service again, you’ll see, besides the caching related headers, new ones describing the status of current rate limiting policy:\ncurl -v $DATA_PLANE_LB/httpbin-route/get * Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Content-Length: 377 \u003c Connection: keep-alive \u003c X-RateLimit-Limit-Minute: 3 \u003c RateLimit-Remaining: 2 \u003c RateLimit-Reset: 32 \u003c RateLimit-Limit: 3 \u003c X-RateLimit-Remaining-Minute: 2 \u003c X-Cache-Key: a00008105a989fd0fa8a1eeeee08924b7205d24ed1adee71698926c12a31f2b7 \u003c X-Cache-Status: Miss \u003c Server: gunicorn \u003c Date: Mon, 11 Aug 2025 14:41:28 GMT \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Credentials: true \u003c X-Kong-Upstream-Latency: 1 \u003c X-Kong-Proxy-Latency: 5 \u003c Via: 1.1 kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: 882b11008e7ddd2eff471a433576524d \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"127.0.0.1\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"882b11008e7ddd2eff471a433576524d\"},\"origin\":\"10.244.0.1\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host 127.0.0.1 left intact If you keep sending new requests to the Runtime Instance, eventually, you’ll get a 429 error code, meaning you have reached the consumption rate limiting policy for this Route.\ncurl -v $DATA_PLANE_LB/httpbin-route/get * Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 429 Too Many Requests \u003c Date: Mon, 11 Aug 2025 14:41:58 GMT \u003c Content-Type: application/json; charset=utf-8 \u003c Connection: keep-alive \u003c X-RateLimit-Limit-Minute: 3 \u003c X-RateLimit-Remaining-Minute: 0 \u003c RateLimit-Reset: 2 \u003c Retry-After: 2 \u003c RateLimit-Remaining: 0 \u003c RateLimit-Limit: 3 \u003c Content-Length: 92 \u003c X-Kong-Response-Latency: 1 \u003c Server: kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: ce56eb67161a85678126a00ef59e6159 \u003c { \"message\":\"API rate limit exceeded\", \"request_id\":\"ce56eb67161a85678126a00ef59e6159\" * Connection #0 to host 127.0.0.1 left intact } Enabling a Kong Plugin globally Besides scoping a plugin to a Kong Service or Route, we can apply it globally also. When we do it so, all Services ans Routes will enforce the police described by the plugin.\nFor example, let’s apply the Proxy Caching plugin globally.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route plugins: - name: proxy-cache config: strategy: memory cache_ttl: 30 services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 3 EOF Submit the declaration deck gateway sync --konnect-token $PAT httpbin.yaml After testing the configuration reset the Control Plane:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f Kong-gratulations! have now reached the end of this module by caching API responses. You can now click Next to proceed with the next module.",
    "description": "Proxy Caching provides a reverse proxy cache implementation for Kong. It caches response entities based on configurable response code and content type, as well as request method. It can cache per-Consumer or per-API. Cache entities are stored for a configurable period of time, after which subsequent requests to the same resource will re-fetch and re-store the resource. Cache entities can also be forcefully purged via the Admin API prior to their expiration time.",
    "tags": [],
    "title": "Proxy Caching",
    "uri": "/12-api-gateway/15-use-cases/150-proxy-caching/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway",
    "content": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nProxy caching API Key with Kong Consumers Rate Limiting using Redis Request Transformer Request Callout OpenID Connect with Keycloak Access Control with Open Policy Agent (OPA) These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "description": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nProxy caching API Key with Kong Consumers Rate Limiting using Redis Request Transformer Request Callout OpenID Connect with Keycloak Access Control with Open Policy Agent (OPA) These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong plugins on the Plugin Hub.",
    "tags": [],
    "title": "Use Cases",
    "uri": "/12-api-gateway/15-use-cases/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "To get started with API Authentication, let’s implement a basic Key Authentication mechanism. API Keys are one of the foundamental security mechanisms provided by Konnect. In order to consume an API, the consumer should inject a previously created API Key in the header of the request. The API consumption is allowed if the Gateway recognizes the API Key. Consumers add their API key either in a query string parameter, a header, or a request body to authenticate their requests and consume the application.\nA Kong Consumer represents a consumer (user or application) of a Service. A Kong Consumer is tightly coupled to an Authentication mechanism the Kong Gateway provides.\nPlease, check the Key-Auth plugin plugin and Kong Consumer documentation pages to learn more about them.\nEnable the Key Authentication Plugin on the Kong Route cat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 EOF deck gateway sync --konnect-token $PAT key-auth.yaml Consume the Route Now, if you try the Route, you’ll get a specific 401 error code meaning that, since you don’t have any API Key injected in your request, you are not allowd to consume it.\ncurl -i $DATA_PLANE_LB/key-auth-route/get HTTP/1.1 401 Unauthorized Date: Mon, 11 Aug 2025 14:44:59 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Key Content-Length: 96 X-Kong-Response-Latency: 2 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 1f8a6c1c9d0d1853d9db426588c1ce1c { \"message\":\"No API key found in request\", \"request_id\":\"1f8a6c1c9d0d1853d9db426588c1ce1c\" } Create a Kong Consumer In order to consume the Route we need to create a Kong Consumer. Here’s its declaration:\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 EOF Submit the declaration deck gateway sync --konnect-token $PAT key-auth.yaml Consume the Route with the API Key Now, you need to inject the Key you’ve just created, as a header, in your requests. Using HTTPie, you can do it easily like this:\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:123456' HTTP/1.1 200 OK Content-Type: application/json Content-Length: 551 Connection: keep-alive Server: gunicorn Date: Mon, 11 Aug 2025 14:45:52 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 9 X-Kong-Proxy-Latency: 4 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: b535885591f5ec7f7fb5f070fa365465 Of course, if you inject a wrong key, you get a specific error like this:\n# curl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:12' HTTP/1.1 401 Unauthorized Date: Mon, 11 Aug 2025 14:46:36 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Key Content-Length: 81 X-Kong-Response-Latency: 1 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 688e271ea4cae5794bc1cb59ea3ec131 NOTE\nThe header has to have the API Key name, which is, in our case, apikey. That was the default name provided by Konnect when you enabled the Key Authentication on the Kong Route. You can change the plugin configuration, if you will. Kong Consumer Policies With the API Key policy in place, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIt’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:\nconsumer1: apikey = 123456 rate limiting policy = 5 rpm consumer2: apikey = 987654 rate limiting policy = 8 rpm Doing that, the Data Plane is capable to not just protect the Route but to identify the consumer based on the key injected to enforce specific policies to the consumer.\nFor this section we’re implementing a Rate Limiting policy. Keep in mind that a Consumer might have other plugins also enabled such as Request Transformer, TCP Log, etc.\nNew Consumer Create the second consumer2, just like you did with the first one, with the 987654 key.\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 - keyauth_credentials: - key: \"987654\" username: consumer2 EOF Submit the declaration deck gateway sync --konnect-token $PAT key-auth.yaml If you will, you can inject both keys to your requests.\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:123456' or\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:987654' Consumers’ Policy Now let’s enhance the plugins declaration enabling the Rate Limiting plugin to each one of our consumers.\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 5 - keyauth_credentials: - key: \"987654\" username: consumer2 plugins: - name: rate-limiting instance_name: rate-limiting2 config: minute: 8 EOF Submit the declaration deck gateway sync --konnect-token $PAT key-auth.yaml Consumer the Route using different API Keys. First of all let’s consume the Route with the Consumer1’s API Key:\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:123456' Expected Output\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 551 Connection: keep-alive X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 4 RateLimit-Reset: 11 RateLimit-Remaining: 4 RateLimit-Limit: 5 Server: gunicorn Date: Mon, 11 Aug 2025 14:47:49 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 3 X-Kong-Proxy-Latency: 2 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: ad38e60e76c57d5826f3c37fdce4925c Now, let’s consume it with the Consumer2’s API Key. As you can see the Data Plane is processing the Rate Limiting processes independently.\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:987654' Expected Output\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 551 Connection: keep-alive X-RateLimit-Limit-Minute: 8 X-RateLimit-Remaining-Minute: 7 RateLimit-Reset: 27 RateLimit-Remaining: 7 RateLimit-Limit: 8 Server: gunicorn Date: Mon, 11 Aug 2025 14:49:33 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 2 X-Kong-Proxy-Latency: 3 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 9cf1fb53db3a9b740d9bd42e9091d245 If we keep sending requests using the first API Key, eventually, as expected, we’ll get an error code:\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:123456' Expected Output\nHTTP/1.1 429 Too Many Requests Date: Mon, 11 Aug 2025 14:50:21 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 0 RateLimit-Reset: 39 Retry-After: 39 RateLimit-Remaining: 0 RateLimit-Limit: 5 Content-Length: 92 X-Kong-Response-Latency: 1 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 38afd254e246a946a65153780606be3c However, the second API Key is still allowed to consume the Kong Route:\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:987654' Expected Output\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 551 Connection: keep-alive X-RateLimit-Limit-Minute: 8 X-RateLimit-Remaining-Minute: 7 RateLimit-Reset: 34 RateLimit-Remaining: 7 RateLimit-Limit: 8 Server: gunicorn Date: Mon, 11 Aug 2025 14:50:26 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 1 X-Kong-Proxy-Latency: 2 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: f8602e2e2778f306fba41f1661ef554c Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.\nOptional Reading Applying Kong Plugins on Services, Routes or Globally helps us to implement an extensive list of policies in the API Gateway layer. However, so far, we are not controlling who is sending the requests to the Data Plane. That is, anyone who has the Runtime Instance ELB address is capable to send requests to it and consumer the Services.\nAPI Gateway Authentication is an important way to control the data that is allowed to be transmitted using your APIs. Basically, it checks that a particular consumer has permission to access the API, using a predefined set of credentials.\nKong Gateway has a library of plugins that provide simple ways to implement the best known and most widely used methods of API gateway authentication. Here are some of the commonly used ones:\nBasic Authentication Key Authentication OAuth 2.0 Authentication LDAP Authentication OpenID Connect Kong Plugin Hub provides documentation about all Authentication based plugins. Refer to the following link to read more about API Gateway Authentication",
    "description": "To get started with API Authentication, let’s implement a basic Key Authentication mechanism. API Keys are one of the foundamental security mechanisms provided by Konnect. In order to consume an API, the consumer should inject a previously created API Key in the header of the request. The API consumption is allowed if the Gateway recognizes the API Key. Consumers add their API key either in a query string parameter, a header, or a request body to authenticate their requests and consume the application.",
    "tags": [],
    "title": "API Key Authentication",
    "uri": "/12-api-gateway/15-use-cases/151-key-authentication/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "For Prompt Engineering use cases, Kong AI Gateway provides:\nAI Prompt Decorator plugin: it injects messages at the start or end of a caller’s chat history. AI Prompt Guard plugin: it lets you configure a series of PCRE-compatible regular expressions to allow and block specific prompts, words, phrases, or otherwise and have more control over the LLM. AI Prompt Template plugin: it’s responsible for pre-configuring AI prompts to users All plugins extend the functionality of the AI Proxy plugin, and requires AI Proxy to be configured first.\nYou can now click Next to proceed further.",
    "description": "For Prompt Engineering use cases, Kong AI Gateway provides:\nAI Prompt Decorator plugin: it injects messages at the start or end of a caller’s chat history. AI Prompt Guard plugin: it lets you configure a series of PCRE-compatible regular expressions to allow and block specific prompts, words, phrases, or otherwise and have more control over the LLM. AI Prompt Template plugin: it’s responsible for pre-configuring AI prompts to users All plugins extend the functionality of the AI Proxy plugin, and requires AI Proxy to be configured first.",
    "tags": [],
    "title": "Prompt Engineering",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "The Response Transformer plugin modifies the upstream response (e.g. response from the server) before returning it to the client.\nIn this section, you will configure the Response Transformer plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\ncat \u003e response-transformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: response-transformer-route paths: - /response-transformer-route plugins: - name: response-transformer instance_name: response-transformer1 config: add: headers: - demo:injected-by-kong EOF Submit the declaration deck gateway sync --konnect-token $PAT response-transformer.yaml Verify Test to make sure Kong transforms the request to the echo server and httpbin server.\ncurl --head $DATA_PLANE_LB/response-transformer-route/get HTTP/1.1 200 OK Content-Type: application/json Content-Length: 403 Connection: keep-alive Server: gunicorn Date: Mon, 11 Aug 2025 14:51:56 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true demo: injected-by-kong X-Kong-Upstream-Latency: 1 X-Kong-Proxy-Latency: 2 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 6d294407e61075665321d07709210e3a Expected Results Notice that demo: injected-by-kong is injected in the header.\nCleanup Reset the Control Plane to ensure that the plugins do not interfere with any other modules in the workshop for demo purposes and each workshop module code continues to function independently.\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f In real world scenario, you can enable as many plugins as you like depending on your use cases.\nKong-gratulations! have now reached the end of this module by configuring the Kong Route to include demo: injected-by-kong before responding to the client. You can now click Next to proceed with the next module.",
    "description": "The Response Transformer plugin modifies the upstream response (e.g. response from the server) before returning it to the client.\nIn this section, you will configure the Response Transformer plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\ncat \u003e response-transformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: response-transformer-route paths: - /response-transformer-route plugins: - name: response-transformer instance_name: response-transformer1 config: add: headers: - demo:injected-by-kong EOF Submit the declaration deck gateway sync --konnect-token $PAT response-transformer.yaml",
    "tags": [],
    "title": "Response Transformer",
    "uri": "/12-api-gateway/15-use-cases/153-response-transformer/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "The Request Callout plugin allows you to insert arbitrary API calls before proxying a request to the upstream service.\nIn this section, you will configure the Request Callout plugin on the Kong Route. Specifically, you will configure the plugin to do the following:\nCall Wikipedia using the “srseach” header as a parameter. The number of hits found and returned by Wikipeadia is added as a new header to the request. The request is sent to httpbin application which echoes the number of hits. Hit Wikipedia Just to get an idea what the Wikipedia response, send the following request:\ncurl -s \"https://en.wikipedia.org/w/api.php?srsearch=Miles%20Davis\u0026action=query\u0026list=search\u0026format=json\" | jq '.query.searchinfo.totalhits' You should get a number like 43555, which represents the number of total hits related to Miles Davis\nCreate the Request Callout Plugin Take the plugins declaration and enable the Request Callout plugin to the Route.\ncat \u003e request-callout.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: request-callout-route paths: - /request-callout-route plugins: - name: request-callout instance_name: request-callout1 config: callouts: - name: wikipedia request: url: https://en.wikipedia.org/w/api.php method: GET query: forward: true by_lua: local srsearch = kong.request.get_header(\"srsearch\"); local srsearch_encoded = ngx.escape_uri(srsearch) query = \"srsearch=\" .. srsearch_encoded .. \"\u0026action=query\u0026list=search\u0026format=json\"; kong.log.inspect(query); kong.ctx.shared.callouts.wikipedia.request.params.query = query response: body: decode: true by_lua: kong.service.request.add_header(\"wikipedia-total-hits-header\", kong.ctx.shared.callouts.wikipedia.response.body.query.searchinfo.totalhits) EOF Submit the declaration deck gateway sync --konnect-token $PAT request-callout.yaml Verify Send the request to Kong and check the response\ncurl -s \"http://$DATA_PLANE_LB/request-callout-route/get\" -H srsearch:\"Miles Davis\" | jq { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Content-Length\": \"0\", \"Host\": \"httpbin.kong.svc.cluster.local:8000\", \"Srsearch\": \"Miles Davis\", \"User-Agent\": \"curl/8.7.1\", \"Wipikedia-Total-Hits-Header\": \"43555\", \"X-Forwarded-Host\": \"127.0.0.1\", \"X-Forwarded-Path\": \"/request-callout-route/get\", \"X-Forwarded-Prefix\": \"/request-callout-route\", \"X-Kong-Request-Id\": \"6e4df528567f446630c6ae5c0b461c2e\" }, \"origin\": \"10.244.0.1\", \"url\": \"http://httpbin.kong.svc.cluster.local:8000/get\" } Expected Results Notice that new Wikipedia-Total-Hits-Header header is injected.\nCleanup Reset the Control Plane to ensure that the plugins do not interfere with any other modules in the workshop for demo purposes and each workshop module code continues to function independently.\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f In real world scenario, you can enable as many plugins as you like depending on your use cases.\nKong-gratulations! have now reached the end of this module. You can now click Next to proceed with the next module.",
    "description": "The Request Callout plugin allows you to insert arbitrary API calls before proxying a request to the upstream service.\nIn this section, you will configure the Request Callout plugin on the Kong Route. Specifically, you will configure the plugin to do the following:\nCall Wikipedia using the “srseach” header as a parameter. The number of hits found and returned by Wikipeadia is added as a new header to the request. The request is sent to httpbin application which echoes the number of hits. Hit Wikipedia Just to get an idea what the Wikipedia response, send the following request:",
    "tags": [],
    "title": "Request Callout",
    "uri": "/12-api-gateway/15-use-cases/154-request-callout/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Request Transformer and AI Response Transformer plugins integrate with the LLM on Amazon Bedrock, enabling introspection and transformation of the request’s body before proxying it to the Upstream Service and prior to forwarding the response to the client.\nThe plugins support llm/v1/chat style requests for all of the same providers as the AI Proxy plugin. It also use all of the same configuration and tuning parameters as the AI Proxy plugin, under the config.llm block.\nThe AI Request Transformer plugin runs before all of the AI Prompt plugins and the AI Proxy plugin, allowing it to also introspect LLM requests against the same, or a different, LLM. On the other hand, the AI Response Transformer plugin runs after the AI Proxy plugin, and after proxying to the Upstream Service, allowing it to also introspect LLM responses against the same, or a different, LLM service.\nThe diagram shows the journey of a consumer’s request through Kong Gateway to the backend service, where it is transformed by both an AI LLM service and Kong’s AI Request Transformer and the AI Response Transformer plugins.\nFor each plugin the configuration and usage processes are:\nThe Kong Gateway admin sets up an llm: configuration block, following the same configuration format as the AI Proxy plugin, and the same driver capabilities. The Kong Gateway admin sets up a prompt for the request introspection. The prompt becomes the system message in the LLM chat request, and prepares the LLM with transformation instructions for the incoming user request body (for the AI Request Transformer plugin) and for the returning upstream response body (for the AI Response Transformer plugin) The user makes an HTTP(S) call. Before proxying the user’s request to the backend, Kong Gateway sets the entire request body as the user message in the LLM chat request, and then sends it to the configured LLM service. After receiving the response from the backend, Kong Gateway sets the entire response body as the user message in the LLM chat request, then sends it to the configured LLM service. The LLM service returns a response assistant message, which is subsequently set as the upstream request body. The following example is going to apply the plugins to transform both request and reponse when consuming the httpbin Upstream Service.\nNow, configure both plugins. Keep in mind that the plugins are totally independent from each other so, the configuration depends on your use case.\ncat \u003e ai-request-response-tranformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /httpbin-route plugins: - name: ai-request-transformer instance_name: ai-request-transformer enabled: true config: prompt: In my JSON message, anywhere there is a JSON tag for a \"city\", also add a \"country\" tag with the name of the country in which the city resides. Return me only the JSON message, no extra text.\" llm: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-response-transformer instance_name: ai-response-transformer enabled: true config: prompt: For any city name, add its current temperature, in brackets next to it. Reply with the JSON result only. llm: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-request-response-tranformer.yaml curl -s -X POST \\ --url $DATA_PLANE_LB/httpbin-route/post \\ --header 'Content-Type: application/json' \\ --data '{ \"user\": { \"name\": \"Kong User\", \"city\": \"Tokyo\" } }' | jq Expected output { \"user\": { \"name\": \"Kong User\", \"city\": \"Tokyo [12°C]\", \"country\": \"Japan\" } } Kong-gratulations! have now reached the end of this module by using Kong Gateway to invoke a AWS Lambda function. You can now click Next to proceed with the next chapter.",
    "description": "The AI Request Transformer and AI Response Transformer plugins integrate with the LLM on Amazon Bedrock, enabling introspection and transformation of the request’s body before proxying it to the Upstream Service and prior to forwarding the response to the client.\nThe plugins support llm/v1/chat style requests for all of the same providers as the AI Proxy plugin. It also use all of the same configuration and tuning parameters as the AI Proxy plugin, under the config.llm block.",
    "tags": [],
    "title": "AI Request and Response Transfomers",
    "uri": "/16-ai-gateway/17-use-cases/155-request-response-transformer/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "Semantic caching enhances data retrieval efficiency by focusing on the meaning or context of queries rather than just exact matches. It stores responses based on the underlying intent and semantic similarities between different queries and can then retrieve those cached queries when a similar request is made.\nWhen a new request is made, the system can retrieve and reuse previously cached responses if they are contextually relevant, even if the phrasing is different. This method reduces redundant processing, speeds up response times, and ensures that answers are more relevant to the user’s intent, ultimately improving overall system performance and user experience.\nFor example, if a user asks, “how to integrate our API with a mobile app” and later asks, “what are the steps for connecting our API to a smartphone application?”, the system understands that both questions are asking for the same information. It can then retrieve and reuse previously cached responses, even if the wording is different. This approach reduces processing time and speeds up responses.\nThe AI Semantic Cache plugin may not be ideal for you if:\nIf you have limited hardware or budget. Storing semantic vectors and running similarity searches require a lot of storage and computing power, which could be an issue. If your data doesn’t rely on semantics, or exact matches work fine, semantic caching may offer little benefit. Traditional or keyword-based caching might be more efficient. How it works The diagram below illustrates the semantic caching mechanism implemented by the AI Semantic Cache plugin.\nThe process involves three parts: request handling, embedding generation, and response caching.\nFirst, a user starts a chat request with the LLM. The AI Semantic Cache plugin queries the vector database to see if there are any semantically similar requests that have already been cached. If there is a match, the vector database returns the cached response to the user. If there isn’t a match, the AI Semantic Cache plugin prompts the embeddings LLM to generate an embedding for the response. The AI Semantic Cache plugin uses a vector database and cache to store responses to requests. The plugin can then retrieve a cached response if a new request matches the semantics of a previous request, or it can tell the vector database to store a new response if there are no matches. With the AI Semantic Cache plugin, you can configure a cache of your choice to store the responses from the LLM. Currently, the plugin supports Redis as a cache.\nRedis as a Vector database We are going to configure the AI Semantic Cache to consume the Redis deployment available in the EKS Cluster. Redis, this time, will play the Vector database role.\nApply the Semantic Cache plugin cat \u003e ai-semantic-cache.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - semantic-cache - llm _konnect: control_plane_name: kong-workshop services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai enabled: true config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-semantic-cache instance_name: ai-semantic-cache-openai enabled: true config: embeddings: auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: \"text-embedding-3-small\" vectordb: dimensions: 1024 distance_metric: cosine strategy: redis threshold: 0.2 redis: host: \"redis-stack.redis.svc.cluster.local\" port: 6379 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-semantic-cache.yaml Check Redis Before sending request, you can scan the Redis database:\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan 1st Request Since we don’t have any cached data, the first request is going to return “Miss”:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected response HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-Cache-Status: Miss x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 Date: Tue, 12 Aug 2025 14:47:48 GMT x-ratelimit-remaining-tokens: 29993 access-control-expose-headers: X-Request-ID openai-organization: user-4qzstwunaw6d1dhwnga5bc5q openai-processing-ms: 7218 x-ratelimit-reset-requests: 120ms openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-cache-status: DYNAMIC openai-version: 2020-10-01 Server: cloudflare X-Content-Type-Options: nosniff x-envoy-upstream-service-time: 7420 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload CF-RAY: 96e0c4e97b364d3b-GRU x-ratelimit-limit-requests: 500 x-request-id: req_ae7f43291824451dbfec2a27b1a3ec2a x-ratelimit-reset-tokens: 14ms alt-svc: h3=\":443\"; ma=86400 X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2005 X-Kong-Upstream-Latency: 8820 X-Kong-Proxy-Latency: 876 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 8fd73d1623140f675ed93b0dcb4aeb16 { \"id\": \"chatcmpl-C3kZpdNpSx8eaIHhsLg14RhZgFBww\", \"object\": \"chat.completion\", \"created\": 1755010061, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (full name: James Marshall Hendrix, born November 27, 1942 – died September 18, 1970) was an American guitarist, singer, and songwriter, widely regarded as one of the most influential electric guitarists in the history of popular music. Emerging in the late 1960s, Hendrix revolutionized the way the guitar was played, using feedback, distortion, and an array of innovative techniques that transformed rock, blues, and psychedelic music.\\n\\nHendrix rose to fame with his band, **The Jimi Hendrix Experience**, delivering classic albums such as *Are You Experienced* (1967) and *Electric Ladyland* (1968). His groundbreaking performances included a legendary rendition of \\\"The Star-Spangled Banner\\\" at Woodstock in 1969.\\n\\nDespite his career only spanning about four years, Hendrix's influence endures through his recordings and his impact on generations of musicians. He was posthumously inducted into the Rock and Roll Hall of Fame in 1992. Some of his most famous songs include \\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"Voodoo Child (Slight Return),\\\" and \\\"All Along the Watchtower.\\\" Hendrix died at the age of 27, becoming one of the most iconic members of the so-called \\\"27 Club.\\\"\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 264, \"total_tokens\": 277, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_51e1070cf2\" } Check Redis again The Redis database has an entry now:\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan Expected response \"kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36\" 2nd Request The Semantic Cache plugin will use the cached data for similar requests:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Tell me more about Jimi Hendrix\" } ] }' Expected response HTTP/1.1 200 OK Date: Tue, 12 Aug 2025 14:48:55 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-Cache-Status: Hit Age: 67 X-Cache-Key: kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36 X-Cache-Ttl: 233 Content-Length: 1814 X-Kong-Response-Latency: 1438 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 2debcb5db5e6f3637bef912cca963a5d {\"object\":\"chat.completion\",\"created\":1755010061,\"id\":\"2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36\",\"usage\":{\"completion_tokens\":264,\"prompt_tokens_details\":{\"cached_tokens\":0,\"audio_tokens\":0},\"completion_tokens_details\":{\"accepted_prediction_tokens\":0,\"audio_tokens\":0,\"rejected_prediction_tokens\":0,\"reasoning_tokens\":0},\"total_tokens\":277,\"prompt_tokens\":13},\"model\":\"gpt-4.1-2025-04-14\",\"service_tier\":\"default\",\"system_fingerprint\":\"fp_51e1070cf2\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"annotations\":{},\"role\":\"assistant\",\"refusal\":null,\"content\":\"**Jimi Hendrix** (full name: James Marshall Hendrix, born November 27, 1942 – died September 18, 1970) was an American guitarist, singer, and songwriter, widely regarded as one of the most influential electric guitarists in the history of popular music. Emerging in the late 1960s, Hendrix revolutionized the way the guitar was played, using feedback, distortion, and an array of innovative techniques that transformed rock, blues, and psychedelic music.\\n\\nHendrix rose to fame with his band, **The Jimi Hendrix Experience**, delivering classic albums such as *Are You Experienced* (1967) and *Electric Ladyland* (1968). His groundbreaking performances included a legendary rendition of \\\"The Star-Spangled Banner\\\" at Woodstock in 1969.\\n\\nDespite his career only spanning about four years, Hendrix's influence endures through his recordings and his impact on generations of musicians. He was posthumously inducted into the Rock and Roll Hall of Fame in 1992. Some of his most famous songs include \\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"Voodoo Child (Slight Return),\\\" and \\\"All Along the Watchtower.\\\" Hendrix died at the age of 27, becoming one of the most iconic members of the so-called \\\"27 Club.\\\"\"}}]} 3rd Request As expected, for a non-related request, the AI Gateway will hit the LLM to satisfy the query:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who was Joseph Conrad?\" } ] }' Expected response HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-Cache-Status: Miss openai-version: 2020-10-01 x-envoy-upstream-service-time: 4746 Date: Tue, 12 Aug 2025 14:49:35 GMT x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 CF-RAY: 96e0c7a088ac1b20-GRU x-ratelimit-remaining-tokens: 29992 alt-svc: h3=\":443\"; ma=86400 access-control-expose-headers: X-Request-ID X-Content-Type-Options: nosniff Server: cloudflare openai-organization: user-4qzstwunaw6d1dhwnga5bc5q Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-request-id: req_10dee9e2989e46cbb32a2125f774f446 openai-processing-ms: 4658 cf-cache-status: DYNAMIC openai-project: proj_r4KYFyenuGWthS5te4zaurNN x-ratelimit-reset-tokens: 16ms x-ratelimit-reset-requests: 120ms X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2515 X-Kong-Upstream-Latency: 4996 X-Kong-Proxy-Latency: 2222 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: b5fadb69431d1234d8bbd53a71abc559 { \"id\": \"chatcmpl-C3kbbMudkoKV6rrzvOHz0lQ8IH0Ci\", \"object\": \"chat.completion\", \"created\": 1755010171, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Joseph Conrad** (born Józef Teodor Konrad Korzeniowski; 1857–1924) was a Polish-British writer widely regarded as one of the great novelists writing in English, despite the fact that English was not his first language. He was born in Berdychiv, in the Russian Empire (now in Ukraine), to Polish parents.\\n\\n**Background:**\\n- **Early Life:** Conrad’s parents were exiled for their involvement in Polish independence movements. Orphaned at a young age, he spent much of his youth in Poland and later France.\\n- **Seafaring Career:** In his twenties, Conrad became a merchant marine, traveling around the world and eventually settling in England. He gained British citizenship in 1886.\\n\\n**Literary Career:**\\n- He began writing novels and short stories in English, starting with *Almayer’s Folly* (1895).\\n- **Notable works** include:\\n - *Heart of Darkness* (1899)\\n - *Lord Jim* (1900)\\n - *Nostromo* (1904)\\n - *The Secret Agent* (1907)\\n- His novels often deal with themes of isolation, existential doubt, imperialism, and the complexity of human nature.\\n\\n**Legacy:**\\n- Conrad’s innovative narrative techniques and psychological depth influenced modernist literature and writers such as Virginia Woolf, T.S. Eliot, and William Faulkner.\\n- *Heart of Darkness*, a novella about a journey into the Congo, is considered one of the most important works of 20th-century literature and has inspired many adaptations, including the film *Apocalypse Now*.\\n\\n**Summary:** \\nJoseph Conrad was a Polish-born novelist who wrote in English and became one of the leading literary figures of his time, celebrated for his adventure tales, deep psychological insight, and exploration of moral ambiguity.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 12, \"completion_tokens\": 383, \"total_tokens\": 395, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_799e4ca3f1\" } Check Redis again Redis database has two entries now:\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan Expected response \"kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36\" \"kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:42aa94b4bbbedce497e59e1fd0fc617683a43b58ac7e306a47feb46f502f1499\" Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.",
    "description": "Semantic caching enhances data retrieval efficiency by focusing on the meaning or context of queries rather than just exact matches. It stores responses based on the underlying intent and semantic similarities between different queries and can then retrieve those cached queries when a similar request is made.\nWhen a new request is made, the system can retrieve and reuse previously cached responses if they are contextually relevant, even if the phrasing is different. This method reduces redundant processing, speeds up response times, and ensures that answers are more relevant to the user’s intent, ultimately improving overall system performance and user experience.",
    "tags": [],
    "title": "AI Semantic Cache",
    "uri": "/16-ai-gateway/17-use-cases/156-semantic-cache/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "Add Rate Limiting plugin Just like you did before, add the Rate Limiting plugin on the Route setting Minute as 5 requests per minute, and set the identifier to Service.\ncat \u003e rate-limiting.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: rate-limiting-route paths: - /rate-limiting-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 5 EOF Submit the declaration:\ndeck gateway sync --konnect-token $PAT rate-limiting.yaml Verify traffic control Again, test the rate-limiting policy by executing the following command multiple times and observe the rate-limit headers in the response, specially, X-RateLimit-Remaining-Minute, RateLimit-Reset and Retry-After :\ncurl -I $DATA_PLANE_LB/rate-limiting-route/get Response\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 389 Connection: keep-alive X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 4 RateLimit-Reset: 44 RateLimit-Remaining: 4 RateLimit-Limit: 5 Server: gunicorn Date: Mon, 11 Aug 2025 14:55:16 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 10 X-Kong-Proxy-Latency: 5 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 7f13e52db83e4e673798120134496d03 As explected, after sending too many requests,once the rate limiting is reached, you will see HTTP/1.1 429 Too Many Requests\n# curl -I $DATA_PLANE_LB/rate-limiting-route/get HTTP/1.1 429 Too Many Requests Date: Mon, 11 Aug 2025 14:55:20 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 0 RateLimit-Reset: 40 Retry-After: 40 RateLimit-Remaining: 0 RateLimit-Limit: 5 Content-Length: 92 X-Kong-Response-Latency: 2 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: c01dcb02ea13676ca5e49e7e1c40982b Results As there is a single Kong Data Plane Runtime instance running, Kong correctly imposes the rate-limit and you can make only 5 requests in a minute.",
    "description": "Add Rate Limiting plugin Just like you did before, add the Rate Limiting plugin on the Route setting Minute as 5 requests per minute, and set the identifier to Service.\ncat \u003e rate-limiting.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: rate-limiting-route paths: - /rate-limiting-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 5 EOF Submit the declaration:",
    "tags": [],
    "title": "Rate Limiting",
    "uri": "/12-api-gateway/15-use-cases/156-setup-rate-limiting-plugin/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "In this section, you will configure the Key-Auth plugin on the Kong Route to protect Amazon Bedrock.\nAdd Kong Key Authentication plugin and Kong Consumer Add a KongPlugin resource for authentication, specifically the Key-Auth plugin. Note that, besides describing the plugin configuration, the declaration also creates a Kong Consumer, named user1, with an API Key (123456) as its credential.\ncat \u003e ai-key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth-bedrock enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-key-auth.yaml Verify authentication is required New requests now require authentication\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expect response The response is a HTTP/1.1 401 Unauthorized, meaning the Kong Gateway Service requires authentication.\nHTTP/1.1 401 Unauthorized Date: Tue, 12 Aug 2025 14:53:42 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Key Content-Length: 96 X-Kong-Response-Latency: 1 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: fd1bc16647271a20b7245b0cc9eb5052 { \"message\":\"No API key found in request\", \"request_id\":\"fd1bc16647271a20b7245b0cc9eb5052\" } Send another request with an API key Use the apikey to pass authentication to access the services.\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' The request should now respond with a HTTP/1.1 200 OK.\nWhen submitting requests, the API Key name is defined, by default, apikey. You can change the plugin configuration, if you will.",
    "description": "In this section, you will configure the Key-Auth plugin on the Kong Route to protect Amazon Bedrock.\nAdd Kong Key Authentication plugin and Kong Consumer Add a KongPlugin resource for authentication, specifically the Key-Auth plugin. Note that, besides describing the plugin configuration, the declaration also creates a Kong Consumer, named user1, with an API Key (123456) as its credential.\ncat \u003e ai-key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth-bedrock enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Key Auth",
    "uri": "/16-ai-gateway/17-use-cases/157-apikey/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases",
    "content": "For advanced and enterprise class requirements, OpenID Connect (OIDC) is the preferred option to implement API consumer Authentication. OIDC also provides mechanisms to implement Authorization. In fact, when applying OIDC to secure the APIs, we’re delegating the Authentication process to the external Identity Provider entity.\nOpenID Connect is an authentication protocol built on top of OAuth 2.0 and JWT - JSON Web Token to add login and profile information about the identity who is logged in.\nOAuth 2.0 defines grant types for different use cases. The most common ones are:\nAuthorization Code: for apps running on a web server, browser-based and mobile apps for user authentication. Client Credentials: for application authentication. PKCE - Proof Key for Code Exchange: an extension to the Authorization Code grant. Recommended for SPA or native applications, PKCE acts like a non hard-coded secret. OpenId Connect plugin Konnect provides an OIDC plugin that fully supports the OAuth 2.0 grants. The plugin allows the integration with a 3rd party identity provider (IdP) in a standardized way. This plugin can be used to implement Kong as a (proxying) OAuth 2.0 resource server (RS) and/or as an OpenID Connect relying party (RP) between the client, and the upstream service.\nAs an example, here’s the typical topology and the Authorization Code with PKCE grant:\nConsumer sends a request to Kong Data Plane. Since the API is being protected with the OIDC plugin, the Data Plane redirects the consumer to the IdP. Consumer provides credentials to the Identity Provide (IdP). IdP authenticates the consumer enforcing security policies previously defined. The policies might involve several database technologies (e.g. LDAP, etc.), MFA (Multi-Factor Authentication), etc. After user authentication, IdP redirects the consumer back to the Data Plane with the Authorization Code injected inside the request. Data Plane sends a request to the IdP’s token endpoint with the Authorization Code and gets an Access Token from the IdP. Data Plane routes the request to the upstream service along with the Access Token Once again, it’s important to notice that one of the main benefits provided by an architecture like this is to follow the Separation of Concerns principle:\nIdentity Provider: responsible for User and Application Authentication, Tokenization, MFA, multiples User Databases abstraction, etc. API Gateway: responsible for exposing the Upstream Services and controlling their consumption through an extensive list of policies besides Authentication including Rate Limiting, Caching, Log Processing, etc. In this module, we will configure this plugin to use Kong Identity.\nYou can now click Next to proceed further.",
    "description": "For advanced and enterprise class requirements, OpenID Connect (OIDC) is the preferred option to implement API consumer Authentication. OIDC also provides mechanisms to implement Authorization. In fact, when applying OIDC to secure the APIs, we’re delegating the Authentication process to the external Identity Provider entity.\nOpenID Connect is an authentication protocol built on top of OAuth 2.0 and JWT - JSON Web Token to add login and profile information about the identity who is logged in.",
    "tags": [],
    "title": "OpenID Connect",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "With the existing API Key policy, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIn this section, we are going to define specific Rate Limiting policies for each Consumer represented by its API Key.\nKong Consumer Policies It’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:\nconsumer1: apikey = 123456 rate limiting policy = 500 tokens per minute consumer2: apikey = 987654 rate limiting policy = 10000 tokens per minute Doing that, the Data Plane is capable to not just protect the Route but to identify the consumer based on the key injected to enforce specific policies to the consumer. Keep in mind that a Consumer might have other plugins also enabled such as TCP Log, etc.\nNew Consumer and AI Rate Limiting Advanced plugin Policies Then, create the second consumer2, just like you did with the first one, with the 987654 key. Both Kong Consumers have the AI Rate Limiting Advanced plugin enabled with specific configurations.\ncat \u003e ai-key-auth-rate-limiting-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth-bedrock enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 plugins: - name: ai-rate-limiting-advanced instance_name: ai-rate-limiting-advanced-consumer1 config: llm_providers: - name: openai window_size: - 60 limit: - 500 - keyauth_credentials: - key: \"987654\" username: user2 plugins: - name: ai-rate-limiting-advanced instance_name: ai-rate-limiting-advanced-consumer2 config: llm_providers: - name: openai window_size: - 60 limit: - 10000 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-key-auth-rate-limiting-advanced.yaml Use both Kong Consumers If you will, you can inject both keys to your requests.\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected output HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 500 X-AI-RateLimit-Remaining-minute-openai: 500 openai-version: 2020-10-01 x-envoy-upstream-service-time: 8059 Date: Tue, 12 Aug 2025 15:26:01 GMT x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 CF-RAY: 96e0fce49c204ee9-GRU x-ratelimit-remaining-tokens: 29993 alt-svc: h3=\":443\"; ma=86400 access-control-expose-headers: X-Request-ID X-Content-Type-Options: nosniff Server: cloudflare openai-organization: user-4qzstwunaw6d1dhwnga5bc5q Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-request-id: req_f230ff10e7374c7a8b2fc292a3cc0685 openai-processing-ms: 7964 cf-cache-status: DYNAMIC openai-project: proj_r4KYFyenuGWthS5te4zaurNN x-ratelimit-reset-tokens: 14ms x-ratelimit-reset-requests: 120ms X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2058 X-Kong-Upstream-Latency: 8798 X-Kong-Proxy-Latency: 4 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 379bd531f3411379f613083969a88c07 { \"id\": \"chatcmpl-C3lAneT4SCgIa50fe89CYPadtBdfQ\", \"object\": \"chat.completion\", \"created\": 1755012353, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Jimi Hendrix (born **James Marshall Hendrix**, November 27, 1942 – September 18, 1970) was an American guitarist, singer, and songwriter. He is widely regarded as one of the greatest and most influential electric guitarists in the history of popular music.\\n\\n**Career Highlights:**\\n- Hendrix gained fame in the late 1960s with his band, **The Jimi Hendrix Experience**.\\n- Some of his most famous songs include **\\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"The Wind Cries Mary,\\\"** and **\\\"All Along the Watchtower.\\\"**\\n- He was known for his innovative guitar techniques, including feedback, distortion, and wah-wah, and his energetic and theatrical performance style.\\n- Hendrix's legendary performances include his rendition of \\\"The Star-Spangled Banner\\\" at **Woodstock** in 1969.\\n\\n**Legacy:**\\n- Despite his short career (he died at age 27), Hendrix's music and style had a major impact on rock, blues, and modern guitar playing.\\n- He was posthumously inducted into the **Rock and Roll Hall of Fame** in 1992.\\n- Hendrix is consistently ranked among the greatest guitarists of all time by music publications and critics.\\n\\n**Fun Fact:** \\nJimi Hendrix is part of the so-called \\\"27 Club,\\\" a group of influential musicians who died at the age of 27.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 285, \"total_tokens\": 298, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_b3f1157249\" } or\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 987654' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected output HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 10000 X-AI-RateLimit-Remaining-minute-openai: 10000 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 Date: Tue, 12 Aug 2025 15:26:41 GMT x-ratelimit-remaining-tokens: 29993 access-control-expose-headers: X-Request-ID openai-organization: user-4qzstwunaw6d1dhwnga5bc5q openai-processing-ms: 6376 x-ratelimit-reset-requests: 120ms openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-cache-status: DYNAMIC openai-version: 2020-10-01 Server: cloudflare X-Content-Type-Options: nosniff x-envoy-upstream-service-time: 6463 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload CF-RAY: 96e0fdeec8b7ae57-GRU x-ratelimit-limit-requests: 500 x-request-id: req_f8c4559cbdac4d1bbff938b61a054f3d x-ratelimit-reset-tokens: 14ms alt-svc: h3=\":443\"; ma=86400 X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2324 X-Kong-Upstream-Latency: 6709 X-Kong-Proxy-Latency: 3 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 6e2daa0a80e95772fa46e15637278177 { \"id\": \"chatcmpl-C3lBT2cDacuuICQzugJ5CCMbWtCFV\", \"object\": \"chat.completion\", \"created\": 1755012395, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (born **James Marshall Hendrix**, November 27, 1942 – September 18, 1970) was an American guitarist, singer, and songwriter. Widely regarded as one of the greatest and most influential electric guitarists in the history of popular music, Hendrix is known for his innovative playing style, including groundbreaking use of guitar effects and techniques like feedback and distortion.\\n\\n### Brief Biography:\\n- **Early Life:** Born in Seattle, Washington; started playing guitar as a teenager.\\n- **Career:** Gained prominence in the mid-1960s after moving to England and forming the **Jimi Hendrix Experience** with bassist Noel Redding and drummer Mitch Mitchell.\\n- **Famous Albums:** \\n - *Are You Experienced* (1967)\\n - *Axis: Bold as Love* (1967)\\n - *Electric Ladyland* (1968)\\n- **Iconic Performances:** \\n - Monterey Pop Festival (1967)\\n - Woodstock Festival (1969), where he famously reinterpreted “The Star-Spangled Banner.”\\n\\n### Legacy:\\nHendrix’s innovative approach fused rock, blues, and psychedelia. His use of the wah-wah pedal, feedback, and studio effects transformed notions of what the electric guitar could do. Despite his death at the age of 27, his influence persists across genres and generations.\\n\\n### Key Songs:\\n- “Purple Haze”\\n- “Hey Joe”\\n- “All Along the Watchtower”\\n- “Voodoo Child (Slight Return)”\\n- “Little Wing”\\n\\nHendrix is a major figure in rock history and is frequently cited in “greatest guitarists of all time” lists.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 346, \"total_tokens\": 359, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_799e4ca3f1\" } Again, test the rate-limiting policy by executing the following command multiple times and observe the rate-limit headers in the response, specially, X-AI-RateLimit-Limit-minute-openai and X-AI-RateLimit-Remaining-minute-openai:\nNow, let’s consume it with the Consumer1’s API Key. As you can see the Data Plane is processing the Rate Limiting processes independently.\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 500 X-AI-RateLimit-Remaining-minute-openai: 110 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 Date: Tue, 12 Aug 2025 15:29:05 GMT x-ratelimit-remaining-tokens: 29993 access-control-expose-headers: X-Request-ID openai-organization: user-4qzstwunaw6d1dhwnga5bc5q openai-processing-ms: 4385 x-ratelimit-reset-requests: 120ms openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-cache-status: DYNAMIC openai-version: 2020-10-01 Server: cloudflare X-Content-Type-Options: nosniff x-envoy-upstream-service-time: 4484 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload CF-RAY: 96e10178faf000f1-GRU x-ratelimit-limit-requests: 500 x-request-id: req_b4611580cac4476288895c6315847b8b x-ratelimit-reset-tokens: 14ms alt-svc: h3=\":443\"; ma=86400 X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 1796 X-Kong-Upstream-Latency: 4881 X-Kong-Proxy-Latency: 1 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 8b4285448ad3b335f766d33c61a37851 If we keep sending requests using the first API Key, eventually, as expected, we’ll get an error code:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/1.1 429 Too Many Requests Date: Tue, 12 Aug 2025 15:29:52 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 500 X-AI-RateLimit-Remaining-minute-openai: 0 X-AI-RateLimit-Retry-After-minute-openai: 9 X-AI-RateLimit-Reset: 9 X-AI-RateLimit-Retry-After: 9 X-AI-RateLimit-Reset-minute-openai: 9 Content-Length: 66 X-Kong-Response-Latency: 1 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: c3a2a05b8f77b264230f041c527ade71 {\"message\":\"AI token rate limit exceeded for provider(s): openai\"} However, the second API Key is still allowed to consume the Kong Route:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 987654' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 10000 X-AI-RateLimit-Remaining-minute-openai: 10000 openai-version: 2020-10-01 x-envoy-upstream-service-time: 5053 Date: Tue, 12 Aug 2025 15:30:26 GMT x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 CF-RAY: 96e103739ca64292-VCP x-ratelimit-remaining-tokens: 29993 alt-svc: h3=\":443\"; ma=86400 access-control-expose-headers: X-Request-ID X-Content-Type-Options: nosniff Server: cloudflare openai-organization: user-4qzstwunaw6d1dhwnga5bc5q Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-request-id: req_379758f4fcd549acb57c7e6d911b5a89 openai-processing-ms: 5014 cf-cache-status: DYNAMIC openai-project: proj_r4KYFyenuGWthS5te4zaurNN x-ratelimit-reset-tokens: 14ms x-ratelimit-reset-requests: 120ms X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 1998 X-Kong-Upstream-Latency: 5602 X-Kong-Proxy-Latency: 2 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 118caf09b62c8f4f78771c182f736b00 { \"id\": \"chatcmpl-C3lF7AC6zeB8NOBFQn5Xeacn6Ntf7\", \"object\": \"chat.completion\", \"created\": 1755012621, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (born James Marshall Hendrix on November 27, 1942 – September 18, 1970) was an iconic American guitarist, singer, and songwriter. Widely regarded as one of the most influential electric guitarists in the history of popular music, Hendrix is celebrated for his innovative style, virtuosic playing, and groundbreaking use of guitar effects such as distortion, feedback, and wah-wah pedals.\\n\\nHendrix first gained fame in the mid-1960s after moving to London and forming the **Jimi Hendrix Experience** with bassist Noel Redding and drummer Mitch Mitchell. The band released classic albums such as *Are You Experienced* (1967), *Axis: Bold as Love* (1967), and *Electric Ladyland* (1968), featuring hit songs like \\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"The Wind Cries Mary,\\\" \\\"Voodoo Child (Slight Return),\\\" and his legendary rendition of \\\"The Star-Spangled Banner\\\" performed at Woodstock in 1969.\\n\\nTragically, Hendrix died at the young age of 27. Despite his short career, his influence continues to shape rock, blues, and popular music to this day. He is consistently ranked among the greatest guitarists of all time and has been inducted into the Rock and Roll Hall of Fame.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 271, \"total_tokens\": 284, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_51e1070cf2\" } Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.",
    "description": "With the existing API Key policy, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIn this section, we are going to define specific Rate Limiting policies for each Consumer represented by its API Key.\nKong Consumer Policies It’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:",
    "tags": [],
    "title": "AI Rate Limiting Advanced",
    "uri": "/16-ai-gateway/17-use-cases/158-rate-limiting/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.\nThe plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nLoad balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:\nLowest-usage Round-robin (weighted) Consistent-hashing (sticky-session on given header value) Semantic routing The AI Proxy Advanced plugin supports semantic routing, which enables distribution of requests based on the similarity between the prompt and the description of each model. This allows Kong to automatically select the model that is best suited for the given domain or use case.\nBy analyzing the content of the request, the plugin can match it to the most appropriate model that is known to perform better in similar contexts. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.\nAs a illustration here is the architecture where we are going to implement the multiple load balancing policies. AI Proxy Advanced will manage both LLMs:\ngpt-4.1 llama3.2:1b You can now click Next to proceed further.",
    "description": "The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.\nThe plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nLoad balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:",
    "tags": [],
    "title": "AI Proxy Advanced",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway",
    "content": "With the rapid emergence of multiple AI LLM providers, the AI technology landscape is fragmented and lacking in standards and controls. Kong AI Gateway is a powerful set of features built on top of Kong Gateway, designed to help developers and organizations effectively adopt AI capabilities quickly and securely\nWhile AI providers don’t conform to a standard API specification, the Kong AI Gateway provides a normalized API layer allowing clients to consume multiple AI services from the same client code base. The AI Gateway provides additional capabilities for credential management, AI usage observability, governance, and tuning through prompt engineering. Developers can use no-code AI Plugins to enrich existing API traffic, easily enhancing their existing application functionality.\nYou can enable the AI Gateway features through a set of specialized plugins, using the same model you use for any other Kong Gateway plugin.\nKong AI Gateway functional scope Universal API Kong’s AI Gateway Universal API, delivered through the AI Proxy and AI Proxy Advanced plugins, simplifies AI model integration by providing a single, standardized interface for interacting with models across multiple providers.\nEasy to use: Configure once and access any AI model with minimal integration effort.\nLoad balancing: Automatically distribute AI requests across multiple models or providers for optimal performance and cost efficiency.\nRetry and fallback: Optimize AI requests based on model performance, cost, or other factors.\nCross-plugin integration: Leverage AI in non-AI API workflows through other Kong Gateway plugins.\nHigh Level Tasks You will complete the following:\nSet up Kong AI Proxy for LLM Integration Implement Kong AI Plugins to secure prompt message You can now click Next to proceed further.",
    "description": "With the rapid emergence of multiple AI LLM providers, the AI technology landscape is fragmented and lacking in standards and controls. Kong AI Gateway is a powerful set of features built on top of Kong Gateway, designed to help developers and organizations effectively adopt AI capabilities quickly and securely\nWhile AI providers don’t conform to a standard API specification, the Kong AI Gateway provides a normalized API layer allowing clients to consume multiple AI services from the same client code base. The AI Gateway provides additional capabilities for credential management, AI usage observability, governance, and tuning through prompt engineering. Developers can use no-code AI Plugins to enrich existing API traffic, easily enhancing their existing application functionality.",
    "tags": [],
    "title": "Introduction",
    "uri": "/16-ai-gateway/159-ai-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "In this second part of the workshop we are going to explore the AI capabilities provides by Kong AI Gateway and the specific collection of plugins.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Kong AI Gateway",
    "description": "In this second part of the workshop we are going to explore the AI capabilities provides by Kong AI Gateway and the specific collection of plugins.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Kong AI Gateway",
    "tags": [],
    "title": "Kong AI Gateway",
    "uri": "/16-ai-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway \u003e Use Cases",
    "content": "What is RAG? RAG offers a solution to some of the limitations in traditional generative AI models, such as accuracy and hallucinations, allowing companies to create more contextually relevant AI applications.\nBasically, the RAG application comprises two main processes:\nData Preparation: External data sources, including all sorts of documents, are chunked and submitted to an Embedding Model which converts them into embeddings. The embeddings are stored in a Vector Database.\nQuery time: A consumer wants to send a query to the actual Prompt/Chat LLM model. However, the query should be enhanced with relevant data, taken from the Vector Database, and not available in the LLM model. The following steps are then performed:\nThe consumer builds a Prompt. The RAG application converts the Prompt into an embedding, calling the Embedding Model. Leveraging Semantic Search, RAG matches the Prompt Embedding with the most relevant information and retrieves that Vector Database. The Vector Database returns relevant data as a response to the search query. The RAG application sends a query to the Prompt/Chat LLM Model combining the Prompt with the Relevant Data returned by the Vector Database. The LLM Model returns a response. Implementation Architecture Data Preparation time During the preparation time, the following steps are executed:\nThe Document Loader script asks the AI RAG Injector plugin to provides the configuration regarding both Embedding Model and Vector Database. The Document Loader sends data chunks to the Embedding Model to gets the embeddings related to them The Document Loader stores the embeddings and content into the Vector Database. RAG time The following steps are performed during the execution time:\nThe API Consumer send a request with a prompt. The AI RAG Injector Plugin converts the prompt into embeddings calling the Embedding Model. The AI RAG Injector Plugin sends a KNN vector search query to the Vector Database to find the top “k-nearest neighbors” to a query vector. The AI Proxy Advanced Plugin sends a regular request to the LLM model adding the relevant data received from the Vector Database. Here’s the declaration including both plugins: AI RAG Injector and AI Proxy Advanced.\ncat \u003e ai-rag-injector.yaml \u003c\u003c 'EOF' _format_version: '3.0' _konnect: control_plane_name: kong-workshop services: - name: ai-proxy url: http://localhost:65535 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - logging: log_statistics: true route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-5 options: temperature: 1.0 - name: ai-rag-injector id: 3194f12e-60c9-4cb6-9cbc-c8fd7a00cff1 instance_name: ai-rag-injector1 config: inject_template: | Only use the following information surrounded by \u003cRAG\u003e\u003c/RAG\u003e to and your existing knowledge to provide the best possible answer to the user. \u003cRAG\u003e\u003cCONTEXT\u003e\u003c/RAG\u003e User's question: \u003cPROMPT\u003e fetch_chunks_count: 1 embeddings: auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: \"text-embedding-3-small\" vectordb: strategy: redis redis: host: redis-stack.redis port: 6379 distance_metric: cosine dimensions: 1024 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-rag-injector.yaml If you send a request with no context, we’ll see the LLM is not able to respond to it:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header \"Content-Type: application/json\" \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"What did Marco say about AI Gateways?\" } ] }' | jq '.choices[].message.content' Typical response: \"Could you clarify which Marco you mean and where he said it (e.g., a podcast, post, talk, or email)? If you can share a link, quote, or more context, I can find and summarize exactly what he said about AI Gateways.\" We are going to inject some context using an interview transcript snippet Marco Palladino, Kong’s co-founder and CTO, gave some time ago.\nThe transcript snippet is available in the following file.\ncurl 'https://raw.githubusercontent.com/Kong/workshop-kong-api-ai-gateway/refs/heads/main/content/static/code/SED1683-Kong-short.txt' --output ./SED1683-Kong-short.txt We have to copy the Document Loader script, algo available in a file, to the Data Plane:\ncurl 'https://raw.githubusercontent.com/Kong/workshop-kong-api-ai-gateway/refs/heads/main/content/static/code/ingest_update.lua' --output ./ingest_update.lua kubectl cp ./ingest_update.lua -n kong $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name'):/tmp/ingest_update.lua Now, execute the Document Loader passing the content as a parameter. Note that, to make to process a bit easier, we have created the AI RAG Injector plugin with a pre-defined id.\nkubectl exec -ti $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name') -n kong -- kong runner /tmp/ingest_update.lua 3194f12e-60c9-4cb6-9cbc-c8fd7a00cff1 \"'\"$(cat ./SED1683-Kong-short.txt)\"'\" You should be able to get a much better response from the LLM model this time:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header \"Content-Type: application/json\" \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"What did Marco say about AI Gateways?\" } ] }' | jq '.choices[].message.content' \"Marco said that for AI use cases you need an AI gateway to connect multiple LLMs and orchestrate them. He added that Kong can deploy its gateway in this AI gateway role (alongside edge gateway and service mesh) under a unified control plane, so you can see, manage, monitor, consume, and expose APIs consistently across these use cases.\" If you Redis, you’ll se there a new entry for RAG Injector\n% kubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan \"kong_rag_injector:7e9d1404-5cfb-4585-bada-132e6d6595c1\"",
    "description": "What is RAG? RAG offers a solution to some of the limitations in traditional generative AI models, such as accuracy and hallucinations, allowing companies to create more contextually relevant AI applications.\nBasically, the RAG application comprises two main processes:\nData Preparation: External data sources, including all sorts of documents, are chunked and submitted to an Embedding Model which converts them into embeddings. The embeddings are stored in a Vector Database.",
    "tags": [],
    "title": "RAG - Retrieval-Augmented Generation",
    "uri": "/16-ai-gateway/17-use-cases/170-rag/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong AI Gateway",
    "content": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nSimple AI Gateway Proxy Prompt Engineering AI Request and Response Tranformers AI Semantic Cache AI Rate Limiting AI Proxy Advanced and load balancing algoritms RAG These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong AI plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "description": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nSimple AI Gateway Proxy Prompt Engineering AI Request and Response Tranformers AI Semantic Cache AI Rate Limiting AI Proxy Advanced and load balancing algoritms RAG These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong AI plugins on the Plugin Hub.",
    "tags": [],
    "title": "Use Cases",
    "uri": "/16-ai-gateway/17-use-cases/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "Basically Konnect provides two main Builtin Observability services:\nKonnect Advanced Analytics: it is a real-time, contextual analytics platform that provides insights into API health, performance, and usage.\nKonnect Debugger: it provides a connected debugging experience and real-time trace-level visibility into API traffic.",
    "description": "Basically Konnect provides two main Builtin Observability services:\nKonnect Advanced Analytics: it is a real-time, contextual analytics platform that provides insights into API health, performance, and usage.\nKonnect Debugger: it provides a connected debugging experience and real-time trace-level visibility into API traffic.",
    "tags": [],
    "title": "Konnect Builtin Observability",
    "uri": "/20-observability/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Builtin Observability",
    "content": "Konnect Advanced Analytics is a real-time, highly contextual analytics platform that provides deep insights into API health, performance, and usage. It helps businesses optimize their API strategies and improve operational efficiency. This feature is offered as a premium service within Konnect.\nKey benefits:\nCentralized visibility: Gain insights across all APIs, Services, and data planes. Contextual API analytics: Analyze API requests, Routes, Consumers, and Services. Democratized data insights: Generate reports based on your needs. Fast time to insight: Retrieve critical API metrics in less than a second. Reduced cost of ownership: A turn-key analytics solution without third-party dependencies. Enabling data ingestion Manage data ingestion from any Control Plane Dashboard using the Advanced Analytics toggle. This toggle lets you enable or disable data collection for your API traffic per control plane.\nModes:\nOn: Both basic and advanced analytics data is collected, allowing in-depth insights and reporting. Off: Advanced analytics collection stops, but basic API metrics remain available in Gateway Manager, and can still be used for custom reports. Explorer Interface The Explorer interface displays API usage data gathered by Konnect Analytics from your Data Plane nodes. You can use this tool to:\nDiagnose performance issues Monitor LLM token consumption and costs Capture essential usage metrics The Analytics Explorer also lets you save the output as a custom report.\nCheck the Advanced Analytics Explorer documentation to learn more.\nDashboards The Summary Dashboard shows performance and health statistics of all your APIs across your organization on a single page and provides insights into your Service usage.\nCustom Dasboards Advanced Analytics includes the ability to build organization-specific views with Custom Dashboards. You can create them from scratch or use existing templates. The functionality is powered by a robust API, and Terraform integration.\nCreate a dashboard You can create custom dashboards either from scratch or from a template. In this tutorial, we’ll use a template.\nTo create a custom dashboard, do the following:\nIn Konnect, navigate to Dashboards in the sidebar. From the Create dashboard dropdown menu, select “Create from template”. Click Quick summary dashboard. Click Use template. This creates a new template with pre-configured tiles. Add a filter Filters help you narrow down the data shown in charts without modifying individual tiles.\nFor this example, let’s add a filter so that the data shown in the dashboard is scoped to only one control plane:\nFrom the dashboard, click Add filter. This brings up the configuration options. Select “Control plane” from the Filter by dropdown menu. Select “In” from the Operator dropdown menu. Select “kong-workshop from the Filter value dropdown menu. Select the Make this a preset for all viewers checkbox. Click Apply. This applies the filter to the dashboard. Anyone that views this dashboard will be viewing it scoped to the filter you created.\nCheck the Advanced Analytics Custom Dashboards documentation to learn more.\nRequests The Requests options shows all requests that have been processed by the Data Planes. For example, here’s the requests processed by the Data Planes created for the kong-workshop Control Plane.",
    "description": "Konnect Advanced Analytics is a real-time, highly contextual analytics platform that provides deep insights into API health, performance, and usage. It helps businesses optimize their API strategies and improve operational efficiency. This feature is offered as a premium service within Konnect.\nKey benefits:\nCentralized visibility: Gain insights across all APIs, Services, and data planes. Contextual API analytics: Analyze API requests, Routes, Consumers, and Services. Democratized data insights: Generate reports based on your needs. Fast time to insight: Retrieve critical API metrics in less than a second. Reduced cost of ownership: A turn-key analytics solution without third-party dependencies. Enabling data ingestion Manage data ingestion from any Control Plane Dashboard using the Advanced Analytics toggle. This toggle lets you enable or disable data collection for your API traffic per control plane.",
    "tags": [],
    "title": "Konnect Advanced Analytics",
    "uri": "/20-observability/21-builtin/211-advanced_analytics/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Konnect Builtin Observability",
    "content": "Konnect Debugger provides a connected debugging experience and real-time trace-level visibility into API traffic, enabling you to:\nTroubleshoot issues: Investigate and resolve problems during deployments or incidents with targeted, on-demand traces. Understand request lifecycle: Visualize exactly what happened during a specific request, including order and duration plugin execution. See Debugger spans for a list of spans captured. Improve performance and reliability: Use deep insights to fine-tune configurations and resolve bottlenecks. Capture traces and logs Konnect Debugger allows you to capture traces and logs.\nTraces Traces provide a visual representation of the request and response lifecycle, offering a comprehensive overview of Kong’s request processing pipeline.\nThe debugger helps capture OpenTelemetry-compatible traces for all requests matching the sampling criteria. The detailed spans are captured for the entire request/response lifecycle. These traces can be visualized with Konnect’s built-in span viewer with no additional instrumentation or telemetry tools. For a complete list of available spans and their meanings, see Debugger spans.\nKey Highlights Traces can be generated for a service or per route Refined traces can be generated for all requests matching a sampling criteria Sampling criteria can be defined with simple expressions language, for example: http.method == GET Trace sessions are retained for up to 7 days Traces can be visualized in Konnect’s built in trace viewer To ensure consistency and interoperability, tracing adheres to OpenTelemetry naming conventions for spans and attributes, wherever possible.\nLogs For deeper insights, logs can be captured along with traces. When initiating a debug session, administrators can choose to capture logs. Detailed Kong Gateway logs are captured for the duration of the session. These logs are then correlated with traces using trace_id and span_id providing a comprehensive and drill-down view of logs generated during specific trace or span.\nReading traces and logs Traces captured during a debug session can be visualized in debugger’s built-in trace viewer. The trace viewer displays Summary, Spans and Logs view. You can gain instant insights with the summary view while the spans and logs view help you to dive deeper.\nSummary view Summary view helps you visualize the entire API request-response flow in a single glance. This view provides a concise overview of critical latency metrics and a transaction map. The lifecycle map includes the different phases of Kong Gateway and the plugins executed by Kong Gateway on both the request and the response along with the times spent in each phase. Use the summary view to quickly understand the end-to-end API flow, identify performance bottlenecks, and optimize your API strategy.\nSpans view The span view gives you unparalleled visibility into Kong Gateway’s internal workings. This detailed view breaks down into individual spans, providing a comprehensive understanding of:\nKong Gateway’s internal processes and phases Plugin execution and performance Request and response handling For detailed definitions of each span, see Debugger spans. Use the span view to troubleshoot issues, optimize performance, and refine your configuration.\nLogs View A drill-down view of all the logs generated during specific debug session are shown in the logs tab. All the spans in the trace are correlated using trace_id and span_id. The logs can be filtered on log level and spans. Logs are displayed in reverse chronological order. Konnect encrypts all the logs that are ingested. You can further ensure complete privacy and control by using customer-managed encryption keys (CMEK). Use the logs view to quickly troubleshoot and pinpoint issues.\nData Security with Customer-Managed Encryption Keys (CMEK) By default, logs are automatically encrypted using encryption keys that are owned and managed by Konnect. However if you have a specific compliance and regulatory requirements related to the keys that protect your data, you can use the customer-managed encryption keys. This ensures that sensitive data are secured for each organization with their own key and nobody, including Konnect, has access to that data. For more information about how to create and manage CMEK keys, see Customer-Managed Encryption Keys (CMEK).\nStart your first debug session To begin using the Debugger, ensure the following requirements are met:\nYour data plane nodes are running Kong Gateway version 3.9.1 or later. Logs require Kong Gateway version 3.11.0 or later. Your Konnect data planes are hosted using self-managed hybrid, Dedicated Cloud Gateways, or serverless gateways. Kong Ingress Controller or Kong Native Event Proxy Gateways aren’t currently supported. In Gateway Manager, select the control plane that contains the data plane to be traced. In the left navigation menu, click Debugger. Click New session. Define the sampling criteria and click Start Session. Once the session starts, traces will be captured for requests that match the rule. Click a trace to view it in the span viewer.\nEach session can be configured to run for a time between 10 seconds and 30 minutes. Sessions are retained for up to 7 days.\nFor details on defining sampling rules, see Debugger sessions.\nSampling rules Sampling rules help you capture only relevant traffic. Requests that match the defined criteria are included in the session. There are two types:\nBasic sampling rules: Filter by Route or Service. Advanced sampling rules: Use expressions for fine-grained filtering.",
    "description": "Konnect Debugger provides a connected debugging experience and real-time trace-level visibility into API traffic, enabling you to:\nTroubleshoot issues: Investigate and resolve problems during deployments or incidents with targeted, on-demand traces. Understand request lifecycle: Visualize exactly what happened during a specific request, including order and duration plugin execution. See Debugger spans for a list of spans captured. Improve performance and reliability: Use deep insights to fine-tune configurations and resolve bottlenecks. Capture traces and logs Konnect Debugger allows you to capture traces and logs.",
    "tags": [],
    "title": "Konnect Debugger",
    "uri": "/20-observability/21-builtin/212-debugger/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway \u003e Kong API Gateway \u003e Use Cases \u003e OpenID Connect",
    "content": "Kong Identity Kong Identity enables you to use Konnect to generate, authenticate, and authorize API access. You can use Kong Identity to:\nCreate authorization servers per region Issue and validate access tokens Integrate secure authentication into your Kong Gateway APIs OAuth Grants The two next topics describe Authorization Code OAuth and Client Credentials grants implemented by Kong Konnect and Kong Identity as the Identity Provider. Let’s start instantiating an Authentication Service in Kong Identity.\nAuthentication Server in Kong Identity Before you can configure any authentication plugin, you must first create an auth server in Kong Identity. The auth server name is unique per each organization and each Konnect region.\nCreate an auth server using the /v1/auth-servers endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"AuthN_Server_1\", \"audience\": \"http://myhttpbin.dev\", \"description\": \"AuthN Server 1\" }' | jq You should get a response like this:\n{ \"audience\": \"http://myhttpbin.dev\", \"created_at\": \"2025-09-19T20:59:43.302149Z\", \"description\": \"AuthN Server 1\", \"id\": \"6ddf6bee-6cc0-417f-89ad-8375eead428b\", \"issuer\": \"https://dhafxmmgz56rlw6b.us.identity.konghq.com/auth\", \"labels\": {}, \"metadata_uri\": \"https://dhafxmmgz56rlw6b.us.identity.konghq.com/auth/.well-known/openid-configuration\", \"name\": \"AuthN_Server_1\", \"signing_algorithm\": \"RS256\", \"updated_at\": \"2025-09-19T20:59:43.302149Z\" } Check your AuthN Server curl -sX GET \"https://us.api.konghq.com/v1/auth-servers\" \\ -H \"Authorization: Bearer $PAT\" | jq Get the AuthN Server Id:\nexport AUTHN_SERVER_ID=$(curl -sX GET \"https://us.api.konghq.com/v1/auth-servers\" -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].id') Get the Issuer URL:\nexport ISSUER_URL=$(curl -sX GET \"https://us.api.konghq.com/v1/auth-servers\" -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].issuer') Delete the AuthN Server If you want to delete it run:\ncurl -sX DELETE \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID\" \\ -H \"Authorization: Bearer $PAT\" | jq Configure the auth server with scopes Configure a scope in your auth server using the /v1/auth-servers/$AUTHN_SERVER_ID/scopes endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/scopes\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"scope1\", \"description\": \"scope1\", \"default\": false, \"include_in_metadata\": false, \"enabled\": true }' | jq Expected response\n{ \"created_at\": \"2025-09-19T21:05:34.604098Z\", \"default\": false, \"description\": \"scope1\", \"enabled\": true, \"id\": \"897f7a59-b6e0-4dca-b0d7-b896c681d50b\", \"include_in_metadata\": false, \"name\": \"scope1\", \"updated_at\": \"2025-09-19T21:05:34.604098Z\" } Export your scope ID:\nexport SCOPE_ID=$(curl -sX GET \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/scopes\" -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].id') Configure the auth server with custom claims Configure a custom claim using the /v1/auth-servers/$AUTHN_SERVER_ID/claims endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/claims\" \\ -H \"Authorization: Bearer $PAT\" \\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"claim1\", \"value\": \"claim1\", \"include_in_token\": true, \"include_in_all_scopes\": false, \"include_in_scopes\": [ \"'$SCOPE_ID'\" ], \"enabled\": true }' | jq Expected output:\n{ \"created_at\": \"2025-09-19T21:08:28.258487Z\", \"enabled\": true, \"id\": \"9be4da8d-6a0e-404c-86bd-df11b6c74e32\", \"include_in_all_scopes\": false, \"include_in_scopes\": [ \"897f7a59-b6e0-4dca-b0d7-b896c681d50b\" ], \"include_in_token\": true, \"name\": \"claim1\", \"updated_at\": \"2025-09-19T21:08:28.258487Z\", \"value\": \"claim1\" } Create a client in the AuthN Server The client is the machine-to-machine credential. In this tutorial, Konnect will autogenerate the client ID and secret, but you can alternatively specify one yourself.\nConfigure the client using the /v1/auth-servers/$AUTHN_SERVER_ID/clients endpoint:\ncurl -sX POST \"https://us.api.konghq.com/v1/auth-servers/$AUTHN_SERVER_ID/clients\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"client1\", \"grant_types\": [ \"client_credentials\" ], \"allow_all_scopes\": false, \"allow_scopes\": [ \"'$SCOPE_ID'\" ], \"access_token_duration\": 3600, \"id_token_duration\": 3600, \"response_types\": [ \"id_token\", \"token\" ] }' | jq Expected output:\n{ \"access_token_duration\": 3600, \"allow_all_scopes\": false, \"allow_scopes\": [ \"897f7a59-b6e0-4dca-b0d7-b896c681d50b\" ], \"client_secret\": \"50e6onbo97p3v8oxq1dlz0me\", \"created_at\": \"2025-09-19T21:10:46.583789Z\", \"grant_types\": [ \"client_credentials\" ], \"id\": \"324939us3jgatrdc\", \"id_token_duration\": 3600, \"labels\": {}, \"login_uri\": null, \"name\": \"client1\", \"redirect_uris\": [], \"response_types\": [ \"id_token\", \"token\" ], \"token_endpoint_auth_method\": \"client_secret_post\", \"updated_at\": \"2025-09-19T21:10:46.583789Z\" } The Client Secret will not be shown again, so copy both ID and Secret:\nexport CLIENT_ID=\u003cYOUR_CLIENT_ID\u003e export CLIENT_SECRET=\u003cYOUR_CLIENT_SECRET\u003e Configure the OIDC plugin You can configure the OIDC plugin to use Kong Identity as the identity provider for your Gateway Services. In this example, you’ll apply the plugin to the control plane globally, but you can alternatively apply it to the Gateway Service.\nFirst, get the ID of the serverless-cp1 control plane you configured in the prerequisites:\nexport CONTROL_PLANE_ID=$(curl -sX GET \"https://us.api.konghq.com/v2/control-planes?filter%5Bname%5D%5Bcontains%5D=serverless-cp1\" \\ -H \"Authorization: Bearer $PAT\" | jq -r '.data[0].id') Enable the OIDC plugin globally:\ncurl -sX POST \"https://us.api.konghq.com/v2/control-planes/$CONTROL_PLANE_ID/core-entities/plugins/\" \\ -H \"Authorization: Bearer $PAT\"\\ -H \"Content-Type: application/json\" \\ --json '{ \"name\": \"openid-connect\", \"config\": { \"issuer\": \"'$ISSUER_URL'\", \"auth_methods\": [ \"bearer\" ], \"audience\": [ \"http://myhttpbin.dev\" ] } }' | jq In this example:\nissuer: Setting that connects the plugin to your IdP (in this case, Kong Identity). auth_methods: Specifies that the plugin should use bearer for authentication. Generate a token for the client The Gateway Service requires an access token from the client to access the Service. Generate a token for the client by making a call to the issuer URL:\ncurl -sX POST \"$ISSUER_URL/oauth/token\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=client_credentials\" \\ -d \"client_id=$CLIENT_ID\" \\ -d \"client_secret=$CLIENT_SECRET\" \\ -d \"scope=scope1\" | jq Export your access token:\nexport ACCESS_TOKEN=$(curl -sX POST \"$ISSUER_URL/oauth/token\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=client_credentials\" \\ -d \"client_id=$CLIENT_ID\" \\ -d \"client_secret=$CLIENT_SECRET\" \\ -d \"scope=scope1\" | jq -r '.access_token') Check the token\necho $ACCESS_TOKEN | jwt decode --json - Expected result\n{ \"header\": { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"b79c88f4-564d-4aea-9be7-3377e95ff758\" }, \"payload\": { \"aud\": [ \"http://myhttpbin.dev\" ], \"claim1\": \"claim1\", \"client_id\": \"324939us3jgatrdc\", \"exp\": 1758321135, \"iat\": 1758317535, \"iss\": \"https://dhafxmmgz56rlw6b.us.identity.konghq.com/auth\", \"jti\": \"a449f513-4c2d-41d6-85d7-685feb758e64\", \"nbf\": 1758317535, \"scope\": \"scope1\", \"sub\": \"324939us3jgatrdc\" } } Access the Gateway service using the token Access the httpbin Gateway Service using the short-lived token generated by the authorization server from Kong Identity:\ncurl -i -X GET \"$DATA_PLANE_URL/oidc-route/get\" \\ -H \"Authorization: Bearer $ACCESS_TOKEN\" Check the token with:\ncurl -sX GET \"$DATA_PLANE_URL/oidc-route/get\" \\ -H \"Authorization: Bearer $ACCESS_TOKEN\" | jq -r '.headers.Authorization' | cut -d ' ' -f 2 | jwt decode -j - { \"header\": { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"b79c88f4-564d-4aea-9be7-3377e95ff758\" }, \"payload\": { \"aud\": [ \"http://myhttpbin.dev\" ], \"claim1\": \"claim1\", \"client_id\": \"324939us3jgatrdc\", \"exp\": 1758321135, \"iat\": 1758317535, \"iss\": \"https://dhafxmmgz56rlw6b.us.identity.konghq.com/auth\", \"jti\": \"a449f513-4c2d-41d6-85d7-685feb758e64\", \"nbf\": 1758317535, \"scope\": \"scope1\", \"sub\": \"324939us3jgatrdc\" } }",
    "description": "Kong Identity Kong Identity enables you to use Konnect to generate, authenticate, and authorize API access. You can use Kong Identity to:\nCreate authorization servers per region Issue and validate access tokens Integrate secure authentication into your Kong Gateway APIs OAuth Grants The two next topics describe Authorization Code OAuth and Client Credentials grants implemented by Kong Konnect and Kong Identity as the Identity Provider. Let’s start instantiating an Authentication Service in Kong Identity.",
    "tags": [],
    "title": "Kong Identity",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/1573-kong-identity/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "",
    "description": "",
    "tags": [],
    "title": "14-Apiops-Decks",
    "uri": "/14-apiops-deck/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "",
    "description": "",
    "tags": [],
    "title": "15-Developer-Portals",
    "uri": "/15-developer-portal/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Introduction Kong Konnect is an API lifecycle management platform delivered as a service. The management plane is hosted in the cloud by Kong. The management plane enables customers to securely execute API management activities such as create API routes, define services etc. Runtime environments connect with the management plane using mutual transport layer authentication (mTLS), receive the updates and take customer facing API traffic.\nThe runtime environments can be:\nSelf-managed and deployed in your personal environment Managed by Kong in two options: Dedicated Cloud Gateway: it runs on isolated infrastructure within Kong-managed environments in AWS, Azure, or GCP offering the performance and security of dedicated infrastructure with the operational ease of SaaS. Serverless Gateway: it is a lightweight API gateways, ideal for developers who want to test or experiment in a pre-production environment. Learning Objectives In this workshop, you will:\nGet an architectural overview of Kong Konnect platform.\nSet up Konnect runtime as a Serverless Gateway.\nLearn what are services, routes and plugin.\nDeploy a sample microservice and access the application using the defined route.\nUse the platform to address the following API Gateway use cases\nProxy caching Authentication and Authorization Response Transformer Request Callout Rate limiting Observability And the following AI Gateway use cases\nPrompt Engineering LLM-based Request and Reponse transformation Semantic Caching Token-based Rate Limiting Semantic Routing RAG - Retrieval-Augmented Generation Expected Duration Workshop Introduction (15 minutes) Architectural Walkthrough (10 minutes) Sample Application and initial use case (15 minutes) Addressing API and AI Gateway use cases (90 minutes) Observability (30 minutes) Next Steps and Cleanup (5 min)",
    "description": "Introduction Kong Konnect is an API lifecycle management platform delivered as a service. The management plane is hosted in the cloud by Kong. The management plane enables customers to securely execute API management activities such as create API routes, define services etc. Runtime environments connect with the management plane using mutual transport layer authentication (mTLS), receive the updates and take customer facing API traffic.\nThe runtime environments can be:\nSelf-managed and deployed in your personal environment Managed by Kong in two options: Dedicated Cloud Gateway: it runs on isolated infrastructure within Kong-managed environments in AWS, Azure, or GCP offering the performance and security of dedicated infrastructure with the operational ease of SaaS. Serverless Gateway: it is a lightweight API gateways, ideal for developers who want to test or experiment in a pre-production environment. Learning Objectives In this workshop, you will:",
    "tags": [],
    "title": "API Management with Kong Konnect and Serverless API Gateway",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect and Serverless API Gateway",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
