var relearn_searchindex = [
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Decorator plugin adds an array of llm/v1/chat messages to either the start or end of an LLM consumer’s chat history. This allows you to pre-engineer complex prompts, or steer (and guard) prompts in such a way that the modification to the consumer’s LLM message is completely transparent.\nYou can use this plugin to pre-set a system prompt, set up specific prompt history, add words and phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway.\ncat \u003e ai-prompt-decorator.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai options: temperature: 1.0 - name: ai-prompt-decorator instance_name: ai-prompt-decorator-openai config: prompts: prepend: - role: system content: \"You will always respond in the Portuguese (Brazil) language.\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-decorator.yaml Send a request now:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-5\" }' | jq You can now click Next to proceed further.",
    "description": "The AI Prompt Decorator plugin adds an array of llm/v1/chat messages to either the start or end of an LLM consumer’s chat history. This allows you to pre-engineer complex prompts, or steer (and guard) prompts in such a way that the modification to the consumer’s LLM message is completely transparent.\nYou can use this plugin to pre-set a system prompt, set up specific prompt history, add words and phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway.",
    "tags": [],
    "title": "AI Prompt Decorator",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/1-prompt-decorator/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Template plugin lets you provide tuned AI prompts to users. Users only need to fill in the blanks with variable placeholders in the following format: {{variable}}. This lets admins set up templates, which can be then be used by anyone in the organization. It also allows admins to present an LLM as an API in its own right - for example, a bot that can provide software class examples and/or suggestions.\nThis plugin also sanitizes string inputs to ensure that JSON control characters are escaped, preventing arbitrary prompt injection.\nWhen calling a template, simply replace the messages (llm/v1/chat) or prompt (llm/v1/completions) with a template reference, in the following format: {template://TEMPLATE_NAME}\nHere’s an example of template definition:\ncat \u003e ai-prompt-template.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-prompt-template instance_name: ai-prompt-template-openai enabled: true config: allow_untemplated_requests: true templates: - name: template1 template: |- { \"messages\": [ { \"role\": \"user\", \"content\": \"Explain to me what {{thing}} is.\" } ] } EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-template.yaml Now, send a request referring the template:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": \"{template://template1}\", \"properties\": { \"thing\": \"niilism\" } }' | jq Kong-gratulations! have now reached the end of this module by authenticating your API requests with AWS Cognito. You can now click Next to proceed with the next module.",
    "description": "The AI Prompt Template plugin lets you provide tuned AI prompts to users. Users only need to fill in the blanks with variable placeholders in the following format: {{variable}}. This lets admins set up templates, which can be then be used by anyone in the organization. It also allows admins to present an LLM as an API in its own right - for example, a bot that can provide software class examples and/or suggestions.",
    "tags": [],
    "title": "AI Prompt Template",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/2-prompt-template/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Let’s get started with a simple round-robin policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: round-robin targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Send the request few times. Note we are going to receive responses from both LLMs, in a round-robin way.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq Here’s the OpenAI’s gpt-4.1 response:\n{ \"id\": \"chatcmpl-C3lbQFtYTrEWwq7N6zKWh4CHl3idX\", \"object\": \"chat.completion\", \"created\": 1755014004, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"The title of \\\"greatest Polish writer\\\" is somewhat subjective and can depend on whom you ask and which literary genre or era is in focus. However, several names consistently rise to the top in critical consensus and cultural significance:\\n\\n1. **Adam Mickiewicz (1798–1855):** Often considered the national poet of Poland and a key figure in Romanticism, Mickiewicz is renowned for works such as *Pan Tadeusz* and *Dziady* (Forefathers' Eve). His impact on Polish identity and literature is unparalleled.\\n\\n2. **Henryk Sienkiewicz (1846–1916):** Winner of the Nobel Prize for Literature in 1905, Sienkiewicz is famous for historical novels such as *Quo Vadis* and his \\\"Trilogy\\\" (*With Fire and Sword*, *The Deluge*, *Fire in the Steppe*).\\n\\n3. **Wisława Szymborska (1923–2012):** One of the most celebrated modern poets, Szymborska received the Nobel Prize in 1996. Her poetry is noted for its wit, irony, and philosophical depth.\\n\\n4. **Czesław Miłosz (1911–2004):** A Nobel Prize-winning poet (1980), essayist, and thinker, Miłosz's impact on world literature and Polish thought is immense.\\n\\nGiven the foundational role Adam Mickiewicz played in defining Polish literature and his enduring influence, **Mickiewicz is most frequently regarded as the greatest Polish writer, particularly in terms of cultural and historical significance**.\\n\\nHowever, for different periods or genres, Sienkiewicz, Szymborska, or Miłosz may be regarded as the greatest by some readers or scholars.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 15, \"completion_tokens\": 364, \"total_tokens\": 379, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_799e4ca3f1\" } And here is llama3.2:1b’s:\n{ \"object\": \"chat.completion\", \"model\": \"llama3.2:1b\", \"choices\": [ { \"index\": 0, \"finish_reason\": \"stop\", \"message\": { \"content\": \"The greatest Polish writer is often debated among scholars and literary enthusiasts. However, some of the most highly regarded Polish writers include:\\n\\n1. Juliusz Słowacki (1795-1861): Considered one of the greatest Polish writers, Słowacki was a poet, playwright, and translator. He is known for his lyric poetry and plays that often explored themes of love, nature, and social issues.\\n2. Adam Mickiewicz (1809-1849): A leading figure in Poland's national revival movement, Mickiewicz was a poet, composer, and politician. He wrote the epic poem \\\"Pan Tadeusz,\\\" which is considered one of the greatest Polish works of literature.\\n3. Józef Piłsudski (1867-1935): While not traditionally considered a writer, Piłsudski was a prominent statesman and military leader who played a key role in Poland's struggle for independence. He also wrote two literary novels, \\\"The Tragedy of Maksimilian\\\" and \\\"The Trial of Maximilian.\\\"\\n4. Cyprian Kamil Norwid (1821-1883): A poet, playwright, and critic, Norwid was one of the most influential Polish writers of his time. He is known for his innovative style and exploration of themes such as love, nature, and social justice.\\n5. Henryk Sienkiewicz (1846-1916): Although primarily a novelist and playwright, Sienkiewicz's works often explored themes of politics, history, and morality. His novel \\\"Quo Vadis\\\" is considered one of the greatest Polish novels of all time.\\n\\nHowever, if I had to choose one writer who is widely regarded as the greatest Polish writer, it would be Juliusz Słowacki.\", \"role\": \"assistant\" } } ], \"usage\": { \"completion_tokens\": 370, \"total_tokens\": 403, \"prompt_tokens\": 33 }, \"created\": 1755014233 }",
    "description": "Let’s get started with a simple round-robin policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: round-robin targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Round Robin",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/2-round-robin/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Guard plugin lets you to configure a series of PCRE-compatible regular expressions as allow or deny lists, to guard against misuse of llm/v1/chat or llm/v1/completions requests.\nYou can use this plugin to allow or block specific prompts, words, phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway. It does this by scanning all chat messages (where the role is user) for the specific expressions set. You can use a combination of allow and deny rules to preserve integrity and compliance when serving an LLM service using Kong Gateway.\nFor llm/v1/chat type models: You can optionally configure the plugin to ignore existing chat history, wherein it will only scan the trailing user message. For llm/v1/completions type models: There is only one prompt field, thus the whole prompt is scanned on every request. The plugin matches lists of regular expressions to requests through AI Proxy. The matching behavior is as follows:\nIf any deny expressions are set, and the request matches any regex pattern in the deny list, the caller receives a 400 response. If any allow expressions are set, but the request matches none of the allowed expressions, the caller also receives a 400 response. If any allow expressions are set, and the request matches one of the allow expressions, the request passes through to the LLM. If there are both deny and allow expressions set, the deny condition takes precedence over allow. Any request that matches an entry in the deny list will return a 400 response, even if it also matches an expression in the allow list. If the request does not match an expression in the deny list, then it must match an expression in the allow list to be passed through to the LLM Here’s an example to allow only valid credit cards numbers:\ncat \u003e ai-prompt-guard.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-prompt-guard instance_name: ai-prompt-guard-openai enabled: true config: allow_all_conversation_history: true allow_patterns: - \".*\\\\\\\"card\\\\\\\".*\\\\\\\"4[0-9]{3}\\\\*{12}\\\\\\\"\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-prompt-guard.yaml Send a request with a valid credit card pattern:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Validate this card: {\\\"card\\\": \\\"4111************\\\", \\\"cvv\\\": \\\"000\\\"}\" } ] }' | jq '.' Now, send a non-valid number:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Validate this card: {\\\"card\\\": \\\"4111xyz************\\\", \\\"cvv\\\": \\\"000\\\"}\" } ] }' | jq '.' The expect result is:\n{ \"error\": { \"message\": \"bad request\" } }",
    "description": "The AI Prompt Guard plugin lets you to configure a series of PCRE-compatible regular expressions as allow or deny lists, to guard against misuse of llm/v1/chat or llm/v1/completions requests.\nYou can use this plugin to allow or block specific prompts, words, phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway. It does this by scanning all chat messages (where the role is user) for the specific expressions set. You can use a combination of allow and deny rules to preserve integrity and compliance when serving an LLM service using Kong Gateway.",
    "tags": [],
    "title": "AI Prompt Guard",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/3-prompt-guard/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Now, let’s redirect 80% of the request to OpenAI’s gpt-4.1 with a weight based policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} weight: 80 - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" weight: 20 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml",
    "description": "Now, let’s redirect 80% of the request to OpenAI’s gpt-4.1 with a weight based policy:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} weight: 80 - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" weight: 20 EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Weight",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/3-weight/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.\nCreate a file with the following declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Test the Route again.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq Lowest Usage policy The lowest-usage algorithm in AI Proxy Advanced is based on the volume of usage for each model. It balances the load by distributing requests to models with the lowest usage, measured by factors such as prompt token counts, response token counts, or other resource metrics.\nReplace the declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-usage targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration:\ndeck gateway reset --konnect-control-plane-name kong-aws --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml And test the Route again.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is considered the greatest Polish writer?\" } ] }' | jq",
    "description": "Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.\nCreate a file with the following declaration:\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Lowest-Latency and Lowest-Usage",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/4-lowest-latency-usage/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced",
    "content": "Semantic The semantic algorithm distributes requests to different models based on the similarity between the prompt in the request and the description provided in the model configuration. This allows Kong to automatically select the model that is best suited for the given domain or use case. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.\ncat \u003e ai-proxy-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: semantic embeddings: auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: \"text-embedding-3-small\" vectordb: dimensions: 1024 distance_metric: cosine strategy: redis threshold: 1.0 redis: host: \"redis-stack.redis.svc.cluster.local\" port: 6379 database: 0 targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: \"llm/v1/chat\" auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} description: \"mathematics, algebra, calculus, trigonometry\" - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: \"llm/v1/chat\" description: \"piano, orchestra, liszt, classical music\" EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy-advanced.yaml Send a request related to Mathematics. The response should come from OpenAI’s gpt-4.1\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Tell me about the last theorem of Fermat\" } ] }' | jq On the other hand, Llama3.1 should be responsible for requests related to Classical Music.\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who wrote the Hungarian Rhapsodies piano pieces?\" } ] }' | jq curl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Tell me a contemporaty pianist of Chopin\" } ] }' | jq If you check Redis, you’ll se there are two entries, related to the models\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan Expected output \"ai_proxy_advanced_semantic:01c84f59-b7c3-418b-818d-4369ef3e55ef:8f74aeaab95482bb37fbd69cd42154dcd6d321e1631ffdfd1802e1609d4c2481\" \"ai_proxy_advanced_semantic:01c84f59-b7c3-418b-818d-4369ef3e55ef:72a33ce9079fd34f6fb3624c3a4ba1a0df0c1aad267986db2249dc26a8808a41\"",
    "description": "Semantic The semantic algorithm distributes requests to different models based on the similarity between the prompt in the request and the description provided in the model configuration. This allows Kong to automatically select the model that is best suited for the given domain or use case. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.",
    "tags": [],
    "title": "Semantic Routing",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/5-semantic-routing/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "Knowledge Attendees should have intermediate knowledge of Kubernetes and Docker for installations of Konnect Data Plane and other products like Keycloak, Redis, Prometheus etc.\nAttendees should have basic knowledge of GenAI models, specially LLMs and Embedding models as well as providers like OpenAI, Mistral, Anthropic, AWS, GCP, Azure.\nKong Academy Complete two badges via Kong Academy: Kong Gateway Foundations and Kong Gateway Operations\nKong Konnect Subscription You will need a Kong Konnect Subscription to execute this workshop.\nEnvironment Please, make sure you have Minikube 1.36.0 Kubernetes Cluster over Podman 5.6.0 installed.\nPodman installation Minikube installation Command Line Utilities In this workshop, we will use the following command line utilities\nkubectl helm k9s curl jq yq jwt-cli wget Redis Redis is used in some use cases, including Rate Limiting, Caching, Semantic Routing and RAG.\nLLM Ollama The Kong AI Gateway use cases consume and protect LLMs running on Ollama\nOpenAI Some AI use cases also use OpenAI’s Embeddings and LLMs. Please make sure you have an OpenAI API key.\nRecommended hardware not including Ollama CPU: 4-6 vCPUs Memory: 8-16GB Disk: 30–50GB Recommended hardware including Ollama CPU: 6-8 vCPUs Memory: 12-24GB Disk:\t50-100GB",
    "description": "Knowledge Attendees should have intermediate knowledge of Kubernetes and Docker for installations of Konnect Data Plane and other products like Keycloak, Redis, Prometheus etc.\nAttendees should have basic knowledge of GenAI models, specially LLMs and Embedding models as well as providers like OpenAI, Mistral, Anthropic, AWS, GCP, Azure.\nKong Academy Complete two badges via Kong Academy: Kong Gateway Foundations and Kong Gateway Operations\nKong Konnect Subscription You will need a Kong Konnect Subscription to execute this workshop.",
    "tags": [],
    "title": "Prerequisites",
    "uri": "/10-prerequisites/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Prerequisites",
    "content": "Konnect Plus Subscription You will need a Konnect subscription. Follow the steps below to obtain a Konnect Plus subscription. Initially $500 of credits are provided for up to 30 days, after the credits have expired or the time has finished Konnect Plus will charge based on Konnect Plus pricing. Following the workshop instructions will use less than $500 credits. After the workshop has finished the Konnect Plus environment can be deleted.\nClick on the Registration link and present your credentials.\nKonnect will send you an email to confirm the subscription. Click on the link in email to confirm your subscription.\nThe Konnect environment can be accessed via the Konnect log in page.\nAfter logging in create an organisation name, select a region, then answer a few questions.\nCredit available can be monitored though Plan and Usage page.",
    "description": "Konnect Plus Subscription You will need a Konnect subscription. Follow the steps below to obtain a Konnect Plus subscription. Initially $500 of credits are provided for up to 30 days, after the credits have expired or the time has finished Konnect Plus will charge based on Konnect Plus pricing. Following the workshop instructions will use less than $500 credits. After the workshop has finished the Konnect Plus environment can be deleted.",
    "tags": [],
    "title": "Konnect Subscription",
    "uri": "/10-prerequisites/konnect-subscription/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Prerequisites",
    "content": "Podman We are going to deploy our Data Plane in a Minikube Cluster over Podman. You can start Podman with:\npodman machine set --memory 8196 podman machine start If you want to stop it run:\npodman machine stop $ podman --version podman version 5.5.2 Minikube Then you can install Minikube with:\nminikube start --driver=podman --memory='no-limit' $ minikube version minikube version: v1.36.0 commit: f8f52f5de11fc6ad8244afac475e1d0f96841df1 Use should see your cluster running with:\nkubectl get all --all-namespaces Typical output is:\nNAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-674b8bbfcf-xrllp 0/1 Running 0 12s kube-system pod/etcd-minikube 1/1 Running 0 18s kube-system pod/kube-apiserver-minikube 1/1 Running 0 18s kube-system pod/kube-controller-manager-minikube 1/1 Running 0 18s kube-system pod/kube-proxy-xkfn9 1/1 Running 0 13s kube-system pod/kube-scheduler-minikube 1/1 Running 0 18s kube-system pod/storage-provisioner 1/1 Running 0 17s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 19s kube-system service/kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 18s NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-proxy 1 1 1 1 1 kubernetes.io/os=linux 18s NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 0/1 1 0 18s NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-674b8bbfcf 1 1 0 12s To be able to consume the Kubernetes Load Balancer Services, in another terminal run:\nminikube tunnel You can now click Next to install the operator.",
    "description": "Podman We are going to deploy our Data Plane in a Minikube Cluster over Podman. You can start Podman with:\npodman machine set --memory 8196 podman machine start If you want to stop it run:\npodman machine stop $ podman --version podman version 5.5.2 Minikube Then you can install Minikube with:\nminikube start --driver=podman --memory='no-limit' $ minikube version minikube version: v1.36.0 commit: f8f52f5de11fc6ad8244afac475e1d0f96841df1 Use should see your cluster running with:",
    "tags": [],
    "title": "Minikube",
    "uri": "/10-prerequisites/minikube/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "The Kong Konnect platform provides a cloud control plane (CP), which manages all service configurations. It propagates those configurations to all Runtime control planes, which use in-memory storage. These nodes can be installed anywhere, on-premise or in AWS.\nFor today workshop, we will be focusing on Kong Gateway. Kong Gateway data plane listen for traffic on the proxy port 443 by default. The data plane evaluates incoming client API requests and routes them to the appropriate backend APIs. While routing requests and providing responses, policies can be applied with plugins as necessary.\nKonnect modules Kong Konnect Enterprise features are described in this section, including modules and plugins that extend and enhance the functionality of the Kong Konnect platform.\nControl Plane (Gateway Manager) Control Plane empowers your teams to securely collaborate and manage their own set of runtimes and services without the risk of impacting other teams and projects. Control Plane instantly provisions hosted Kong Gateway control planes and supports securely attaching Kong Gateway data planes from your cloud or hybrid environments.\nThrough the Control Plane, increase the security of your APIs with out-of-the-box enterprise and community plugins, including OpenID Connect, Open Policy Agent, Mutual TLS, and more.\nAI Manager Manage all of your LLMs in a single dashboard providing a unified control plane to create, manage, and monitor LLMs using the Konnect platform. With AI Manager you can assign Gateway Services and define how traffic is distributed across models, enable streaming responses and manage authentication through the AI Gateway, monitor request and token volumes, track error rates, and measure average latency with historical comparisons, etc.\nDev Portal Streamline developer onboarding with the Dev Portal, which offers a self-service developer experience to discover, register, and consume published services from your Service Hub catalog. This customizable experience can be used to match your own unique branding and highlights the documentation and interactive API specifications of your services. Enable application registration to automatically secure your APIs with a variety of authorization providers.\nAnalytics Use Analytics to gain deep insights into service, route, and application usage and health monitoring data. Keep your finger on the pulse of the health of your API products with custom reports and contextual dashboards. In addition, you can enhance the native monitoring and analytics capabilities with Kong Gateway plugins that enable streaming monitoring metrics to third-party analytics providers.\nTeams To help secure and govern your environment, Konnect provides the ability to manage authorization with teams. You can use Konnect’s predefined teams for a standard set of roles, or create custom teams with any roles you choose. Invite users and add them to these teams to manage user access. You can also map groups from your existing identity provider into Konnect teams.\nFurther Reading Gateway Manager AI Manager Dev Portal Analytics",
    "description": "The Kong Konnect platform provides a cloud control plane (CP), which manages all service configurations. It propagates those configurations to all Runtime control planes, which use in-memory storage. These nodes can be installed anywhere, on-premise or in AWS.\nFor today workshop, we will be focusing on Kong Gateway. Kong Gateway data plane listen for traffic on the proxy port 443 by default. The data plane evaluates incoming client API requests and routes them to the appropriate backend APIs. While routing requests and providing responses, policies can be applied with plugins as necessary.",
    "tags": [],
    "title": "Kong Konnect Architectural Overview",
    "uri": "/architecture/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Prerequisites",
    "content": "Install Redis Use the redis-stack Helm Charts to install Redis as our vector database.\nhelm repo add redis-stack https://redis-stack.github.io/helm-redis-stack helm repo update helm install redis-stack redis-stack/redis-stack -n redis --create-namespace Check the installation:\n$ kubectl exec $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-server --version Redis server v=7.4.5 sha=00000000:0 malloc=libc bits=64 build=d2e5921793838dd If you want to uninstall it:\nhelm uninstall redis-stack -n redis kubectl delete namespace redis Install Ollama As our Embedding model, we’re going to consume the “mxbai-embed-large:latest” model handled locally by Ollama. Use the Ollama Helm Charts to install it.\nhelm repo add ollama-helm https://otwld.github.io/ollama-helm/ helm repo update helm install ollama ollama-helm/ollama \\ -n ollama \\ --create-namespace \\ --set ollama.models.pull[0]=\"mxbai-embed-large:latest\" \\ --set ollama.models.pull[1]=\"llama3.2:1b\" \\ --set service.type=LoadBalancer Check the version and models\n$ kubectl exec -it $(kubectl get pod -n ollama -o json | jq -r '.items[].metadata.name') -n ollama -- ollama --version ollama version is 0.11.2 $ kubectl exec -it $(kubectl get pod -n ollama -o json | jq -r '.items[].metadata.name') -n ollama -- ollama list NAME ID SIZE MODIFIED llama3.2:1b baf6a787fdff 1.3 GB 31 minutes ago mxbai-embed-large:latest 468836162de7 669 MB 32 minutes ago Send request to test it:\ncurl -sX POST http://localhost:11434/api/generate -d '{ \"model\": \"llama3.2:1b\", \"prompt\": \"Tell me about Miles Davis\", \"stream\": false }' | jq '.response' Expected response:\n\"Miles Davis (1926-1991) was an American jazz trumpeter, bandleader, and composer. He is widely regarded as one of the most influential musicians in the history of jazz.\\n\\nEarly Life and Career:\\n\\nBorn in Alton, Illinois, Davis grew up in a musical family and began playing trumpet at age five. He studied music theory and composition at the Juilliard School in New York City before serving in the United States Army during World War II.\\n\\nAfter the war, Davis moved to New York City's lower East Side, where he formed his first jazz group with guitarist Red Garland and pianist Cannonball Adderley. In 1954, he joined the cool jazz group Chirps, later renamed Art Bayou's Jazz Experience.\\n\\nThe \\\"Cool\\\" Period:\\n\\nIn 1956, Davis moved to Los Angeles to form a new band with pianist John Coltrane, bassist Charles Mingus, and drummer Bill Evans. This group was known as Miles Davis Quintet and released several critically acclaimed albums, including \\\"Birth of the Cool\\\" (1957) and \\\"Kind of Blue\\\" (1959). The quintet's music marked a turning point in jazz history, with its emphasis on cool, introspective, and expressive playing.\\n\\nIn the early 1960s, Davis began to experiment with more avant-garde and experimental approaches to jazz. He collaborated with pianist Herbie Hancock on the album \\\"Milestones\\\" (1960), which featured a more complex and electronic approach to jazz.\\n\\nLater Years:\\n\\nIn the late 1960s, Davis's playing became increasingly introspective and personal. He released several critically acclaimed albums, including \\\"Bitches Brew\\\" (1970) and \\\"A Tribute to Jack Johnson\\\" (1971). His later work was marked by a more relaxed and improvisational approach, as he explored new musical territories and collaborated with artists from other genres.\\n\\nPersonal Life:\\n\\nDavis's personal life was marked by periods of great creativity and introspection. He had several high-profile relationships, including with actress Joanna Glenn and fashion designer Carole King. In the 1970s, Davis became increasingly interested in Eastern spirituality and meditation, which influenced his later music.\\n\\nDeath:\\n\\nMiles Davis died on September 28, 1991, at the age of 65, due to complications from heart failure. He was buried in the Forest Lawn Memorial Park Cemetery in Glendale, California.\\n\\nLegacy:\\n\\nMiles Davis's legacy is profound and far-reaching. He helped shape the development of cool jazz and improvisational music, and his influence can be heard in countless artists across multiple genres. His innovative approach to jazz has inspired generations of musicians, from John Coltrane to Wayne Shorter and beyond.\\n\\nDavis's music continues to be celebrated for its complexity, depth, and emotional resonance. He remains one of the most beloved and respected figures in jazz history, and his impact on modern music is immeasurable.\" If you want to uninstall it:\nhelm uninstall ollama -n ollama kubectl delete namespace ollama Enable Metrics Server minikube addons enable metrics-server minikube addons list Keycloak and OPA The installation procedures for both servers are available in the OpenID Connect and OPA (Open Policy Agent) sections",
    "description": "Install Redis Use the redis-stack Helm Charts to install Redis as our vector database.\nhelm repo add redis-stack https://redis-stack.github.io/helm-redis-stack helm repo update helm install redis-stack redis-stack/redis-stack -n redis --create-namespace Check the installation:\n$ kubectl exec $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-server --version Redis server v=7.4.5 sha=00000000:0 malloc=libc bits=64 build=d2e5921793838dd If you want to uninstall it:\nhelm uninstall redis-stack -n redis kubectl delete namespace redis Install Ollama As our Embedding model, we’re going to consume the “mxbai-embed-large:latest” model handled locally by Ollama. Use the Ollama Helm Charts to install it.",
    "tags": [],
    "title": "Redis and Ollama",
    "uri": "/10-prerequisites/redis-ollama-keycloak-opa/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "This chapter will walk you through\nKonnect Control Plane and Data Plane creation using Kong Gateway Operator (KGO). Scale Kong data plane nodes on Kubernetes using HPA - Horizontal Pod Autoscaler. Access Kong data plane through Minikube tunnel Here’s a Reference Architecture that will be implemented in this workshop:\nKong Konnect Control Plane: responsible for managing your APIs Kong Konnect Data Plane: connected to the Control Plane, it is responsible for processing all the incoming requests sent by the consumers. Kong provides a plugin framework, where each one of them is responsible for a specific functionality. As a can see, there are two main collections of plugins: On the left, the historic and regular API Gateway plugins, implementing all sort of policies including, for example, OIDC based Authentication processes with Keycloak, Amazon Cognito and Okta or Observability with Prometheus/Grafana and Dynatrace. On the right, another plugin collection for AI-based use cases. For example, the AI Rate Limiting plugin implements policies like this based on the number of tokens consumed be the requests. Or, as another example is the AI Semantic Cache plugin, which caches data based on the semantics related to the responses coming from the LLM models. Kong AI Gateway supports, out of the box, a variety of infrastructures, including not just OpenAI, but also Amazon Bedrock, Google Gemini, Mistral, Anthropic, etc. In order to deal with embeddings, the Gateway also supports also vector databases. Kong Gateway protects not just the LLM Models but also the upstream services, including your application micros surfaces or services. Konnect Control Plane After Konnect registration, you need to create your first Control Plane. There are multiple ways to do it:\nKonnect User Interface. RESTful Admin API, a fundamental mechanism for administration purposes. Kong Gateway Operator (KGO) and Kubernetes CRDs To get an easier and faster deployment, this workshop uses KGO. You may observe the output in Konnect UI.\nThis tutorial is intended to be used for labs and PoC only. There are many aspects and processes, typically implemented in production sites, not described here. For example: Digital Certificate issuing, Cluster monitoring, etc. For a production ready deployment, refer Kong on Terraform Constructs, available here\nYou can now click Next to begin the module.",
    "description": "This chapter will walk you through\nKonnect Control Plane and Data Plane creation using Kong Gateway Operator (KGO). Scale Kong data plane nodes on Kubernetes using HPA - Horizontal Pod Autoscaler. Access Kong data plane through Minikube tunnel Here’s a Reference Architecture that will be implemented in this workshop:\nKong Konnect Control Plane: responsible for managing your APIs Kong Konnect Data Plane: connected to the Control Plane, it is responsible for processing all the incoming requests sent by the consumers. Kong provides a plugin framework, where each one of them is responsible for a specific functionality. As a can see, there are two main collections of plugins: On the left, the historic and regular API Gateway plugins, implementing all sort of policies including, for example, OIDC based Authentication processes with Keycloak, Amazon Cognito and Okta or Observability with Prometheus/Grafana and Dynatrace. On the right, another plugin collection for AI-based use cases. For example, the AI Rate Limiting plugin implements policies like this based on the number of tokens consumed be the requests. Or, as another example is the AI Semantic Cache plugin, which caches data based on the semantics related to the responses coming from the LLM models. Kong AI Gateway supports, out of the box, a variety of infrastructures, including not just OpenAI, but also Amazon Bedrock, Google Gemini, Mistral, Anthropic, etc. In order to deal with embeddings, the Gateway also supports also vector databases. Kong Gateway protects not just the LLM Models but also the upstream services, including your application micros surfaces or services. Konnect Control Plane After Konnect registration, you need to create your first Control Plane. There are multiple ways to do it:",
    "tags": [],
    "title": "Konnect Setup",
    "uri": "/11-konnect-setup/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Konnect Setup",
    "content": "Install the Operator To get started let’s install the Operator:\nhelm repo add kong https://charts.konghq.com helm repo update kong helm upgrade --install kgo kong/gateway-operator \\ -n kong-system \\ --create-namespace \\ --set image.tag=1.6.2 \\ --set kubernetes-configuration-crds.enabled=true \\ --set env.ENABLE_CONTROLLER_KONNECT=true You can check the Operator’s log with: kubectl logs -f $(kubectl get pod -n kong-system -o json | jq -r '.items[].metadata | select(.name | startswith(\"kgo-gateway-operator\"))' | jq -r '.name') -n kong-system Delete KGO If you want to delete KGO run:\nhelm uninstall kgo -n kong-system kubectl delete namespace kong-system Kong-gratulations! have now reached the end of this module by creating a Kong Operator. You can now click Next to proceed with the next module.",
    "description": "Install the Operator To get started let’s install the Operator:\nhelm repo add kong https://charts.konghq.com helm repo update kong helm upgrade --install kgo kong/gateway-operator \\ -n kong-system \\ --create-namespace \\ --set image.tag=1.6.2 \\ --set kubernetes-configuration-crds.enabled=true \\ --set env.ENABLE_CONTROLLER_KONNECT=true You can check the Operator’s log with: kubectl logs -f $(kubectl get pod -n kong-system -o json | jq -r '.items[].metadata | select(.name | startswith(\"kgo-gateway-operator\"))' | jq -r '.name') -n kong-system Delete KGO If you want to delete KGO run:",
    "tags": [],
    "title": "Kong Gateway Operator",
    "uri": "/11-konnect-setup/111-kgo-installation/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Konnect Setup",
    "content": "KGO requires a Konnect Personal Access Token (PAT) for creating the Control Plane. To generate your PAT, click on your initials in the upper right corner of the Konnect home page, then select Personal Access Tokens. Click on + Generate Token, name your PAT, set its expiration time, and be sure to copy and save it, as Konnect won’t display it again.\nNote Be sure to copy and save your PAT, as Konnect won’t display it again.\nKonnect PAT secret Create a Kubernetes (K8) Secret with your PAT in the kong namespace. KGO requires the secret to be labeled.\nSave PAT in an environment variables export PAT=PASTE_THE_CONTENTS_OF_COPIED_PAT Create the namespace kubectl create namespace kong Create K8s Secret with PAT Note Don’t forget to replace PASTE_THE_CONTENTS_OF_COPIED_PAT in the command above with the copied PAT from Kong UI.\nkubectl create secret generic konnect-pat -n kong --from-literal=token=$(echo $PAT) kubectl label secret konnect-pat -n kong \"konghq.com/credential=konnect\" Check your Secret. You should your PAT. kubectl get secret konnect-pat -n kong -o jsonpath='{.data.*}' | base64 -d You can now click Next to install the operator.",
    "description": "KGO requires a Konnect Personal Access Token (PAT) for creating the Control Plane. To generate your PAT, click on your initials in the upper right corner of the Konnect home page, then select Personal Access Tokens. Click on + Generate Token, name your PAT, set its expiration time, and be sure to copy and save it, as Konnect won’t display it again.",
    "tags": [],
    "title": "PAT - Personal Access Token",
    "uri": "/11-konnect-setup/112-personal-access-token/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Konnect Setup",
    "content": "Control Plane Deployment The following declaration defines an Authentication Configuration, based on the Kubernetes Secret and referring to a Konnect API URL, and the actual Konnect Control Plane.\ncat \u003c\u003cEOF | kubectl apply -f - kind: KonnectAPIAuthConfiguration apiVersion: konnect.konghq.com/v1alpha1 metadata: name: konnect-api-auth-conf namespace: kong spec: type: secretRef secretRef: name: konnect-pat namespace: kong serverURL: us.api.konghq.com --- kind: KonnectGatewayControlPlane apiVersion: konnect.konghq.com/v1alpha1 metadata: name: kong-workshop namespace: kong spec: name: kong-workshop konnect: authRef: name: konnect-api-auth-conf EOF If you go to Konnect UI \u003e Gateway manager, you should see a new control plane named kong-workshop getting created.\nData Plane deployment The next declaration instantiates a Data Plane connected to your Control Plane. It creates a KonnectExtension, asking KGO to manage the certificate and private key provisioning automatically, and the actual Data Plane. The Data Plane declaration specifies the Docker image, in our case 3.11, as well as how the Kubernetes Service, related to the Data Plane, should be created. Also, we use the the Data Plane deployment refers to the Kubernetes Service Account we created before.\ncat \u003c\u003cEOF | kubectl apply -f - kind: KonnectExtension apiVersion: konnect.konghq.com/v1alpha1 metadata: name: konnect-config1 namespace: kong spec: clientAuth: certificateSecret: provisioning: Automatic konnect: controlPlane: ref: type: konnectNamespacedRef konnectNamespacedRef: name: kong-workshop --- apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 network: services: ingress: name: proxy1 type: LoadBalancer EOF It takes some minutes to get the Load Balancer provisioned and avaiable. Get its domain name with:\nexport DATA_PLANE_LB=$(kubectl get svc -n kong proxy1 --output=jsonpath='{.status.loadBalancer.ingress[].ip}') View the load balancer DNS as\necho $DATA_PLANE_LB Try calling it as\ncurl -w '\\n' $DATA_PLANE_LB Expected Output\n{ \"message\":\"no Route matched with those values\", \"request_id\":\"d364362a60b32142fed73712a9ea1948\" } You can check the Data Plane logs with kubectl logs -f $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name') -n kong Control Plane and Data Plane deletion If you want to delete the DP run run:\nkubectl delete dataplane dataplane1 -n kong kubectl delete konnectextensions.konnect.konghq.com konnect-config1 -n kong If you want to delete the CP run:\nkubectl delete konnectgatewaycontrolplane kong-workshop -n kong kubectl delete konnectapiauthconfiguration konnect-api-auth-conf -n kong If you want to delete the PAT and namespace run:\nkubectl delete secret konnect-pat -n kong kubectl delete namespace kong Further Reading Kong Konnect API auth configuration Kong-gratulations! have now reached the end of this module by creating control plane and data plane. You can now click Next to proceed with the next module.",
    "description": "Control Plane Deployment The following declaration defines an Authentication Configuration, based on the Kubernetes Secret and referring to a Konnect API URL, and the actual Konnect Control Plane.\ncat \u003c\u003cEOF | kubectl apply -f - kind: KonnectAPIAuthConfiguration apiVersion: konnect.konghq.com/v1alpha1 metadata: name: konnect-api-auth-conf namespace: kong spec: type: secretRef secretRef: name: konnect-pat namespace: kong serverURL: us.api.konghq.com --- kind: KonnectGatewayControlPlane apiVersion: konnect.konghq.com/v1alpha1 metadata: name: kong-workshop namespace: kong spec: name: kong-workshop konnect: authRef: name: konnect-api-auth-conf EOF If you go to Konnect UI \u003e Gateway manager, you should see a new control plane named kong-workshop getting created.",
    "tags": [],
    "title": "Control Plane and Data Plane",
    "uri": "/11-konnect-setup/113-cp-dp/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Konnect Setup",
    "content": "One of the most important capabilities provided by Kubernetes is to easily scale out a Deployment. With a single command we can create or terminate pod replicas in order to optimally support a given throughput.\nThis capability is especially interesting for Kubernetes applications like Kong for Kubernetes Ingress Controller.\nHere’s our deployment before scaling it out:\nkubectl get service -n kong Sample Output\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dataplane-admin-dataplane1-wjk92 ClusterIP None \u003cnone\u003e 8444/TCP 4d21h proxy1 LoadBalancer 10.110.77.48 127.0.0.1 80:32462/TCP,443:31666/TCP 4d21h Notice, at this point in the workshop, there is only one pod taking data plane traffic.\nkubectl get pod -n kong -o wide Sample Output\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dataplane-dataplane1-2pvw2-67fbd76d98-jxhfd 1/1 Running 1 (40h ago) 4d21h 10.244.0.82 minikube \u003cnone\u003e \u003cnone\u003e Manual Scaling Out Now, let’s scale the deployment out creating 3 replicas of the pod\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 replicas: 3 network: services: ingress: name: proxy1 type: LoadBalancer EOF Check the Deployment again and now you should see 3 replicas of the pod.\nkubectl get pod -n kong -o wide Sample Output\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dataplane-dataplane1-ch6g9-6889fdf76b-5gjh9 1/1 Running 0 20h 192.168.61.40 ip-192-168-44-68.us-east-2.compute.internal \u003cnone\u003e \u003cnone\u003e dataplane-dataplane1-ch6g9-6889fdf76b-9gt4s 1/1 Running 0 6m15s 192.168.52.45 ip-192-168-44-68.us-east-2.compute.internal \u003cnone\u003e \u003cnone\u003e dataplane-dataplane1-ch6g9-6889fdf76b-mrjdx 1/1 Running 0 6m15s 192.168.36.12 ip-192-168-44-68.us-east-2.compute.internal \u003cnone\u003e \u003cnone\u003e As we can see, the 2 new Pods have been created and are up and running. If we check our Kubernetes Service again, we will see it has been updated with the new IP addresses. That allows the Service to implement Load Balancing across the Pod replicas.\nkubectl describe service proxy1 -n kong Sample Output\nName: proxy1 Namespace: kong Labels: app=dataplane1 gateway-operator.konghq.com/dataplane-service-state=live gateway-operator.konghq.com/dataplane-service-type=ingress gateway-operator.konghq.com/managed-by=dataplane Annotations: \u003cnone\u003e Selector: app=dataplane1,gateway-operator.konghq.com/selector=8525cad1-ca9c-42c5-9c19-a053a25ff580 Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.110.155.140 IPs: 10.110.155.140 LoadBalancer Ingress: 127.0.0.1 (VIP) Port: http 80/TCP TargetPort: 8000/TCP NodePort: http 30164/TCP Endpoints: 10.244.0.89:8000,10.244.0.91:8000,10.244.0.90:8000 Port: https 443/TCP TargetPort: 8443/TCP NodePort: https 32512/TCP Endpoints: 10.244.0.89:8443,10.244.0.91:8443,10.244.0.90:8443 Session Affinity: None External Traffic Policy: Cluster Internal Traffic Policy: Cluster Events: \u003cnone\u003e Reduce the number of Pods to 1 again running as now we will turn on Horizontal pod autoscalar.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 replicas: 1 network: services: ingress: name: proxy1 type: LoadBalancer EOF HPA - Horizontal Autoscaler HPA (“Horizontal Pod Autoscaler”) is the Kubernetes resource to automatically control the number of replicas of Pods. With HPA, Kubernetes is able to support the requests produced by the consumers, keeping a given Service Level.\nBased on CPU utilization or custom metrics, HPA starts and terminates Pods replicas updating all service data to help on the load balancing policies over those replicas.\nHPA is described at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/. Also, there’s a nice walkthrough at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/\nKubernetes defines its own units for cpu and memory. You can read more about it at: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/. We use these units to set our Deployments with HPA.\nMetrics Server HPA relies no the Metrics Server to control the number of replicas of a given deployment. Check it as follows:\nkubectl get pod -n kube-system -o json | jq -r '.items[].metadata | select(.name | startswith(\"metrics-server-\"))' | jq -r '.name' Now you should see two metrics-server- pods in Running state\nSample Output\nmetrics-server-7fbb699795-5qqp5 Turn HPA on Still using the Operator, let’s upgrade our Data Plane deployment including new and specific settings for HPA. The new settings are defining the ammount of CPU and memory each Pod should allocate. At the same time, the “scaling” sets are telling HPA how to proceed to instantiate new Pod replicas.\nHere’s the final declaration:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 resources: requests: memory: \"300Mi\" cpu: \"300m\" limits: memory: \"800Mi\" cpu: \"1200m\" scaling: horizontal: minReplicas: 1 maxReplicas: 20 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 20 EOF Checking HPA After submitting the command check the Deployment again. Since we’re not consume the Data Plane, we are supposed to see a single Pod running. In the next sections we’re going to send requests to the Data Plane and new Pod will get created to handle them.\nkubectl get pod -n kong Sample Output\nNAME READY STATUS RESTARTS AGE dataplane-dataplane1-ch6g9-5fb9c6484b-kklw5 1/1 Running 0 73s You can check the HPA status with:\nkubectl get hpa -n kong Send traffic\nWe are going to use Fortio to consume the Data Plane and see the HPA in action. Note we are interested on sending traffic to the Data Plane only, so we are consuming a non-existing Route.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: fortio labels: app: fortio spec: containers: - name: fortio image: fortio/fortio args: [\"load\", \"-c\", \"100\", \"-qps\", \"500\", \"-t\", \"20m\", \"-allow-initial-errors\", \"http://proxy1.kong.svc.cluster.local:80/route1/get\"] EOF Sample Output\nEventually, HPA will start a new replica:\n% kubectl get hpa -n kong NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE dataplane1 Deployment/dataplane-dataplane1-ch6g9 cpu: 2%/75% 1 20 2 14m % kubectl get pod -n kong -o json | jq -r '.items[].metadata.name' dataplane-dataplane1-ch6g9-8c6c9cfcd-qbqp5 dataplane-dataplane1-ch6g9-8c6c9cfcd-rxcwm If you delete the Fortio pod, HPA should terminate one pod and get back to 1 replica only. kubectl delete pod fortio Delete HPA\nDelete the HPA setting applying the original declaration\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 replicas: 1 network: services: ingress: name: proxy1 type: LoadBalancer EOF Kong-gratulations! have now reached the end of this module by implementing and successfully testing Horizontal Pod AutoScaling. You can now click Next to proceed with the next chapter.",
    "description": "One of the most important capabilities provided by Kubernetes is to easily scale out a Deployment. With a single command we can create or terminate pod replicas in order to optimally support a given throughput.\nThis capability is especially interesting for Kubernetes applications like Kong for Kubernetes Ingress Controller.\nHere’s our deployment before scaling it out:\nkubectl get service -n kong Sample Output\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dataplane-admin-dataplane1-wjk92 ClusterIP None \u003cnone\u003e 8444/TCP 4d21h proxy1 LoadBalancer 10.110.77.48 127.0.0.1 80:32462/TCP,443:31666/TCP 4d21h Notice, at this point in the workshop, there is only one pod taking data plane traffic.",
    "tags": [],
    "title": "Data Plane Elasticity",
    "uri": "/11-konnect-setup/114-elasticity/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "With our Control Plane created and Data Plane layer deployed it’s time to create an API and expose an application. In this module, we will:\nDeploy an application to get protected by the Data Plane Use decK to define a Kong Service based on an endpoint provided by the application and a Kong Route on top of the Kong Service to expose the application. Enable Kong Plugins to the Kong Route or Kong Service. Define Kong Consumers to represent the entities sending request to the Gateway and enable Kong Plugin to them. With decK (declarations for Kong) you can manage Kong Konnect configuration in a declaratively way.\ndecK operates on state files. decK state files describe the configuration of Kong API Gateway. State files encapsulate the complete configuration of Kong in a declarative format, including services, routes, plugins, consumers, and other entities that define how requests are processed and routed through Kong.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Konnect Gateway Manager",
    "description": "With our Control Plane created and Data Plane layer deployed it’s time to create an API and expose an application. In this module, we will:\nDeploy an application to get protected by the Data Plane Use decK to define a Kong Service based on an endpoint provided by the application and a Kong Route on top of the Kong Service to expose the application. Enable Kong Plugins to the Kong Route or Kong Service. Define Kong Consumers to represent the entities sending request to the Gateway and enable Kong Plugin to them. With decK (declarations for Kong) you can manage Kong Konnect configuration in a declaratively way.",
    "tags": [],
    "title": "Kong API Gateway",
    "uri": "/12-api-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway",
    "content": "Gateway Services represent the upstream services in your system. These applications are the business logic components of your system responsible for responding to requests.\nThe configuration of a Gateway Service defines the connectivity details between the Kong Gateway and the upstream service, along with other metadata. Generally, you should map one Gateway Service to each upstream service.\nFor simple deployments, the upstream URL can be provided directly in the Gateway Service. For sophisticated traffic management needs, a Gateway Service can point at an Upstream.\nGateway Services, in conjunction with Routes, let you expose your upstream services to clients with Kong Gateway.\nPlugins can be attached to a Service, and will run against every request that triggers a request to the Service that they’re attached to.\nFor the purpose of this workshop, you’ll create and expose a service to the HTTPbin API. HTTPbin is an echo-type application that returns requests back to the requester as responses.\nDeploy the Application Deploy the application using the following declaration with both Kubernetes Deployment and Service.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: httpbin namespace: kong labels: app: httpbin spec: type: ClusterIP ports: - name: http port: 8000 targetPort: 80 selector: app: httpbin --- apiVersion: apps/v1 kind: Deployment metadata: name: httpbin namespace: kong spec: replicas: 1 selector: matchLabels: app: httpbin version: v1 template: metadata: labels: app: httpbin version: v1 spec: containers: - image: docker.io/kong/httpbin imagePullPolicy: IfNotPresent name: httpbin ports: - containerPort: 8000 EOF Check the Deployment Observe Kubernetes Services in kong namespace kubectl get service httpbin -n kong Sample Output\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE httpbin ClusterIP 10.100.89.150 \u003cnone\u003e 8000/TCP 20h Observe Pods in kong namespace kubectl get pod -n kong Sample Output\nNAME READY STATUS RESTARTS AGE dataplane-dataplane1-rbwck-98fcbc654-rt75p 1/1 Running 0 76m httpbin-5c69574c95-xq76q 1/1 Running 0 20h Ping Konnect with decK Before start using decK, you should ping Konnect to check if the connecting is up. Note we assume you have the PAT environment variable set. Please, refer to the previous section to learn how to issue a PAT.\ndeck gateway ping --konnect-control-plane-name kong-workshop --konnect-token $PAT Expected Output\nSuccessfully Konnected to the Example-Name organization! Create a Kong Gateway Service and Kong Route Create the following declaration first. Remarks:\nNote the host and port refers to the HTTPbin’s Kubernetes Service FQDN (Fully Qualified Domain Name), in our case http://httpbin.kong.svc.cluster.local:8000. The declaration tags the objects so you can managing them apart from other ones. cat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /httpbin-route EOF Submit the declaration Now, you can use the following command to sync your Konnect Control Plane with the declaration. Note that all other existing objects will be deleted.\ndeck gateway sync --konnect-token $PAT httpbin.yaml Expected Output\ncreating service httpbin-service creating route httpbin-route Summary: Created: 2 Updated: 0 Deleted: 0 You should see your new service’s overview page.\nConsume the Route We are to use the same ELB provisioned during the Data Plane deployment:\ncurl -v $DATA_PLANE_LB/httpbin-route/get If successful, you should see the httpbin output:\n* Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Content-Length: 377 \u003c Connection: keep-alive \u003c Server: gunicorn \u003c Date: Wed, 06 Aug 2025 16:19:15 GMT \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Credentials: true \u003c X-Kong-Upstream-Latency: 6 \u003c X-Kong-Proxy-Latency: 3 \u003c Via: 1.1 kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: 0cbe555eefb4f14bb43f9b511435bd5c \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"127.0.0.1\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"0cbe555eefb4f14bb43f9b511435bd5c\"},\"origin\":\"10.244.0.1\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host 127.0.0.1 left intact Kong-gratulations! have now reached the end of this module by having your first service set up, running, and routing traffic proxied through a Kong data plane. You can now click Next to proceed with the next module.",
    "description": "Gateway Services represent the upstream services in your system. These applications are the business logic components of your system responsible for responding to requests.\nThe configuration of a Gateway Service defines the connectivity details between the Kong Gateway and the upstream service, along with other metadata. Generally, you should map one Gateway Service to each upstream service.\nFor simple deployments, the upstream URL can be provided directly in the Gateway Service. For sophisticated traffic management needs, a Gateway Service can point at an Upstream.",
    "tags": [],
    "title": "Kong Gateway Service and Kong Route",
    "uri": "/12-api-gateway/122-kong-service-route/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Proxy plugin is the fundamental AI Gateway component. It lets you transform and proxy requests to a number of AI providers and models. The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nThe following table describes which providers and requests the AI Proxy plugin supports:\nObs 1: OpenAI has marked Completions as legacy and recommends using the Chat Completions API for developing new applications.\nObs 2: Starting with Kong AI Gateway 3.11, new GenAI APIs are supported:\nGetting Started with Kong AI Gateway We are going to get started with a simple configuration. The following decK declaration enables the AI Proxy plugin to the Kong Gateway Service, to send requests to the LLM and consume the Ollama’s lamma3.2:1b FM and OpenAI’s gpt-5 FM with chat LLM requests.\nUpdate your ai-proxy.yaml file with that. Make sure you have the DECK_OPENAI_API_KEY environment variable set with your OpenAI’s API Key.\ncat \u003e ai-proxy.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-5 options: temperature: 1.0 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy.yaml OpenAI API Kong AI Gateway provides one API to access all of the LLMs it supports. To accomplish this, Kong AI Gateway has standardized on the OpenAI API specification. This will help developers to onboard more quickly by providing them with an API specification that they’re already familiar with. You can start using LLMs behind the AI Gateway simply by redirecting your requests to a URL that points to a route of the AI Gateway.\nSend a request to Kong AI Gateway Now, send a request to Kong AI Gateway following the OpenAI API Chat specification as a reference:\ncurl -s -X POST \\ --url http://$DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ] }' | jq Expected Output\nNote the response also complies to the OpenAI API spec:\n{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ] }' | jq { \"id\": \"chatcmpl-C3jWHoMI65rb0Ojkai1NjBq0JoRMG\", \"object\": \"chat.completion\", \"created\": 1755005997, \"model\": \"gpt-5-2025-08-07\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Pi (π) is the mathematical constant equal to the ratio of a circle’s circumference to its diameter. It’s the same for all circles.\\n\\n- Approximate value: 3.141592653589793…\\n- Nature: irrational (non-terminating, non-repeating) and transcendental.\\n- Common formulas:\\n - Circumference: C = 2πr\\n - Area of a circle: A = πr²\\n - Appears widely, e.g., e^(iπ) + 1 = 0, normal distribution, waves/Fourier analysis.\\n- Handy approximations: 22/7 ≈ 3.142857, 355/113 ≈ 3.14159292.\\n\\nIf you want more digits or historical background, say the word.\", \"refusal\": null, \"annotations\": [] }, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 10, \"completion_tokens\": 621, \"total_tokens\": 631, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 448, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": null } You can also consume the Ollama’s route\ncurl -s -X POST \\ --url http://$DATA_PLANE_LB/ollama-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ] }' | jq AI Proxy configuration parameters The AI Proxy plugin is responsible for a variety of topics. For example:\nRequest and response formats appropriate for the configured provider and route_type settings. provider can be set as anthropic, azure, bedrock, cohere, gemini, huggingface, llama2, mistral or openai. The route_type AI Proxy configuration parameter defines which kind of request the AI Gateway is going to perform. It must be one of: audio/v1/audio/speech audio/v1/audio/transcriptions audio/v1/audio/translations image/v1/images/edits image/v1/images/generations llm/v1/assistants llm/v1/batches llm/v1/chat llm/v1/completions llm/v1/embeddings llm/v1/files llm/v1/responses preserve realtime/v1/realtime Authentication on behalf of the Kong API consumer. Decorating the request with parameters from the config.model.options block, appropriate for the chosen provider. For our case, we tell the temperature we are going to use. Define the model to be consume when sending the request As you may have noticed our AI Proxy plugin defines the model it should consume. That is can be done for individual requests, if required. Change the ai-proxy.yaml file, removing the model’s name parameter and apply the declaration again:\ncat \u003e ai-proxy.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai options: temperature: 1.0 EOF deck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-proxy.yaml Send the request specifing the model:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-5\" }' or\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-4\" }' Note the Kong AI Proxy plugin adds a new X-Kong-LLM-Model header with the model we consumer: openai/gpt-5 or openai/gpt-4\nStreaming Normally, a request is processed and completely buffered by the LLM before being sent back to Kong AI Gateway and then to the caller in a single large JSON block. This process can be time-consuming, depending on the request parameters, and the complexity of the request sent to the LLM model. To avoid making the user wait for their chat response with a loading animation, most models can stream each word (or sets of words and tokens) back to the client. This allows the chat response to be rendered in real time.\nThe config AI Proxy configuration section has a response_streaming parameter to define the response streaming. By default is set as allow but it can be set with deny or always.\nAs an example, if you send the same request with the stream parameter as true you should see a response like this:\ncurl -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"what is pi?\" } ], \"model\": \"gpt-4\", \"stream\": true }' data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\",\"refusal\":null},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"D5jIQAiER0kD2\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Pi\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"3S9RmT4NS9k3b\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\" (\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"3ARtgUA4COqRA\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\"π\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"IS99TImGO4SoLp\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\")\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"8jpC3eE7bQvh7b\"} ... data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\"},\"logprobs\":null,\"finish_reason\":null}],\"obfuscation\":\"YHL01GZcTNF1Wh\"} data: {\"id\":\"chatcmpl-C3jmyBQgTgsLd421Wg8fBhM0xkOiK\",\"object\":\"chat.completion.chunk\",\"created\":1755007032,\"model\":\"gpt-4-0613\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"choices\":[{\"index\":0,\"delta\":{},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"obfuscation\":\"vf2t9C6t3\"} data: [DONE] Extra Model Options The Kong AI Proxy provides other configuration options. For example:\nmax_tokens: defines the max_tokens, if using chat or completion models. temperature: it is a number between 0 and 5 and it defines the matching temperature, if using chat or completion models. top_p: a number between 0 and 1 defining the top-p probability mass, if supported. top_k: an integer between 0 and 500 defining the top-k most likely tokens, if supported. Kong-gratulations! have now reached the end of this module by caching API responses. You can now click Next to proceed with the next module.",
    "description": "The AI Proxy plugin is the fundamental AI Gateway component. It lets you transform and proxy requests to a number of AI providers and models. The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nThe following table describes which providers and requests the AI Proxy plugin supports:",
    "tags": [],
    "title": "AI Proxy",
    "uri": "/16-ai-gateway/17-use-cases/150-ai-proxy/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "Proxy Caching provides a reverse proxy cache implementation for Kong. It caches response entities based on configurable response code and content type, as well as request method. It can cache per-Consumer or per-API. Cache entities are stored for a configurable period of time, after which subsequent requests to the same resource will re-fetch and re-store the resource. Cache entities can also be forcefully purged via the Admin API prior to their expiration time.\nKong Gateway Plugin list Before enabling the Proxy Caching, let’s check the list of plugins Konnect provides. Inside the kong-workshop Control Plane, click on Plugins menu option and + New plugin. You should the following page with all plugins available:\nEnabling a Kong Plugin on a Kong Service Create another declaration with plugins option. With this option you can enable and configure the plugin on your Kong Service.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 plugins: - name: proxy-cache instance_name: proxy-cache1 config: strategy: memory cache_ttl: 30 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route EOF For the plugin configuration we used the following settings:\nstrategy with memory. The plugin will use the Runtime Instance’s memory to implement to cache. cache_ttl with 30, which means the plugin will clear all data that reached this time limit. All plugin configuration paramenters are described inside Kong Plugin Hub portal, in its specific documentation page.\nSubmit the new declaration deck gateway sync --konnect-token $PAT httpbin.yaml Expected Output\ncreating plugin proxy-cache for service httpbin-service Summary: Created: 1 Updated: 0 Deleted: 0 Consume the Service If you consume the service again, you’ll see some new headers describing the caching status:\ncurl -v $DATA_PLANE_LB/httpbin-route/get * Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Content-Length: 377 \u003c Connection: keep-alive \u003c X-Cache-Key: a00008105a989fd0fa8a1eeeee08924b7205d24ed1adee71698926c12a31f2b7 \u003c X-Cache-Status: Miss \u003c Server: gunicorn \u003c Date: Mon, 11 Aug 2025 14:39:46 GMT \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Credentials: true \u003c X-Kong-Upstream-Latency: 8 \u003c X-Kong-Proxy-Latency: 6 \u003c Via: 1.1 kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: 4501cc0fa798cf08435edc01bb2b1a40 \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"127.0.0.1\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"4501cc0fa798cf08435edc01bb2b1a40\"},\"origin\":\"10.244.0.1\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host 127.0.0.1 left intact Notice that, for the first request we get Miss for the X-Cache-Status header, meaning that the Runtime Instance didn’t have any data avaialble in the cache and had to connect to the Upstream Service, httpbin.org.\nIf we send a new request, the Runtime Instance has all it needs to satify the request, therefore the status is Hit. Note that the latency time has dropped considerably.\n# curl -v $DATA_PLANE_LB/httpbin-route/get * Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Connection: keep-alive \u003c X-Cache-Key: a00008105a989fd0fa8a1eeeee08924b7205d24ed1adee71698926c12a31f2b7 \u003c Access-Control-Allow-Credentials: true \u003c X-Cache-Status: Hit \u003c Access-Control-Allow-Origin: * \u003c Date: Mon, 11 Aug 2025 14:40:17 GMT \u003c age: 3 \u003c Server: gunicorn \u003c Content-Length: 377 \u003c X-Kong-Upstream-Latency: 0 \u003c X-Kong-Proxy-Latency: 1 \u003c Via: 1.1 kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: 97cc6027e33f240a67d8930161b44e57 \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"127.0.0.1\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"2228de44dadd2e6126d82c4fb2e43961\"},\"origin\":\"10.244.0.1\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host 127.0.0.1 left intact Enabling a Kong Plugin on a Kong Route Now, we are going to define a Rate Limiting policy for our Service. This time, you are going to enable the Rate Limiting plugin to the Kong Route, not to the Kong Gateway Service. In this sense, new Routes defined for the Service will not have the Rate Limiting plugin enabled, only the Proxy Caching.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 plugins: - name: proxy-cache config: strategy: memory cache_ttl: 30 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 3 EOF The configuration includes:\nminute as 3, which means the Route can be consumed only 3 times a given minute. Submit the declaration deck gateway sync --konnect-token $PAT httpbin.yaml Consume the Service If you consume the service again, you’ll see, besides the caching related headers, new ones describing the status of current rate limiting policy:\ncurl -v $DATA_PLANE_LB/httpbin-route/get * Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Content-Length: 377 \u003c Connection: keep-alive \u003c X-RateLimit-Limit-Minute: 3 \u003c RateLimit-Remaining: 2 \u003c RateLimit-Reset: 32 \u003c RateLimit-Limit: 3 \u003c X-RateLimit-Remaining-Minute: 2 \u003c X-Cache-Key: a00008105a989fd0fa8a1eeeee08924b7205d24ed1adee71698926c12a31f2b7 \u003c X-Cache-Status: Miss \u003c Server: gunicorn \u003c Date: Mon, 11 Aug 2025 14:41:28 GMT \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Credentials: true \u003c X-Kong-Upstream-Latency: 1 \u003c X-Kong-Proxy-Latency: 5 \u003c Via: 1.1 kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: 882b11008e7ddd2eff471a433576524d \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"127.0.0.1\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"882b11008e7ddd2eff471a433576524d\"},\"origin\":\"10.244.0.1\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host 127.0.0.1 left intact If you keep sending new requests to the Runtime Instance, eventually, you’ll get a 429 error code, meaning you have reached the consumption rate limiting policy for this Route.\ncurl -v $DATA_PLANE_LB/httpbin-route/get * Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 429 Too Many Requests \u003c Date: Mon, 11 Aug 2025 14:41:58 GMT \u003c Content-Type: application/json; charset=utf-8 \u003c Connection: keep-alive \u003c X-RateLimit-Limit-Minute: 3 \u003c X-RateLimit-Remaining-Minute: 0 \u003c RateLimit-Reset: 2 \u003c Retry-After: 2 \u003c RateLimit-Remaining: 0 \u003c RateLimit-Limit: 3 \u003c Content-Length: 92 \u003c X-Kong-Response-Latency: 1 \u003c Server: kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: ce56eb67161a85678126a00ef59e6159 \u003c { \"message\":\"API rate limit exceeded\", \"request_id\":\"ce56eb67161a85678126a00ef59e6159\" * Connection #0 to host 127.0.0.1 left intact } Enabling a Kong Plugin globally Besides scoping a plugin to a Kong Service or Route, we can apply it globally also. When we do it so, all Services ans Routes will enforce the police described by the plugin.\nFor example, let’s apply the Proxy Caching plugin globally.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route plugins: - name: proxy-cache config: strategy: memory cache_ttl: 30 services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 3 EOF Submit the declaration deck gateway sync --konnect-token $PAT httpbin.yaml After testing the configuration reset the Control Plane:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f Kong-gratulations! have now reached the end of this module by caching API responses. You can now click Next to proceed with the next module.",
    "description": "Proxy Caching provides a reverse proxy cache implementation for Kong. It caches response entities based on configurable response code and content type, as well as request method. It can cache per-Consumer or per-API. Cache entities are stored for a configurable period of time, after which subsequent requests to the same resource will re-fetch and re-store the resource. Cache entities can also be forcefully purged via the Admin API prior to their expiration time.",
    "tags": [],
    "title": "Proxy Caching",
    "uri": "/12-api-gateway/15-use-cases/150-proxy-caching/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway",
    "content": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nProxy caching API Key with Kong Consumers Rate Limiting using Redis Request Transformer Request Callout OpenID Connect with Keycloak Access Control with Open Policy Agent (OPA) These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "description": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nProxy caching API Key with Kong Consumers Rate Limiting using Redis Request Transformer Request Callout OpenID Connect with Keycloak Access Control with Open Policy Agent (OPA) These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong plugins on the Plugin Hub.",
    "tags": [],
    "title": "Use Cases",
    "uri": "/12-api-gateway/15-use-cases/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "To get started with API Authentication, let’s implement a basic Key Authentication mechanism. API Keys are one of the foundamental security mechanisms provided by Konnect. In order to consume an API, the consumer should inject a previously created API Key in the header of the request. The API consumption is allowed if the Gateway recognizes the API Key. Consumers add their API key either in a query string parameter, a header, or a request body to authenticate their requests and consume the application.\nA Kong Consumer represents a consumer (user or application) of a Service. A Kong Consumer is tightly coupled to an Authentication mechanism the Kong Gateway provides.\nPlease, check the Key-Auth plugin plugin and Kong Consumer documentation pages to learn more about them.\nEnable the Key Authentication Plugin on the Kong Route cat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 EOF deck gateway sync --konnect-token $PAT key-auth.yaml Consume the Route Now, if you try the Route, you’ll get a specific 401 error code meaning that, since you don’t have any API Key injected in your request, you are not allowd to consume it.\ncurl -i $DATA_PLANE_LB/key-auth-route/get HTTP/1.1 401 Unauthorized Date: Mon, 11 Aug 2025 14:44:59 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Key Content-Length: 96 X-Kong-Response-Latency: 2 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 1f8a6c1c9d0d1853d9db426588c1ce1c { \"message\":\"No API key found in request\", \"request_id\":\"1f8a6c1c9d0d1853d9db426588c1ce1c\" } Create a Kong Consumer In order to consume the Route we need to create a Kong Consumer. Here’s its declaration:\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 EOF Submit the declaration deck gateway sync --konnect-token $PAT key-auth.yaml Consume the Route with the API Key Now, you need to inject the Key you’ve just created, as a header, in your requests. Using HTTPie, you can do it easily like this:\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:123456' HTTP/1.1 200 OK Content-Type: application/json Content-Length: 551 Connection: keep-alive Server: gunicorn Date: Mon, 11 Aug 2025 14:45:52 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 9 X-Kong-Proxy-Latency: 4 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: b535885591f5ec7f7fb5f070fa365465 Of course, if you inject a wrong key, you get a specific error like this:\n# curl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:12' HTTP/1.1 401 Unauthorized Date: Mon, 11 Aug 2025 14:46:36 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Key Content-Length: 81 X-Kong-Response-Latency: 1 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 688e271ea4cae5794bc1cb59ea3ec131 NOTE\nThe header has to have the API Key name, which is, in our case, apikey. That was the default name provided by Konnect when you enabled the Key Authentication on the Kong Route. You can change the plugin configuration, if you will. Kong Consumer Policies With the API Key policy in place, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIt’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:\nconsumer1: apikey = 123456 rate limiting policy = 5 rpm consumer2: apikey = 987654 rate limiting policy = 8 rpm Doing that, the Data Plane is capable to not just protect the Route but to identify the consumer based on the key injected to enforce specific policies to the consumer.\nFor this section we’re implementing a Rate Limiting policy. Keep in mind that a Consumer might have other plugins also enabled such as Request Transformer, TCP Log, etc.\nNew Consumer Create the second consumer2, just like you did with the first one, with the 987654 key.\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 - keyauth_credentials: - key: \"987654\" username: consumer2 EOF Submit the declaration deck gateway sync --konnect-token $PAT key-auth.yaml If you will, you can inject both keys to your requests.\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:123456' or\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:987654' Consumers’ Policy Now let’s enhance the plugins declaration enabling the Rate Limiting plugin to each one of our consumers.\ncat \u003e key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /key-auth-route plugins: - name: key-auth instance_name: key-auth1 consumers: - keyauth_credentials: - key: \"123456\" username: consumer1 plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 5 - keyauth_credentials: - key: \"987654\" username: consumer2 plugins: - name: rate-limiting instance_name: rate-limiting2 config: minute: 8 EOF Submit the declaration deck gateway sync --konnect-token $PAT key-auth.yaml Consumer the Route using different API Keys. First of all let’s consume the Route with the Consumer1’s API Key:\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:123456' Expected Output\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 551 Connection: keep-alive X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 4 RateLimit-Reset: 11 RateLimit-Remaining: 4 RateLimit-Limit: 5 Server: gunicorn Date: Mon, 11 Aug 2025 14:47:49 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 3 X-Kong-Proxy-Latency: 2 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: ad38e60e76c57d5826f3c37fdce4925c Now, let’s consume it with the Consumer2’s API Key. As you can see the Data Plane is processing the Rate Limiting processes independently.\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:987654' Expected Output\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 551 Connection: keep-alive X-RateLimit-Limit-Minute: 8 X-RateLimit-Remaining-Minute: 7 RateLimit-Reset: 27 RateLimit-Remaining: 7 RateLimit-Limit: 8 Server: gunicorn Date: Mon, 11 Aug 2025 14:49:33 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 2 X-Kong-Proxy-Latency: 3 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 9cf1fb53db3a9b740d9bd42e9091d245 If we keep sending requests using the first API Key, eventually, as expected, we’ll get an error code:\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:123456' Expected Output\nHTTP/1.1 429 Too Many Requests Date: Mon, 11 Aug 2025 14:50:21 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 0 RateLimit-Reset: 39 Retry-After: 39 RateLimit-Remaining: 0 RateLimit-Limit: 5 Content-Length: 92 X-Kong-Response-Latency: 1 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 38afd254e246a946a65153780606be3c However, the second API Key is still allowed to consume the Kong Route:\ncurl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:987654' Expected Output\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 551 Connection: keep-alive X-RateLimit-Limit-Minute: 8 X-RateLimit-Remaining-Minute: 7 RateLimit-Reset: 34 RateLimit-Remaining: 7 RateLimit-Limit: 8 Server: gunicorn Date: Mon, 11 Aug 2025 14:50:26 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 1 X-Kong-Proxy-Latency: 2 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: f8602e2e2778f306fba41f1661ef554c Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.\nOptional Reading Applying Kong Plugins on Services, Routes or Globally helps us to implement an extensive list of policies in the API Gateway layer. However, so far, we are not controlling who is sending the requests to the Data Plane. That is, anyone who has the Runtime Instance ELB address is capable to send requests to it and consumer the Services.\nAPI Gateway Authentication is an important way to control the data that is allowed to be transmitted using your APIs. Basically, it checks that a particular consumer has permission to access the API, using a predefined set of credentials.\nKong Gateway has a library of plugins that provide simple ways to implement the best known and most widely used methods of API gateway authentication. Here are some of the commonly used ones:\nBasic Authentication Key Authentication OAuth 2.0 Authentication LDAP Authentication OpenID Connect Kong Plugin Hub provides documentation about all Authentication based plugins. Refer to the following link to read more about API Gateway Authentication",
    "description": "To get started with API Authentication, let’s implement a basic Key Authentication mechanism. API Keys are one of the foundamental security mechanisms provided by Konnect. In order to consume an API, the consumer should inject a previously created API Key in the header of the request. The API consumption is allowed if the Gateway recognizes the API Key. Consumers add their API key either in a query string parameter, a header, or a request body to authenticate their requests and consume the application.",
    "tags": [],
    "title": "API Key Authentication",
    "uri": "/12-api-gateway/15-use-cases/151-key-authentication/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "For Prompt Engineering use cases, Kong AI Gateway provides:\nAI Prompt Decorator plugin: it injects messages at the start or end of a caller’s chat history. AI Prompt Guard plugin: it lets you configure a series of PCRE-compatible regular expressions to allow and block specific prompts, words, phrases, or otherwise and have more control over the LLM. AI Prompt Template plugin: it’s responsible for pre-configuring AI prompts to users All plugins extend the functionality of the AI Proxy plugin, and requires AI Proxy to be configured first.\nYou can now click Next to proceed further.",
    "description": "For Prompt Engineering use cases, Kong AI Gateway provides:\nAI Prompt Decorator plugin: it injects messages at the start or end of a caller’s chat history. AI Prompt Guard plugin: it lets you configure a series of PCRE-compatible regular expressions to allow and block specific prompts, words, phrases, or otherwise and have more control over the LLM. AI Prompt Template plugin: it’s responsible for pre-configuring AI prompts to users All plugins extend the functionality of the AI Proxy plugin, and requires AI Proxy to be configured first.",
    "tags": [],
    "title": "Prompt Engineering",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "The Response Transformer plugin modifies the upstream response (e.g. response from the server) before returning it to the client.\nIn this section, you will configure the Response Transformer plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\ncat \u003e response-transformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: response-transformer-route paths: - /response-transformer-route plugins: - name: response-transformer instance_name: response-transformer1 config: add: headers: - demo:injected-by-kong EOF Submit the declaration deck gateway sync --konnect-token $PAT response-transformer.yaml Verify Test to make sure Kong transforms the request to the echo server and httpbin server.\ncurl --head $DATA_PLANE_LB/response-transformer-route/get HTTP/1.1 200 OK Content-Type: application/json Content-Length: 403 Connection: keep-alive Server: gunicorn Date: Mon, 11 Aug 2025 14:51:56 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true demo: injected-by-kong X-Kong-Upstream-Latency: 1 X-Kong-Proxy-Latency: 2 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 6d294407e61075665321d07709210e3a Expected Results Notice that demo: injected-by-kong is injected in the header.\nCleanup Reset the Control Plane to ensure that the plugins do not interfere with any other modules in the workshop for demo purposes and each workshop module code continues to function independently.\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f In real world scenario, you can enable as many plugins as you like depending on your use cases.\nKong-gratulations! have now reached the end of this module by configuring the Kong Route to include demo: injected-by-kong before responding to the client. You can now click Next to proceed with the next module.",
    "description": "The Response Transformer plugin modifies the upstream response (e.g. response from the server) before returning it to the client.\nIn this section, you will configure the Response Transformer plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\ncat \u003e response-transformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: response-transformer-route paths: - /response-transformer-route plugins: - name: response-transformer instance_name: response-transformer1 config: add: headers: - demo:injected-by-kong EOF Submit the declaration deck gateway sync --konnect-token $PAT response-transformer.yaml",
    "tags": [],
    "title": "Response Transformer",
    "uri": "/12-api-gateway/15-use-cases/153-response-transformer/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "The Request Callout plugin allows you to insert arbitrary API calls before proxying a request to the upstream service.\nIn this section, you will configure the Request Callout plugin on the Kong Route. Specifically, you will configure the plugin to do the following:\nCall Wikipedia using the “srseach” header as a parameter. The number of hits found and returned by Wikipeadia is added as a new header to the request. The request is sent to httpbin application which echoes the number of hits. Hit Wikipedia Just to get an idea what the Wikipedia response, send the following request:\ncurl -s \"https://en.wikipedia.org/w/api.php?srsearch=Miles%20Davis\u0026action=query\u0026list=search\u0026format=json\" | jq '.query.searchinfo.totalhits' You should get a number like 43555, which represents the number of total hits related to Miles Davis\nCreate the Request Callout Plugin Take the plugins declaration and enable the Request Callout plugin to the Route.\ncat \u003e request-callout.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: request-callout-route paths: - /request-callout-route plugins: - name: request-callout instance_name: request-callout1 config: callouts: - name: wikipedia request: url: https://en.wikipedia.org/w/api.php method: GET query: forward: true by_lua: local srsearch = kong.request.get_header(\"srsearch\"); local srsearch_encoded = ngx.escape_uri(srsearch) query = \"srsearch=\" .. srsearch_encoded .. \"\u0026action=query\u0026list=search\u0026format=json\"; kong.log.inspect(query); kong.ctx.shared.callouts.wikipedia.request.params.query = query response: body: decode: true by_lua: kong.service.request.add_header(\"wikipedia-total-hits-header\", kong.ctx.shared.callouts.wikipedia.response.body.query.searchinfo.totalhits) EOF Submit the declaration deck gateway sync --konnect-token $PAT request-callout.yaml Verify Send the request to Kong and check the response\ncurl -s \"http://$DATA_PLANE_LB/request-callout-route/get\" -H srsearch:\"Miles Davis\" | jq { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"keep-alive\", \"Content-Length\": \"0\", \"Host\": \"httpbin.kong.svc.cluster.local:8000\", \"Srsearch\": \"Miles Davis\", \"User-Agent\": \"curl/8.7.1\", \"Wipikedia-Total-Hits-Header\": \"43555\", \"X-Forwarded-Host\": \"127.0.0.1\", \"X-Forwarded-Path\": \"/request-callout-route/get\", \"X-Forwarded-Prefix\": \"/request-callout-route\", \"X-Kong-Request-Id\": \"6e4df528567f446630c6ae5c0b461c2e\" }, \"origin\": \"10.244.0.1\", \"url\": \"http://httpbin.kong.svc.cluster.local:8000/get\" } Expected Results Notice that new Wikipedia-Total-Hits-Header header is injected.\nCleanup Reset the Control Plane to ensure that the plugins do not interfere with any other modules in the workshop for demo purposes and each workshop module code continues to function independently.\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f In real world scenario, you can enable as many plugins as you like depending on your use cases.\nKong-gratulations! have now reached the end of this module. You can now click Next to proceed with the next module.",
    "description": "The Request Callout plugin allows you to insert arbitrary API calls before proxying a request to the upstream service.\nIn this section, you will configure the Request Callout plugin on the Kong Route. Specifically, you will configure the plugin to do the following:\nCall Wikipedia using the “srseach” header as a parameter. The number of hits found and returned by Wikipeadia is added as a new header to the request. The request is sent to httpbin application which echoes the number of hits. Hit Wikipedia Just to get an idea what the Wikipedia response, send the following request:",
    "tags": [],
    "title": "Request Callout",
    "uri": "/12-api-gateway/15-use-cases/154-request-callout/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Request Transformer and AI Response Transformer plugins integrate with the LLM on Amazon Bedrock, enabling introspection and transformation of the request’s body before proxying it to the Upstream Service and prior to forwarding the response to the client.\nThe plugins support llm/v1/chat style requests for all of the same providers as the AI Proxy plugin. It also use all of the same configuration and tuning parameters as the AI Proxy plugin, under the config.llm block.\nThe AI Request Transformer plugin runs before all of the AI Prompt plugins and the AI Proxy plugin, allowing it to also introspect LLM requests against the same, or a different, LLM. On the other hand, the AI Response Transformer plugin runs after the AI Proxy plugin, and after proxying to the Upstream Service, allowing it to also introspect LLM responses against the same, or a different, LLM service.\nThe diagram shows the journey of a consumer’s request through Kong Gateway to the backend service, where it is transformed by both an AI LLM service and Kong’s AI Request Transformer and the AI Response Transformer plugins.\nFor each plugin the configuration and usage processes are:\nThe Kong Gateway admin sets up an llm: configuration block, following the same configuration format as the AI Proxy plugin, and the same driver capabilities. The Kong Gateway admin sets up a prompt for the request introspection. The prompt becomes the system message in the LLM chat request, and prepares the LLM with transformation instructions for the incoming user request body (for the AI Request Transformer plugin) and for the returning upstream response body (for the AI Response Transformer plugin) The user makes an HTTP(S) call. Before proxying the user’s request to the backend, Kong Gateway sets the entire request body as the user message in the LLM chat request, and then sends it to the configured LLM service. After receiving the response from the backend, Kong Gateway sets the entire response body as the user message in the LLM chat request, then sends it to the configured LLM service. The LLM service returns a response assistant message, which is subsequently set as the upstream request body. The following example is going to apply the plugins to transform both request and reponse when consuming the httpbin Upstream Service.\nNow, configure both plugins. Keep in mind that the plugins are totally independent from each other so, the configuration depends on your use case.\ncat \u003e ai-request-response-tranformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /httpbin-route plugins: - name: ai-request-transformer instance_name: ai-request-transformer enabled: true config: prompt: In my JSON message, anywhere there is a JSON tag for a \"city\", also add a \"country\" tag with the name of the country in which the city resides. Return me only the JSON message, no extra text.\" llm: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-response-transformer instance_name: ai-response-transformer enabled: true config: prompt: For any city name, add its current temperature, in brackets next to it. Reply with the JSON result only. llm: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-request-response-tranformer.yaml curl -s -X POST \\ --url $DATA_PLANE_LB/httpbin-route/post \\ --header 'Content-Type: application/json' \\ --data '{ \"user\": { \"name\": \"Kong User\", \"city\": \"Tokyo\" } }' | jq Expected output { \"user\": { \"name\": \"Kong User\", \"city\": \"Tokyo [12°C]\", \"country\": \"Japan\" } } Kong-gratulations! have now reached the end of this module by using Kong Gateway to invoke a AWS Lambda function. You can now click Next to proceed with the next chapter.",
    "description": "The AI Request Transformer and AI Response Transformer plugins integrate with the LLM on Amazon Bedrock, enabling introspection and transformation of the request’s body before proxying it to the Upstream Service and prior to forwarding the response to the client.\nThe plugins support llm/v1/chat style requests for all of the same providers as the AI Proxy plugin. It also use all of the same configuration and tuning parameters as the AI Proxy plugin, under the config.llm block.",
    "tags": [],
    "title": "AI Request and Response Transfomers",
    "uri": "/16-ai-gateway/17-use-cases/155-request-response-transformer/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "Semantic caching enhances data retrieval efficiency by focusing on the meaning or context of queries rather than just exact matches. It stores responses based on the underlying intent and semantic similarities between different queries and can then retrieve those cached queries when a similar request is made.\nWhen a new request is made, the system can retrieve and reuse previously cached responses if they are contextually relevant, even if the phrasing is different. This method reduces redundant processing, speeds up response times, and ensures that answers are more relevant to the user’s intent, ultimately improving overall system performance and user experience.\nFor example, if a user asks, “how to integrate our API with a mobile app” and later asks, “what are the steps for connecting our API to a smartphone application?”, the system understands that both questions are asking for the same information. It can then retrieve and reuse previously cached responses, even if the wording is different. This approach reduces processing time and speeds up responses.\nThe AI Semantic Cache plugin may not be ideal for you if:\nIf you have limited hardware or budget. Storing semantic vectors and running similarity searches require a lot of storage and computing power, which could be an issue. If your data doesn’t rely on semantics, or exact matches work fine, semantic caching may offer little benefit. Traditional or keyword-based caching might be more efficient. How it works The diagram below illustrates the semantic caching mechanism implemented by the AI Semantic Cache plugin.\nThe process involves three parts: request handling, embedding generation, and response caching.\nFirst, a user starts a chat request with the LLM. The AI Semantic Cache plugin queries the vector database to see if there are any semantically similar requests that have already been cached. If there is a match, the vector database returns the cached response to the user. If there isn’t a match, the AI Semantic Cache plugin prompts the embeddings LLM to generate an embedding for the response. The AI Semantic Cache plugin uses a vector database and cache to store responses to requests. The plugin can then retrieve a cached response if a new request matches the semantics of a previous request, or it can tell the vector database to store a new response if there are no matches. With the AI Semantic Cache plugin, you can configure a cache of your choice to store the responses from the LLM. Currently, the plugin supports Redis as a cache.\nRedis as a Vector database We are going to configure the AI Semantic Cache to consume the Redis deployment available in the EKS Cluster. Redis, this time, will play the Vector database role.\nApply the Semantic Cache plugin cat \u003e ai-semantic-cache.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _info: select_tags: - semantic-cache - llm _konnect: control_plane_name: kong-workshop services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai enabled: true config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: ai-semantic-cache instance_name: ai-semantic-cache-openai enabled: true config: embeddings: auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: \"text-embedding-3-small\" vectordb: dimensions: 1024 distance_metric: cosine strategy: redis threshold: 0.2 redis: host: \"redis-stack.redis.svc.cluster.local\" port: 6379 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-semantic-cache.yaml Check Redis Before sending request, you can scan the Redis database:\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan 1st Request Since we don’t have any cached data, the first request is going to return “Miss”:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected response HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-Cache-Status: Miss x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 Date: Tue, 12 Aug 2025 14:47:48 GMT x-ratelimit-remaining-tokens: 29993 access-control-expose-headers: X-Request-ID openai-organization: user-4qzstwunaw6d1dhwnga5bc5q openai-processing-ms: 7218 x-ratelimit-reset-requests: 120ms openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-cache-status: DYNAMIC openai-version: 2020-10-01 Server: cloudflare X-Content-Type-Options: nosniff x-envoy-upstream-service-time: 7420 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload CF-RAY: 96e0c4e97b364d3b-GRU x-ratelimit-limit-requests: 500 x-request-id: req_ae7f43291824451dbfec2a27b1a3ec2a x-ratelimit-reset-tokens: 14ms alt-svc: h3=\":443\"; ma=86400 X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2005 X-Kong-Upstream-Latency: 8820 X-Kong-Proxy-Latency: 876 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 8fd73d1623140f675ed93b0dcb4aeb16 { \"id\": \"chatcmpl-C3kZpdNpSx8eaIHhsLg14RhZgFBww\", \"object\": \"chat.completion\", \"created\": 1755010061, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (full name: James Marshall Hendrix, born November 27, 1942 – died September 18, 1970) was an American guitarist, singer, and songwriter, widely regarded as one of the most influential electric guitarists in the history of popular music. Emerging in the late 1960s, Hendrix revolutionized the way the guitar was played, using feedback, distortion, and an array of innovative techniques that transformed rock, blues, and psychedelic music.\\n\\nHendrix rose to fame with his band, **The Jimi Hendrix Experience**, delivering classic albums such as *Are You Experienced* (1967) and *Electric Ladyland* (1968). His groundbreaking performances included a legendary rendition of \\\"The Star-Spangled Banner\\\" at Woodstock in 1969.\\n\\nDespite his career only spanning about four years, Hendrix's influence endures through his recordings and his impact on generations of musicians. He was posthumously inducted into the Rock and Roll Hall of Fame in 1992. Some of his most famous songs include \\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"Voodoo Child (Slight Return),\\\" and \\\"All Along the Watchtower.\\\" Hendrix died at the age of 27, becoming one of the most iconic members of the so-called \\\"27 Club.\\\"\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 264, \"total_tokens\": 277, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_51e1070cf2\" } Check Redis again The Redis database has an entry now:\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan Expected response \"kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36\" 2nd Request The Semantic Cache plugin will use the cached data for similar requests:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Tell me more about Jimi Hendrix\" } ] }' Expected response HTTP/1.1 200 OK Date: Tue, 12 Aug 2025 14:48:55 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-Cache-Status: Hit Age: 67 X-Cache-Key: kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36 X-Cache-Ttl: 233 Content-Length: 1814 X-Kong-Response-Latency: 1438 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 2debcb5db5e6f3637bef912cca963a5d {\"object\":\"chat.completion\",\"created\":1755010061,\"id\":\"2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36\",\"usage\":{\"completion_tokens\":264,\"prompt_tokens_details\":{\"cached_tokens\":0,\"audio_tokens\":0},\"completion_tokens_details\":{\"accepted_prediction_tokens\":0,\"audio_tokens\":0,\"rejected_prediction_tokens\":0,\"reasoning_tokens\":0},\"total_tokens\":277,\"prompt_tokens\":13},\"model\":\"gpt-4.1-2025-04-14\",\"service_tier\":\"default\",\"system_fingerprint\":\"fp_51e1070cf2\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"annotations\":{},\"role\":\"assistant\",\"refusal\":null,\"content\":\"**Jimi Hendrix** (full name: James Marshall Hendrix, born November 27, 1942 – died September 18, 1970) was an American guitarist, singer, and songwriter, widely regarded as one of the most influential electric guitarists in the history of popular music. Emerging in the late 1960s, Hendrix revolutionized the way the guitar was played, using feedback, distortion, and an array of innovative techniques that transformed rock, blues, and psychedelic music.\\n\\nHendrix rose to fame with his band, **The Jimi Hendrix Experience**, delivering classic albums such as *Are You Experienced* (1967) and *Electric Ladyland* (1968). His groundbreaking performances included a legendary rendition of \\\"The Star-Spangled Banner\\\" at Woodstock in 1969.\\n\\nDespite his career only spanning about four years, Hendrix's influence endures through his recordings and his impact on generations of musicians. He was posthumously inducted into the Rock and Roll Hall of Fame in 1992. Some of his most famous songs include \\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"Voodoo Child (Slight Return),\\\" and \\\"All Along the Watchtower.\\\" Hendrix died at the age of 27, becoming one of the most iconic members of the so-called \\\"27 Club.\\\"\"}}]} 3rd Request As expected, for a non-related request, the AI Gateway will hit the LLM to satisfy the query:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who was Joseph Conrad?\" } ] }' Expected response HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-Cache-Status: Miss openai-version: 2020-10-01 x-envoy-upstream-service-time: 4746 Date: Tue, 12 Aug 2025 14:49:35 GMT x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 CF-RAY: 96e0c7a088ac1b20-GRU x-ratelimit-remaining-tokens: 29992 alt-svc: h3=\":443\"; ma=86400 access-control-expose-headers: X-Request-ID X-Content-Type-Options: nosniff Server: cloudflare openai-organization: user-4qzstwunaw6d1dhwnga5bc5q Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-request-id: req_10dee9e2989e46cbb32a2125f774f446 openai-processing-ms: 4658 cf-cache-status: DYNAMIC openai-project: proj_r4KYFyenuGWthS5te4zaurNN x-ratelimit-reset-tokens: 16ms x-ratelimit-reset-requests: 120ms X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2515 X-Kong-Upstream-Latency: 4996 X-Kong-Proxy-Latency: 2222 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: b5fadb69431d1234d8bbd53a71abc559 { \"id\": \"chatcmpl-C3kbbMudkoKV6rrzvOHz0lQ8IH0Ci\", \"object\": \"chat.completion\", \"created\": 1755010171, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Joseph Conrad** (born Józef Teodor Konrad Korzeniowski; 1857–1924) was a Polish-British writer widely regarded as one of the great novelists writing in English, despite the fact that English was not his first language. He was born in Berdychiv, in the Russian Empire (now in Ukraine), to Polish parents.\\n\\n**Background:**\\n- **Early Life:** Conrad’s parents were exiled for their involvement in Polish independence movements. Orphaned at a young age, he spent much of his youth in Poland and later France.\\n- **Seafaring Career:** In his twenties, Conrad became a merchant marine, traveling around the world and eventually settling in England. He gained British citizenship in 1886.\\n\\n**Literary Career:**\\n- He began writing novels and short stories in English, starting with *Almayer’s Folly* (1895).\\n- **Notable works** include:\\n - *Heart of Darkness* (1899)\\n - *Lord Jim* (1900)\\n - *Nostromo* (1904)\\n - *The Secret Agent* (1907)\\n- His novels often deal with themes of isolation, existential doubt, imperialism, and the complexity of human nature.\\n\\n**Legacy:**\\n- Conrad’s innovative narrative techniques and psychological depth influenced modernist literature and writers such as Virginia Woolf, T.S. Eliot, and William Faulkner.\\n- *Heart of Darkness*, a novella about a journey into the Congo, is considered one of the most important works of 20th-century literature and has inspired many adaptations, including the film *Apocalypse Now*.\\n\\n**Summary:** \\nJoseph Conrad was a Polish-born novelist who wrote in English and became one of the leading literary figures of his time, celebrated for his adventure tales, deep psychological insight, and exploration of moral ambiguity.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 12, \"completion_tokens\": 383, \"total_tokens\": 395, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_799e4ca3f1\" } Check Redis again Redis database has two entries now:\nkubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan Expected response \"kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:2351ee4c78c607bf3c6123e98680647d3601e1b054b783cb589e05cf3d163e36\" \"kong_semantic_cache:c6dbe643-42af-421a-a094-de7735ebff12:openai-gpt-4.1:42aa94b4bbbedce497e59e1fd0fc617683a43b58ac7e306a47feb46f502f1499\" Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.",
    "description": "Semantic caching enhances data retrieval efficiency by focusing on the meaning or context of queries rather than just exact matches. It stores responses based on the underlying intent and semantic similarities between different queries and can then retrieve those cached queries when a similar request is made.\nWhen a new request is made, the system can retrieve and reuse previously cached responses if they are contextually relevant, even if the phrasing is different. This method reduces redundant processing, speeds up response times, and ensures that answers are more relevant to the user’s intent, ultimately improving overall system performance and user experience.",
    "tags": [],
    "title": "AI Semantic Cache",
    "uri": "/16-ai-gateway/17-use-cases/156-semantic-cache/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "Kong can rate-limit your traffic without any external dependency. In such a case, Kong stores the request counters in-memory and each Kong node applies the rate-limiting policy independently. There is no synchronization of information being done in this case. But if Redis is available in your cluster, Kong can take advantage of it and synchronize the rate-limit information across multiple Kong nodes and enforce a slightly different rate-limiting policy.\nThis section walks through the steps of using Redis for rate-limiting in a multi-node Kong deployment.\nHigh Level Tasks You will complete the following:\nSet up rate-limiting plugin Scale Kong for Kubernetes to multiple pods Verify rate-limiting across cluster You can now click Next to proceed further.",
    "description": "Kong can rate-limit your traffic without any external dependency. In such a case, Kong stores the request counters in-memory and each Kong node applies the rate-limiting policy independently. There is no synchronization of information being done in this case. But if Redis is available in your cluster, Kong can take advantage of it and synchronize the rate-limit information across multiple Kong nodes and enforce a slightly different rate-limiting policy.\nThis section walks through the steps of using Redis for rate-limiting in a multi-node Kong deployment.",
    "tags": [],
    "title": "Rate Limiting Using Redis",
    "uri": "/12-api-gateway/15-use-cases/156-rate-limiting-using-redis/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "In this section, you will configure the Key-Auth plugin on the Kong Route to protect Amazon Bedrock.\nAdd Kong Key Authentication plugin and Kong Consumer Add a KongPlugin resource for authentication, specifically the Key-Auth plugin. Note that, besides describing the plugin configuration, the declaration also creates a Kong Consumer, named user1, with an API Key (123456) as its credential.\ncat \u003e ai-key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth-bedrock enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-key-auth.yaml Verify authentication is required New requests now require authentication\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expect response The response is a HTTP/1.1 401 Unauthorized, meaning the Kong Gateway Service requires authentication.\nHTTP/1.1 401 Unauthorized Date: Tue, 12 Aug 2025 14:53:42 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Key Content-Length: 96 X-Kong-Response-Latency: 1 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: fd1bc16647271a20b7245b0cc9eb5052 { \"message\":\"No API key found in request\", \"request_id\":\"fd1bc16647271a20b7245b0cc9eb5052\" } Send another request with an API key Use the apikey to pass authentication to access the services.\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' The request should now respond with a HTTP/1.1 200 OK.\nWhen submitting requests, the API Key name is defined, by default, apikey. You can change the plugin configuration, if you will.",
    "description": "In this section, you will configure the Key-Auth plugin on the Kong Route to protect Amazon Bedrock.\nAdd Kong Key Authentication plugin and Kong Consumer Add a KongPlugin resource for authentication, specifically the Key-Auth plugin. Note that, besides describing the plugin configuration, the declaration also creates a Kong Consumer, named user1, with an API Key (123456) as its credential.\ncat \u003e ai-key-auth.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth-bedrock enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 EOF Apply the declaration with decK:",
    "tags": [],
    "title": "Key Auth",
    "uri": "/16-ai-gateway/17-use-cases/157-apikey/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "For advanced and enterprise class requirements, OpenID Connect (OIDC) is the preferred option to implement API consumer Authentication. OIDC also provides mechanisms to implement Authorization. In fact, when applying OIDC to secure the APIs, we’re delegating the Authentication process to the external Identity Provider entity.\nOpenID Connect is an authentication protocol built on top of OAuth 2.0 and JWT - JSON Web Token to add login and profile information about the identity who is logged in.\nOAuth 2.0 defines grant types for different use cases. The most common ones are:\nAuthorization Code: for apps running on a web server, browser-based and mobile apps for user authentication. Client Credentials: for application authentication. PKCE - Proof Key for Code Exchange: an extension to the Authorization Code grant. Recommended for SPA or native applications, PKCE acts like a non hard-coded secret. OpenId Connect plugin Konnect provides an OIDC plugin that fully supports the OAuth 2.0 grants. The plugin allows the integration with a 3rd party identity provider (IdP) in a standardized way. This plugin can be used to implement Kong as a (proxying) OAuth 2.0 resource server (RS) and/or as an OpenID Connect relying party (RP) between the client, and the upstream service.\nAs an example, here’s the typical topology and the Authorization Code with PKCE grant:\nConsumer sends a request to Kong Data Plane. Since the API is being protected with the OIDC plugin, the Data Plane redirects the consumer to the IdP. Consumer provides credentials to the Identity Provide (IdP). IdP authenticates the consumer enforcing security policies previously defined. The policies might involve several database technologies (e.g. LDAP, etc.), MFA (Multi-Factor Authentication), etc. After user authentication, IdP redirects the consumer back to the Data Plane with the Authorization Code injected inside the request. Data Plane sends a request to the IdP’s token endpoint with the Authorization Code and gets an Access Token from the IdP. Data Plane routes the request to the upstream service along with the Access Token Once again, it’s important to notice that one of the main benefits provided by an architecture like this is to follow the Separation of Concerns principle:\nIdentity Provider: responsible for User and Application Authentication, Tokenization, MFA, multiples User Databases abstraction, etc. API Gateway: responsible for exposing the Upstream Services and controlling their consumption through an extensive list of policies besides Authentication including Rate Limiting, Caching, Log Processing, etc. In this module, we will configure this plugin to use Keycloak.\nYou can now click Next to proceed further.",
    "description": "For advanced and enterprise class requirements, OpenID Connect (OIDC) is the preferred option to implement API consumer Authentication. OIDC also provides mechanisms to implement Authorization. In fact, when applying OIDC to secure the APIs, we’re delegating the Authentication process to the external Identity Provider entity.\nOpenID Connect is an authentication protocol built on top of OAuth 2.0 and JWT - JSON Web Token to add login and profile information about the identity who is logged in.",
    "tags": [],
    "title": "OpenID Connect",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "With the existing API Key policy, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIn this section, we are going to define specific Rate Limiting policies for each Consumer represented by its API Key.\nKong Consumer Policies It’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:\nconsumer1: apikey = 123456 rate limiting policy = 500 tokens per minute consumer2: apikey = 987654 rate limiting policy = 10000 tokens per minute Doing that, the Data Plane is capable to not just protect the Route but to identify the consumer based on the key injected to enforce specific policies to the consumer. Keep in mind that a Consumer might have other plugins also enabled such as TCP Log, etc.\nNew Consumer and AI Rate Limiting Advanced plugin Policies Then, create the second consumer2, just like you did with the first one, with the 987654 key. Both Kong Consumers have the AI Rate Limiting Advanced plugin enabled with specific configurations.\ncat \u003e ai-key-auth-rate-limiting-advanced.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - llm services: - name: service1 host: localhost port: 32000 routes: - name: ollama-route paths: - /ollama-route plugins: - name: ai-proxy instance_name: ai-proxy-ollama config: route_type: llm/v1/chat model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat - name: openai-route paths: - /openai-route plugins: - name: ai-proxy instance_name: ai-proxy-openai config: route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-4.1 options: temperature: 1.0 - name: key-auth instance_name: key-auth-bedrock enabled: true consumers: - keyauth_credentials: - key: \"123456\" username: user1 plugins: - name: ai-rate-limiting-advanced instance_name: ai-rate-limiting-advanced-consumer1 config: llm_providers: - name: openai window_size: - 60 limit: - 500 - keyauth_credentials: - key: \"987654\" username: user2 plugins: - name: ai-rate-limiting-advanced instance_name: ai-rate-limiting-advanced-consumer2 config: llm_providers: - name: openai window_size: - 60 limit: - 10000 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-key-auth-rate-limiting-advanced.yaml Use both Kong Consumers If you will, you can inject both keys to your requests.\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected output HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 500 X-AI-RateLimit-Remaining-minute-openai: 500 openai-version: 2020-10-01 x-envoy-upstream-service-time: 8059 Date: Tue, 12 Aug 2025 15:26:01 GMT x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 CF-RAY: 96e0fce49c204ee9-GRU x-ratelimit-remaining-tokens: 29993 alt-svc: h3=\":443\"; ma=86400 access-control-expose-headers: X-Request-ID X-Content-Type-Options: nosniff Server: cloudflare openai-organization: user-4qzstwunaw6d1dhwnga5bc5q Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-request-id: req_f230ff10e7374c7a8b2fc292a3cc0685 openai-processing-ms: 7964 cf-cache-status: DYNAMIC openai-project: proj_r4KYFyenuGWthS5te4zaurNN x-ratelimit-reset-tokens: 14ms x-ratelimit-reset-requests: 120ms X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2058 X-Kong-Upstream-Latency: 8798 X-Kong-Proxy-Latency: 4 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 379bd531f3411379f613083969a88c07 { \"id\": \"chatcmpl-C3lAneT4SCgIa50fe89CYPadtBdfQ\", \"object\": \"chat.completion\", \"created\": 1755012353, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Jimi Hendrix (born **James Marshall Hendrix**, November 27, 1942 – September 18, 1970) was an American guitarist, singer, and songwriter. He is widely regarded as one of the greatest and most influential electric guitarists in the history of popular music.\\n\\n**Career Highlights:**\\n- Hendrix gained fame in the late 1960s with his band, **The Jimi Hendrix Experience**.\\n- Some of his most famous songs include **\\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"The Wind Cries Mary,\\\"** and **\\\"All Along the Watchtower.\\\"**\\n- He was known for his innovative guitar techniques, including feedback, distortion, and wah-wah, and his energetic and theatrical performance style.\\n- Hendrix's legendary performances include his rendition of \\\"The Star-Spangled Banner\\\" at **Woodstock** in 1969.\\n\\n**Legacy:**\\n- Despite his short career (he died at age 27), Hendrix's music and style had a major impact on rock, blues, and modern guitar playing.\\n- He was posthumously inducted into the **Rock and Roll Hall of Fame** in 1992.\\n- Hendrix is consistently ranked among the greatest guitarists of all time by music publications and critics.\\n\\n**Fun Fact:** \\nJimi Hendrix is part of the so-called \\\"27 Club,\\\" a group of influential musicians who died at the age of 27.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 285, \"total_tokens\": 298, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_b3f1157249\" } or\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 987654' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' Expected output HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 10000 X-AI-RateLimit-Remaining-minute-openai: 10000 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 Date: Tue, 12 Aug 2025 15:26:41 GMT x-ratelimit-remaining-tokens: 29993 access-control-expose-headers: X-Request-ID openai-organization: user-4qzstwunaw6d1dhwnga5bc5q openai-processing-ms: 6376 x-ratelimit-reset-requests: 120ms openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-cache-status: DYNAMIC openai-version: 2020-10-01 Server: cloudflare X-Content-Type-Options: nosniff x-envoy-upstream-service-time: 6463 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload CF-RAY: 96e0fdeec8b7ae57-GRU x-ratelimit-limit-requests: 500 x-request-id: req_f8c4559cbdac4d1bbff938b61a054f3d x-ratelimit-reset-tokens: 14ms alt-svc: h3=\":443\"; ma=86400 X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 2324 X-Kong-Upstream-Latency: 6709 X-Kong-Proxy-Latency: 3 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 6e2daa0a80e95772fa46e15637278177 { \"id\": \"chatcmpl-C3lBT2cDacuuICQzugJ5CCMbWtCFV\", \"object\": \"chat.completion\", \"created\": 1755012395, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (born **James Marshall Hendrix**, November 27, 1942 – September 18, 1970) was an American guitarist, singer, and songwriter. Widely regarded as one of the greatest and most influential electric guitarists in the history of popular music, Hendrix is known for his innovative playing style, including groundbreaking use of guitar effects and techniques like feedback and distortion.\\n\\n### Brief Biography:\\n- **Early Life:** Born in Seattle, Washington; started playing guitar as a teenager.\\n- **Career:** Gained prominence in the mid-1960s after moving to England and forming the **Jimi Hendrix Experience** with bassist Noel Redding and drummer Mitch Mitchell.\\n- **Famous Albums:** \\n - *Are You Experienced* (1967)\\n - *Axis: Bold as Love* (1967)\\n - *Electric Ladyland* (1968)\\n- **Iconic Performances:** \\n - Monterey Pop Festival (1967)\\n - Woodstock Festival (1969), where he famously reinterpreted “The Star-Spangled Banner.”\\n\\n### Legacy:\\nHendrix’s innovative approach fused rock, blues, and psychedelia. His use of the wah-wah pedal, feedback, and studio effects transformed notions of what the electric guitar could do. Despite his death at the age of 27, his influence persists across genres and generations.\\n\\n### Key Songs:\\n- “Purple Haze”\\n- “Hey Joe”\\n- “All Along the Watchtower”\\n- “Voodoo Child (Slight Return)”\\n- “Little Wing”\\n\\nHendrix is a major figure in rock history and is frequently cited in “greatest guitarists of all time” lists.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 346, \"total_tokens\": 359, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_799e4ca3f1\" } Again, test the rate-limiting policy by executing the following command multiple times and observe the rate-limit headers in the response, specially, X-AI-RateLimit-Limit-minute-openai and X-AI-RateLimit-Remaining-minute-openai:\nNow, let’s consume it with the Consumer1’s API Key. As you can see the Data Plane is processing the Rate Limiting processes independently.\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 500 X-AI-RateLimit-Remaining-minute-openai: 110 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 Date: Tue, 12 Aug 2025 15:29:05 GMT x-ratelimit-remaining-tokens: 29993 access-control-expose-headers: X-Request-ID openai-organization: user-4qzstwunaw6d1dhwnga5bc5q openai-processing-ms: 4385 x-ratelimit-reset-requests: 120ms openai-project: proj_r4KYFyenuGWthS5te4zaurNN cf-cache-status: DYNAMIC openai-version: 2020-10-01 Server: cloudflare X-Content-Type-Options: nosniff x-envoy-upstream-service-time: 4484 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload CF-RAY: 96e10178faf000f1-GRU x-ratelimit-limit-requests: 500 x-request-id: req_b4611580cac4476288895c6315847b8b x-ratelimit-reset-tokens: 14ms alt-svc: h3=\":443\"; ma=86400 X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 1796 X-Kong-Upstream-Latency: 4881 X-Kong-Proxy-Latency: 1 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 8b4285448ad3b335f766d33c61a37851 If we keep sending requests using the first API Key, eventually, as expected, we’ll get an error code:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 123456' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/1.1 429 Too Many Requests Date: Tue, 12 Aug 2025 15:29:52 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 500 X-AI-RateLimit-Remaining-minute-openai: 0 X-AI-RateLimit-Retry-After-minute-openai: 9 X-AI-RateLimit-Reset: 9 X-AI-RateLimit-Retry-After: 9 X-AI-RateLimit-Reset-minute-openai: 9 Content-Length: 66 X-Kong-Response-Latency: 1 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: c3a2a05b8f77b264230f041c527ade71 {\"message\":\"AI token rate limit exceeded for provider(s): openai\"} However, the second API Key is still allowed to consume the Kong Route:\ncurl -i -X POST \\ --url $DATA_PLANE_LB/openai-route \\ --header 'Content-Type: application/json' \\ --header 'apikey: 987654' \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"Who is Jimi Hendrix?\" } ] }' HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-openai: 10000 X-AI-RateLimit-Remaining-minute-openai: 10000 openai-version: 2020-10-01 x-envoy-upstream-service-time: 5053 Date: Tue, 12 Aug 2025 15:30:26 GMT x-ratelimit-limit-requests: 500 x-ratelimit-limit-tokens: 30000 x-ratelimit-remaining-requests: 499 CF-RAY: 96e103739ca64292-VCP x-ratelimit-remaining-tokens: 29993 alt-svc: h3=\":443\"; ma=86400 access-control-expose-headers: X-Request-ID X-Content-Type-Options: nosniff Server: cloudflare openai-organization: user-4qzstwunaw6d1dhwnga5bc5q Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-request-id: req_379758f4fcd549acb57c7e6d911b5a89 openai-processing-ms: 5014 cf-cache-status: DYNAMIC openai-project: proj_r4KYFyenuGWthS5te4zaurNN x-ratelimit-reset-tokens: 14ms x-ratelimit-reset-requests: 120ms X-Kong-LLM-Model: openai/gpt-4.1 Content-Length: 1998 X-Kong-Upstream-Latency: 5602 X-Kong-Proxy-Latency: 2 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 118caf09b62c8f4f78771c182f736b00 { \"id\": \"chatcmpl-C3lF7AC6zeB8NOBFQn5Xeacn6Ntf7\", \"object\": \"chat.completion\", \"created\": 1755012621, \"model\": \"gpt-4.1-2025-04-14\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"**Jimi Hendrix** (born James Marshall Hendrix on November 27, 1942 – September 18, 1970) was an iconic American guitarist, singer, and songwriter. Widely regarded as one of the most influential electric guitarists in the history of popular music, Hendrix is celebrated for his innovative style, virtuosic playing, and groundbreaking use of guitar effects such as distortion, feedback, and wah-wah pedals.\\n\\nHendrix first gained fame in the mid-1960s after moving to London and forming the **Jimi Hendrix Experience** with bassist Noel Redding and drummer Mitch Mitchell. The band released classic albums such as *Are You Experienced* (1967), *Axis: Bold as Love* (1967), and *Electric Ladyland* (1968), featuring hit songs like \\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"The Wind Cries Mary,\\\" \\\"Voodoo Child (Slight Return),\\\" and his legendary rendition of \\\"The Star-Spangled Banner\\\" performed at Woodstock in 1969.\\n\\nTragically, Hendrix died at the young age of 27. Despite his short career, his influence continues to shape rock, blues, and popular music to this day. He is consistently ranked among the greatest guitarists of all time and has been inducted into the Rock and Roll Hall of Fame.\", \"refusal\": null, \"annotations\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 13, \"completion_tokens\": 271, \"total_tokens\": 284, \"prompt_tokens_details\": { \"cached_tokens\": 0, \"audio_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0, \"audio_tokens\": 0, \"accepted_prediction_tokens\": 0, \"rejected_prediction_tokens\": 0 } }, \"service_tier\": \"default\", \"system_fingerprint\": \"fp_51e1070cf2\" } Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.",
    "description": "With the existing API Key policy, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIn this section, we are going to define specific Rate Limiting policies for each Consumer represented by its API Key.\nKong Consumer Policies It’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:",
    "tags": [],
    "title": "AI Rate Limiting Advanced",
    "uri": "/16-ai-gateway/17-use-cases/158-rate-limiting/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "OAuth and OpenID Connect are great and recommended options to implement not just Authentication and Authorization processes. However, there are some use cases where the Authorization policies require a bit of business logic. For example, let’s say we want to prevent our API Consumers from consuming applications, protected by the Gateway, during weekends. In cases like this, one nice possibility is to have a specific layer taking care of the Authorization policies. That’s the main purpose of the Open Policy Agent - OPA engine.\nIn fact, such a decision is simply applying the same Separation of Concerns principle to get two independent layers implementing, each one of them, the Authentication and Authorization policies. Our architecture topology would look slightly different now.\nOn the other hand, as we stated in the beginning of the chapter, it is not the case to remove the Authorization policies from the OAuth/OIDC layer. There will be different abstraction levels for the policies: some of them, possibly coarse-grained enterprise class ones, should still be implemented by the OAuth/OIDC layer. Fine-grained policies, instead, would be better implemented by the specific Authorization layer.\nOPA Installation Create another namespace, this time to install OPA\nkubectl create namespace opa OPA can be installed with this simple declaration. Note it’s going to be exposed with a new Load Balancer:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: opa namespace: opa labels: app: opa spec: replicas: 1 selector: matchLabels: app: opa template: metadata: labels: app: opa spec: containers: - name: opa image: openpolicyagent/opa:edge-static volumeMounts: - readOnly: true mountPath: /policy name: opa-policy args: - \"run\" - \"--server\" - \"--addr=0.0.0.0:8181\" - \"--set=decision_logs.console=true\" - \"--set=status.console=true\" - \"--ignore=.*\" volumes: - name: opa-policy --- apiVersion: v1 kind: Service metadata: name: opa namespace: opa spec: selector: app: opa type: LoadBalancer ports: - name: http protocol: TCP port: 8181 targetPort: 8181 EOF Check the installation\n% kubectl get service -n opa NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE opa LoadBalancer 10.99.181.108 127.0.0.1 8181:32674/TCP 14s % kubectl get pod -n opa NAME READY STATUS RESTARTS AGE opa-b9fb4959c-mh6v6 1/1 Running 0 45s Check if OPA is running properly:\ncurl -i -X GET http://localhost:8181/health Expected output\nHTTP/1.1 200 OK Content-Type: application/json Date: Mon, 11 Aug 2025 21:07:45 GMT Content-Length: 3 {} If you want to delete it:\nkubectl delete service opa -n opa kubectl delete deployment opa -n opa As expected, there no policies available:\ncurl -s -X GET http://localhost:8181/v1/policies {\"result\":[]} Create the Authorization Policy OPA uses Rego language for Policy definition. Here’s the policy we are going to create:\ncat \u003e jwt.rego \u003c\u003c 'EOF' package jwt import rego.v1 default allow := false allow if { check_cid check_working_day } check_cid if { v := input.request.http.headers.authorization startswith(v, \"Bearer\") bearer_token := substring(v, count(\"Bearer \"), -1) [_, token, _] := io.jwt.decode(bearer_token) token.aud == \"silver\" } check_working_day if { wday := time.weekday(time.now_ns()) wday != \"Saturday\"; wday != \"Sunday\" } EOF The simple policy checks two main conditions:\nIf the Access Token issued by Keycloak, validated and mapped by Kong Data Plane, has a specific audience. To try the policy we are requesting the audience to be a different one. Only requests sent during working days should be allowed. Create the jwt.rego file and apply the policy sending a request to OPA:\ncurl -XPUT http://localhost:8181/v1/policies/jwt --data-binary @jwt.rego Check the policy with:\ncurl -s -X GET http://localhost:8181/v1/policies | jq -r '.result[].id' curl -s -X GET http://localhost:8181/v1/policies/jwt | jq -r '.result.raw' Enable the OPA plugin to the Kong Route Just like we did for the other plugins, we can enable the OPA plugin with a request like this. Note the opa_path parameter refers to the allow function defined in the policy. The opa_host and opa_port are references to the OPA Kubernetes Service’s FQDN.\nSince we are going to move the Authorization policy to OPA, we are also returning our OpenID Connect plugin to the original Client Credentials state, with no audience_required configuration:\ncat \u003e oidc.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: oidc-route paths: - /oidc-route plugins: - name: openid-connect instance_name: openid-connect1 config: auth_methods: [\"client_credentials\"] issuer: http://keycloak.keycloak:8080/realms/kong token_endpoint: http://keycloak.keycloak:8080/realms/kong/protocol/openid-connect/token extra_jwks_uris: [\"http://keycloak.keycloak.svc.cluster.local:8080/realms/kong/protocol/openid-connect/certs\"] consumer_optional: false consumer_claim: [\"client_id\"] consumer_by: [\"username\"] - name: opa instance_name: opa1 config: opa_path: \"/v1/data/jwt/allow\" opa_protocol: http opa_host: \"opa.opa.svc.cluster.local\" opa_port: 8181 consumers: - username: kong_id EOF Submit the declaration deck gateway sync --konnect-token $PAT oidc.yaml Consume the Kong Route A new error code should be returned if we try to consume the Route:\ncurl -siX GET http://localhost/oidc-route/get -u \"kong_id:RVXO9SOJskjw4LeVupjRbIMJIAyyil8j\" HTTP/1.1 403 Forbidden Date: Sat, 03 Aug 2024 22:02:37 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive Content-Length: 26 X-Kong-Response-Latency: 4 Server: kong/3.7.1.2-enterprise-edition X-Kong-Request-Id: fa699046f7d21773b626a4311537e171 {\"message\":\"unauthorized\"} This is due the audience required by OPA is different to the existing one defined in our Keycloak Client. Go to Keycloak kong_mapper Client Scope Mapper and change the Included Custom Audience to silver.\nAssuming you are on a working day, OPA should allow you to consume the Route again.\ncurl -sX GET http://localhost/oidc-route/get -u \"kong_id:RVXO9SOJskjw4LeVupjRbIMJIAyyil8j\"| jq -r '.headers.Authorization' | cut -d \" \" -f 2 | jwt decode - Token header ------------ { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"JIao4TIXpSwJxcukz6W0hK8qc_vuYf6HrmGsDmT6kzY\" } Token claims ------------ { \"acr\": \"1\", \"allowed-origins\": [ \"/*\" ], \"aud\": \"silver\", \"azp\": \"kong_id\", \"clientAddress\": \"10.244.0.106\", \"clientHost\": \"10.244.0.106\", \"client_id\": \"kong_id\", \"email_verified\": false, \"exp\": 1754949643, \"iat\": 1754949343, \"iss\": \"http://keycloak.keycloak:8080/realms/kong\", \"jti\": \"trrtcc:a4d77414-c9a7-f5c0-daea-0532b8960b4e\", \"preferred_username\": \"service-account-kong_id\", \"scope\": \"openid email profile\", \"sub\": \"e7b5a37b-d06a-4b40-92d7-36f09768ed79\", \"typ\": \"Bearer\" } Kong-gratulations! have now reached the end of this module by authenticating your API requests with AWS Cognito. You can now click Next to proceed with the next module.",
    "description": "OAuth and OpenID Connect are great and recommended options to implement not just Authentication and Authorization processes. However, there are some use cases where the Authorization policies require a bit of business logic. For example, let’s say we want to prevent our API Consumers from consuming applications, protected by the Gateway, during weekends. In cases like this, one nice possibility is to have a specific layer taking care of the Authorization policies. That’s the main purpose of the Open Policy Agent - OPA engine.",
    "tags": [],
    "title": "OPA (Open Policy Agent)",
    "uri": "/12-api-gateway/15-use-cases/158-opa-authorization/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.\nThe plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nLoad balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:\nLowest-usage Round-robin (weighted) Consistent-hashing (sticky-session on given header value) Semantic routing The AI Proxy Advanced plugin supports semantic routing, which enables distribution of requests based on the similarity between the prompt and the description of each model. This allows Kong to automatically select the model that is best suited for the given domain or use case.\nBy analyzing the content of the request, the plugin can match it to the most appropriate model that is known to perform better in similar contexts. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.\nAs a illustration here is the architecture where we are going to implement the multiple load balancing policies. AI Proxy Advanced will manage both LLMs:\ngpt-4.1 llama3.2:1b You can now click Next to proceed further.",
    "description": "The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.\nThe plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nLoad balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:",
    "tags": [],
    "title": "AI Proxy Advanced",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway",
    "content": "With the rapid emergence of multiple AI LLM providers, the AI technology landscape is fragmented and lacking in standards and controls. Kong AI Gateway is a powerful set of features built on top of Kong Gateway, designed to help developers and organizations effectively adopt AI capabilities quickly and securely\nWhile AI providers don’t conform to a standard API specification, the Kong AI Gateway provides a normalized API layer allowing clients to consume multiple AI services from the same client code base. The AI Gateway provides additional capabilities for credential management, AI usage observability, governance, and tuning through prompt engineering. Developers can use no-code AI Plugins to enrich existing API traffic, easily enhancing their existing application functionality.\nYou can enable the AI Gateway features through a set of specialized plugins, using the same model you use for any other Kong Gateway plugin.\nKong AI Gateway functional scope Universal API Kong’s AI Gateway Universal API, delivered through the AI Proxy and AI Proxy Advanced plugins, simplifies AI model integration by providing a single, standardized interface for interacting with models across multiple providers.\nEasy to use: Configure once and access any AI model with minimal integration effort.\nLoad balancing: Automatically distribute AI requests across multiple models or providers for optimal performance and cost efficiency.\nRetry and fallback: Optimize AI requests based on model performance, cost, or other factors.\nCross-plugin integration: Leverage AI in non-AI API workflows through other Kong Gateway plugins.\nHigh Level Tasks You will complete the following:\nSet up Kong AI Proxy for LLM Integration Implement Kong AI Plugins to secure prompt message You can now click Next to proceed further.",
    "description": "With the rapid emergence of multiple AI LLM providers, the AI technology landscape is fragmented and lacking in standards and controls. Kong AI Gateway is a powerful set of features built on top of Kong Gateway, designed to help developers and organizations effectively adopt AI capabilities quickly and securely\nWhile AI providers don’t conform to a standard API specification, the Kong AI Gateway provides a normalized API layer allowing clients to consume multiple AI services from the same client code base. The AI Gateway provides additional capabilities for credential management, AI usage observability, governance, and tuning through prompt engineering. Developers can use no-code AI Plugins to enrich existing API traffic, easily enhancing their existing application functionality.",
    "tags": [],
    "title": "Introduction",
    "uri": "/16-ai-gateway/159-ai-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "In this second part of the workshop we are going to explore the AI capabilities provides by Kong AI Gateway and the specific collection of plugins.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Kong AI Gateway",
    "description": "In this second part of the workshop we are going to explore the AI capabilities provides by Kong AI Gateway and the specific collection of plugins.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Kong AI Gateway",
    "tags": [],
    "title": "Kong AI Gateway",
    "uri": "/16-ai-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "What is RAG? RAG offers a solution to some of the limitations in traditional generative AI models, such as accuracy and hallucinations, allowing companies to create more contextually relevant AI applications.\nBasically, the RAG application comprises two main processes:\nData Preparation: External data sources, including all sorts of documents, are chunked and submitted to an Embedding Model which converts them into embeddings. The embeddings are stored in a Vector Database.\nQuery time: A consumer wants to send a query to the actual Prompt/Chat LLM model. However, the query should be enhanced with relevant data, taken from the Vector Database, and not available in the LLM model. The following steps are then performed:\nThe consumer builds a Prompt. The RAG application converts the Prompt into an embedding, calling the Embedding Model. Leveraging Semantic Search, RAG matches the Prompt Embedding with the most relevant information and retrieves that Vector Database. The Vector Database returns relevant data as a response to the search query. The RAG application sends a query to the Prompt/Chat LLM Model combining the Prompt with the Relevant Data returned by the Vector Database. The LLM Model returns a response. Implementation Architecture Data Preparation time During the preparation time, the following steps are executed:\nThe Document Loader script asks the AI RAG Injector plugin to provides the configuration regarding both Embedding Model and Vector Database. The Document Loader sends data chunks to the Embedding Model to gets the embeddings related to them The Document Loader stores the embeddings and content into the Vector Database. RAG time The following steps are performed during the execution time:\nThe API Consumer send a request with a prompt. The AI RAG Injector Plugin converts the prompt into embeddings calling the Embedding Model. The AI RAG Injector Plugin sends a KNN vector search query to the Vector Database to find the top “k-nearest neighbors” to a query vector. The AI Proxy Advanced Plugin sends a regular request to the LLM model adding the relevant data received from the Vector Database. Here’s the declaration including both plugins: AI RAG Injector and AI Proxy Advanced.\ncat \u003e ai-rag-injector.yaml \u003c\u003c 'EOF' _format_version: '3.0' _konnect: control_plane_name: kong-workshop services: - name: ai-proxy url: http://localhost:65535 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - logging: log_statistics: true route_type: llm/v1/chat auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: gpt-5 options: temperature: 1.0 - name: ai-rag-injector id: 3194f12e-60c9-4cb6-9cbc-c8fd7a00cff1 instance_name: ai-rag-injector1 config: inject_template: | Only use the following information surrounded by \u003cRAG\u003e\u003c/RAG\u003e to and your existing knowledge to provide the best possible answer to the user. \u003cRAG\u003e\u003cCONTEXT\u003e\u003c/RAG\u003e User's question: \u003cPROMPT\u003e fetch_chunks_count: 1 embeddings: auth: header_name: Authorization header_value: Bearer ${{ env \"DECK_OPENAI_API_KEY\" }} model: provider: openai name: \"text-embedding-3-small\" vectordb: strategy: redis redis: host: redis-stack.redis port: 6379 distance_metric: cosine dimensions: 1024 EOF Apply the declaration with decK:\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT ai-rag-injector.yaml If you send a request with no context, we’ll see the LLM is not able to respond to it:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header \"Content-Type: application/json\" \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"What did Marco say about AI Gateways?\" } ] }' | jq '.choices[].message.content' Typical response: \"Could you clarify which Marco you mean and where he said it (e.g., a podcast, post, talk, or email)? If you can share a link, quote, or more context, I can find and summarize exactly what he said about AI Gateways.\" We are going to inject some context using an interview transcript snippet Marco Palladino, Kong’s co-founder and CTO, gave some time ago.\nThe transcript snippet is available in the following file.\ncurl 'https://raw.githubusercontent.com/Kong/workshop-kong-api-ai-gateway/refs/heads/main/content/static/code/SED1683-Kong-short.txt' --output ./SED1683-Kong-short.txt We have to copy the Document Loader script, algo available in a file, to the Data Plane:\ncurl 'https://raw.githubusercontent.com/Kong/workshop-kong-api-ai-gateway/refs/heads/main/content/static/code/ingest_update.lua' --output ./ingest_update.lua kubectl cp ./ingest_update.lua -n kong $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name'):/tmp/ingest_update.lua Now, execute the Document Loader passing the content as a parameter. Note that, to make to process a bit easier, we have created the AI RAG Injector plugin with a pre-defined id.\nkubectl exec -ti $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name') -n kong -- kong runner /tmp/ingest_update.lua 3194f12e-60c9-4cb6-9cbc-c8fd7a00cff1 \"'\"$(cat ./SED1683-Kong-short.txt)\"'\" You should be able to get a much better response from the LLM model this time:\ncurl -s -X POST \\ --url $DATA_PLANE_LB/route1 \\ --header \"Content-Type: application/json\" \\ --data '{ \"messages\": [ { \"role\": \"user\", \"content\": \"What did Marco say about AI Gateways?\" } ] }' | jq '.choices[].message.content' \"Marco said that for AI use cases you need an AI gateway to connect multiple LLMs and orchestrate them. He added that Kong can deploy its gateway in this AI gateway role (alongside edge gateway and service mesh) under a unified control plane, so you can see, manage, monitor, consume, and expose APIs consistently across these use cases.\" If you Redis, you’ll se there a new entry for RAG Injector\n% kubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan \"kong_rag_injector:7e9d1404-5cfb-4585-bada-132e6d6595c1\"",
    "description": "What is RAG? RAG offers a solution to some of the limitations in traditional generative AI models, such as accuracy and hallucinations, allowing companies to create more contextually relevant AI applications.\nBasically, the RAG application comprises two main processes:\nData Preparation: External data sources, including all sorts of documents, are chunked and submitted to an Embedding Model which converts them into embeddings. The embeddings are stored in a Vector Database.",
    "tags": [],
    "title": "RAG - Retrieval-Augmented Generation",
    "uri": "/16-ai-gateway/17-use-cases/170-rag/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway",
    "content": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nSimple AI Gateway Proxy Prompt Engineering AI Request and Response Tranformers AI Semantic Cache AI Rate Limiting AI Proxy Advanced and load balancing algoritms RAG These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong AI plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "description": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nSimple AI Gateway Proxy Prompt Engineering AI Request and Response Tranformers AI Semantic Cache AI Rate Limiting AI Proxy Advanced and load balancing algoritms RAG These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong AI plugins on the Plugin Hub.",
    "tags": [],
    "title": "Use Cases",
    "uri": "/16-ai-gateway/17-use-cases/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "The third part of the workshop is dedicated to Observability, exploring:\nBuilt-in capabilities provided by Konnect Kong Data Plane plugins to integrate with external Observability infrastructures such as Dynatrace or Honeycomb.",
    "description": "The third part of the workshop is dedicated to Observability, exploring:\nBuilt-in capabilities provided by Konnect Kong Data Plane plugins to integrate with external Observability infrastructures such as Dynatrace or Honeycomb.",
    "tags": [],
    "title": "Observability",
    "uri": "/20-observability/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability",
    "content": "Basically Konnect provides two main Builtin Observability services:\nKonnect Advanced Analytics: it is a real-time, contextual analytics platform that provides insights into API health, performance, and usage.\nKonnect Debugger: it provides a connected debugging experience and real-time trace-level visibility into API traffic.",
    "description": "Basically Konnect provides two main Builtin Observability services:\nKonnect Advanced Analytics: it is a real-time, contextual analytics platform that provides insights into API health, performance, and usage.\nKonnect Debugger: it provides a connected debugging experience and real-time trace-level visibility into API traffic.",
    "tags": [],
    "title": "Konnect Builtin Observability",
    "uri": "/20-observability/21-builtin/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability \u003e Konnect Builtin Observability",
    "content": "Konnect Advanced Analytics is a real-time, highly contextual analytics platform that provides deep insights into API health, performance, and usage. It helps businesses optimize their API strategies and improve operational efficiency. This feature is offered as a premium service within Konnect.\nKey benefits:\nCentralized visibility: Gain insights across all APIs, Services, and data planes. Contextual API analytics: Analyze API requests, Routes, Consumers, and Services. Democratized data insights: Generate reports based on your needs. Fast time to insight: Retrieve critical API metrics in less than a second. Reduced cost of ownership: A turn-key analytics solution without third-party dependencies. Enabling data ingestion Manage data ingestion from any Control Plane Dashboard using the Advanced Analytics toggle. This toggle lets you enable or disable data collection for your API traffic per control plane.\nModes:\nOn: Both basic and advanced analytics data is collected, allowing in-depth insights and reporting. Off: Advanced analytics collection stops, but basic API metrics remain available in Gateway Manager, and can still be used for custom reports. Explorer Interface The Explorer interface displays API usage data gathered by Konnect Analytics from your Data Plane nodes. You can use this tool to:\nDiagnose performance issues Monitor LLM token consumption and costs Capture essential usage metrics The Analytics Explorer also lets you save the output as a custom report.\nCheck the Advanced Analytics Explorer documentation to learn more.\nDashboards The Summary Dashboard shows performance and health statistics of all your APIs across your organization on a single page and provides insights into your Service usage.\nCustom Dasboards Advanced Analytics includes the ability to build organization-specific views with Custom Dashboards. You can create them from scratch or use existing templates. The functionality is powered by a robust API, and Terraform integration.\nCreate a dashboard You can create custom dashboards either from scratch or from a template. In this tutorial, we’ll use a template.\nTo create a custom dashboard, do the following:\nIn Konnect, navigate to Dashboards in the sidebar. From the Create dashboard dropdown menu, select “Create from template”. Click Quick summary dashboard. Click Use template. This creates a new template with pre-configured tiles. Add a filter Filters help you narrow down the data shown in charts without modifying individual tiles.\nFor this example, let’s add a filter so that the data shown in the dashboard is scoped to only one control plane:\nFrom the dashboard, click Add filter. This brings up the configuration options. Select “Control plane” from the Filter by dropdown menu. Select “In” from the Operator dropdown menu. Select “kong-workshop from the Filter value dropdown menu. Select the Make this a preset for all viewers checkbox. Click Apply. This applies the filter to the dashboard. Anyone that views this dashboard will be viewing it scoped to the filter you created.\nCheck the Advanced Analytics Custom Dashboards documentation to learn more.\nRequests The Requests options shows all requests that have been processed by the Data Planes. For example, here’s the requests processed by the Data Planes created for the kong-workshop Control Plane.",
    "description": "Konnect Advanced Analytics is a real-time, highly contextual analytics platform that provides deep insights into API health, performance, and usage. It helps businesses optimize their API strategies and improve operational efficiency. This feature is offered as a premium service within Konnect.\nKey benefits:\nCentralized visibility: Gain insights across all APIs, Services, and data planes. Contextual API analytics: Analyze API requests, Routes, Consumers, and Services. Democratized data insights: Generate reports based on your needs. Fast time to insight: Retrieve critical API metrics in less than a second. Reduced cost of ownership: A turn-key analytics solution without third-party dependencies. Enabling data ingestion Manage data ingestion from any Control Plane Dashboard using the Advanced Analytics toggle. This toggle lets you enable or disable data collection for your API traffic per control plane.",
    "tags": [],
    "title": "Konnect Advanced Analytics",
    "uri": "/20-observability/21-builtin/211-advanced_analytics/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability \u003e Konnect Builtin Observability",
    "content": "Konnect Debugger provides a connected debugging experience and real-time trace-level visibility into API traffic, enabling you to:\nTroubleshoot issues: Investigate and resolve problems during deployments or incidents with targeted, on-demand traces. Understand request lifecycle: Visualize exactly what happened during a specific request, including order and duration plugin execution. See Debugger spans for a list of spans captured. Improve performance and reliability: Use deep insights to fine-tune configurations and resolve bottlenecks. Capture traces and logs Konnect Debugger allows you to capture traces and logs.\nTraces Traces provide a visual representation of the request and response lifecycle, offering a comprehensive overview of Kong’s request processing pipeline.\nThe debugger helps capture OpenTelemetry-compatible traces for all requests matching the sampling criteria. The detailed spans are captured for the entire request/response lifecycle. These traces can be visualized with Konnect’s built-in span viewer with no additional instrumentation or telemetry tools. For a complete list of available spans and their meanings, see Debugger spans.\nKey Highlights Traces can be generated for a service or per route Refined traces can be generated for all requests matching a sampling criteria Sampling criteria can be defined with simple expressions language, for example: http.method == GET Trace sessions are retained for up to 7 days Traces can be visualized in Konnect’s built in trace viewer To ensure consistency and interoperability, tracing adheres to OpenTelemetry naming conventions for spans and attributes, wherever possible.\nLogs For deeper insights, logs can be captured along with traces. When initiating a debug session, administrators can choose to capture logs. Detailed Kong Gateway logs are captured for the duration of the session. These logs are then correlated with traces using trace_id and span_id providing a comprehensive and drill-down view of logs generated during specific trace or span.\nReading traces and logs Traces captured during a debug session can be visualized in debugger’s built-in trace viewer. The trace viewer displays Summary, Spans and Logs view. You can gain instant insights with the summary view while the spans and logs view help you to dive deeper.\nSummary view Summary view helps you visualize the entire API request-response flow in a single glance. This view provides a concise overview of critical latency metrics and a transaction map. The lifecycle map includes the different phases of Kong Gateway and the plugins executed by Kong Gateway on both the request and the response along with the times spent in each phase. Use the summary view to quickly understand the end-to-end API flow, identify performance bottlenecks, and optimize your API strategy.\nSpans view The span view gives you unparalleled visibility into Kong Gateway’s internal workings. This detailed view breaks down into individual spans, providing a comprehensive understanding of:\nKong Gateway’s internal processes and phases Plugin execution and performance Request and response handling For detailed definitions of each span, see Debugger spans. Use the span view to troubleshoot issues, optimize performance, and refine your configuration.\nLogs View A drill-down view of all the logs generated during specific debug session are shown in the logs tab. All the spans in the trace are correlated using trace_id and span_id. The logs can be filtered on log level and spans. Logs are displayed in reverse chronological order. Konnect encrypts all the logs that are ingested. You can further ensure complete privacy and control by using customer-managed encryption keys (CMEK). Use the logs view to quickly troubleshoot and pinpoint issues.\nData Security with Customer-Managed Encryption Keys (CMEK) By default, logs are automatically encrypted using encryption keys that are owned and managed by Konnect. However if you have a specific compliance and regulatory requirements related to the keys that protect your data, you can use the customer-managed encryption keys. This ensures that sensitive data are secured for each organization with their own key and nobody, including Konnect, has access to that data. For more information about how to create and manage CMEK keys, see Customer-Managed Encryption Keys (CMEK).\nStart your first debug session To begin using the Debugger, ensure the following requirements are met:\nYour data plane nodes are running Kong Gateway version 3.9.1 or later. Logs require Kong Gateway version 3.11.0 or later. Your Konnect data planes are hosted using self-managed hybrid, Dedicated Cloud Gateways, or serverless gateways. Kong Ingress Controller or Kong Native Event Proxy Gateways aren’t currently supported. In Gateway Manager, select the control plane that contains the data plane to be traced. In the left navigation menu, click Debugger. Click New session. Define the sampling criteria and click Start Session. Once the session starts, traces will be captured for requests that match the rule. Click a trace to view it in the span viewer.\nEach session can be configured to run for a time between 10 seconds and 30 minutes. Sessions are retained for up to 7 days.\nFor details on defining sampling rules, see Debugger sessions.\nSampling rules Sampling rules help you capture only relevant traffic. Requests that match the defined criteria are included in the session. There are two types:\nBasic sampling rules: Filter by Route or Service. Advanced sampling rules: Use expressions for fine-grained filtering.",
    "description": "Konnect Debugger provides a connected debugging experience and real-time trace-level visibility into API traffic, enabling you to:\nTroubleshoot issues: Investigate and resolve problems during deployments or incidents with targeted, on-demand traces. Understand request lifecycle: Visualize exactly what happened during a specific request, including order and duration plugin execution. See Debugger spans for a list of spans captured. Improve performance and reliability: Use deep insights to fine-tune configurations and resolve bottlenecks. Capture traces and logs Konnect Debugger allows you to capture traces and logs.",
    "tags": [],
    "title": "Konnect Debugger",
    "uri": "/20-observability/21-builtin/212-debugger/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability",
    "content": "The external Observability infrastructure we are going to use is based on the OpenTelemetry standard and it comprises:\nOTel Collector: implements the component responsible for receiving, processing and exporting telemetry data. Loki: plays the log processing role and receiving all requests and responses processed by the Kong API and AI Gateway Data Plane. Prometheus: responsible for scraping and storing the metrics the Kong API and AI Gateway generate. Grafana: used to query and analyze logs and metrics. Jaeger: distributed tracing platform. You can now click Next to begin the module.\nOptional Reading Analytics and Monitor plugins Logging plugins",
    "description": "The external Observability infrastructure we are going to use is based on the OpenTelemetry standard and it comprises:\nOTel Collector: implements the component responsible for receiving, processing and exporting telemetry data. Loki: plays the log processing role and receiving all requests and responses processed by the Kong API and AI Gateway Data Plane. Prometheus: responsible for scraping and storing the metrics the Kong API and AI Gateway generate. Grafana: used to query and analyze logs and metrics. Jaeger: distributed tracing platform.",
    "tags": [],
    "title": "External Observability Stack",
    "uri": "/20-observability/22-external/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability \u003e External Observability Stack",
    "content": "The Kong Konnect and Observability Stack topology is quite simple in this example:\nThe main components here are:\nKonnect Control Plane: responsible for administration tasks including APIs and Policies definition Konnect Data Plane: handles the requests sent by the API consumers Kong Gateway Plugins: components running inside the Data Plane to produce OpenTelemetry signals Upstream Service: services or microservices protected by the Konnnect Data Plane OpenTelemetry Collector: handles and processes the signals sent by the OTel plugin and sends them to the Dynatrace tenant Observability Stack: formed by Loki, Prometheus, Jaeger and Grafana, provides a single pane of glass with dashboards, reports, etc.",
    "description": "The Kong Konnect and Observability Stack topology is quite simple in this example:\nThe main components here are:\nKonnect Control Plane: responsible for administration tasks including APIs and Policies definition Konnect Data Plane: handles the requests sent by the API consumers Kong Gateway Plugins: components running inside the Data Plane to produce OpenTelemetry signals Upstream Service: services or microservices protected by the Konnnect Data Plane OpenTelemetry Collector: handles and processes the signals sent by the OTel plugin and sends them to the Dynatrace tenant Observability Stack: formed by Loki, Prometheus, Jaeger and Grafana, provides a single pane of glass with dashboards, reports, etc.",
    "tags": [],
    "title": "Reference Architecture",
    "uri": "/20-observability/22-external/220-reference_architecture/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability \u003e External Observability Stack",
    "content": "Observability focuses mainly on three core pillars:\nLogs: Detailed, timestamped records of events and activities within a system, offering a granular view of operations Metrics: Quantitative data points that capture various aspects of system performance, such as resource usage, response times, and throughput Traces: Visual paths that requests follow as they traverse through different system components, enabling end-to-end analysis of transactions and interactions OpenTelemetry Here’s a concise definition of OpenTelemetry, available on its website:\n“OpenTelemetry, also known as OTel, is a vendor-neutral open source Observability framework for instrumenting, generating, collecting, and exporting telemetry data such as traces, metrics, and logs.”\nOTel Collector The OTel specification comprises several components, including, for example, the OpenTelemetry Protocol (OTLP). From the architecture perspective, one of the main components is the OpenTelemetry Collector, which is responsible for receiving, processing, and exporting telemetry data. The following diagram is taken from the official OpenTelemetry Collector documentation page.\nAlthough it’s totally valid to send telemetry signals directly from the application to the observability backends with no collector in place, it’s generally recommended to use the OTel Collector. The collector abstracts the backend observability infrastructure, so the services can normalize this kind of processing more quickly in a standardized manner as well as let the collector take care of error handling, encryption, data filtering, transformation, etc.\nAs you can see in the diagram, the collector defines multiple components such as:\nReceivers: Responsible for collecting telemetry data from the sources Processors: Apply transformation, filtering, and calculation to the received data Exporters: Send data to the Observability backend OTel Collector offers other types of connectors and extensions. Please, refer to OTel Collector documentation to learn more about these components.\nThe components are tied together in Pipelines, inside the Service section of the collector configuration file.\nFrom the deployment perspective, here’s the minimum recommended scenario called Agent Pattern. The application uses the OTel SDK to send telemetry data to the collector through OTLP. The collector, in turn, sends the data to the existing backends. The collector is also flexible enough to support a variety of topologies to address scalability, high availability, fan-out, etc. Check the OTel Collector deployment page for more information.\nThe OTel Collector comes from the community, but Dynatrace provides a distribution for the OpenTelemetry Collector. It is a customized implementation tailored for typical use cases in a Dynatrace context. It ships with an optimized and verified set of collector components.",
    "description": "Observability focuses mainly on three core pillars:\nLogs: Detailed, timestamped records of events and activities within a system, offering a granular view of operations Metrics: Quantitative data points that capture various aspects of system performance, such as resource usage, response times, and throughput Traces: Visual paths that requests follow as they traverse through different system components, enabling end-to-end analysis of transactions and interactions OpenTelemetry Here’s a concise definition of OpenTelemetry, available on its website:",
    "tags": [],
    "title": "OpenTelemetry Introduction",
    "uri": "/20-observability/22-external/222-introduction/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability \u003e External Observability Stack",
    "content": "To deploy the OpenTelemetry Collector and to get better control over it, we’re going to do it through the OpenTelemetry Kubernetes Operator. In fact, the collector is also capable of auto-instrument applications and services using OpenTelemetry instrumentation libraries.\nInstalling Cert-Manager The OpenTelemetry Operator requires Cert-Manager to be installed in your Kubernetes cluster. The Cert-Manager can then issue certificates to be used by the communication between the Kubernetes API Server and the existing webhook included in the operator.\nUse the Cert-Manager Helm Charts to get it installed.\nhelm install \\ cert-manager oci://quay.io/jetstack/charts/cert-manager \\ --version v1.18.2 \\ --namespace cert-manager \\ --create-namespace \\ --set crds.enabled=true Installing OpenTelemetry Operator Now we’re going to use the OpenTelemetry Helm Charts to install it. Add its repo:\nhelm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts Install the operator:\nhelm install opentelemetry-operator open-telemetry/opentelemetry-operator \\ --namespace opentelemetry-operator-system \\ --create-namespace \\ --set manager.collectorImage.repository=otel/opentelemetry-collector-k8s \\ --set admissionWebhooks.certManager.enabled=true The “admissionWebhooks” parameter asks Cert-Manager to generate a self-signed certificate. The operator installs some new CRDs used to create a new OpenTelemetry Collector. You can check them out with:\nkubectl describe crd opentelemetrycollectors.opentelemetry.io kubectl describe crd instrumentations.opentelemetry.io",
    "description": "To deploy the OpenTelemetry Collector and to get better control over it, we’re going to do it through the OpenTelemetry Kubernetes Operator. In fact, the collector is also capable of auto-instrument applications and services using OpenTelemetry instrumentation libraries.\nInstalling Cert-Manager The OpenTelemetry Operator requires Cert-Manager to be installed in your Kubernetes cluster. The Cert-Manager can then issue certificates to be used by the communication between the Kubernetes API Server and the existing webhook included in the operator.",
    "tags": [],
    "title": "OTel Collector Operator Installation",
    "uri": "/20-observability/22-external/223-otel_collector_operator_installation/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability \u003e External Observability Stack",
    "content": "Jaeger Installation We are going to use the Jaeger Helm Charts\nSave the values.yaml Jaeger provides as an example:\nwget -O jaeger-values.yaml https://raw.githubusercontent.com/jaegertracing/helm-charts/refs/heads/v2/charts/jaeger/values.yaml And use it to install Jaeger 2.9.0\nhelm install jaeger jaegertracing/jaeger -n jaeger \\ --create-namespace \\ --set allInOne.image.repository=jaegertracing/jaeger \\ --set allInOne.image.tag=2.9.0 \\ --values ./jaeger-values.yaml kubectl patch deployment jaeger -n jaeger --type json \\ -p='[ {\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/readinessProbe\"}, {\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"} ]' Check Jaeger’s log with:\nkubectl logs -f $(kubectl get pod -n jaeger -o json | jq -r '.items[].metadata | select(.name | startswith(\"jaeger-\"))' | jq -r '.name') -n jaeger Prometheus Installation Add the Helm Charts first:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts Again, after the installation, we should have two new Minikube tunnels defined:\nhelm install prometheus -n prometheus prometheus-community/kube-prometheus-stack \\ --create-namespace \\ --set alertmanager.enabled=false \\ --set grafana.enabled=false \\ --set prometheus.service.type=LoadBalancer \\ --set prometheus.service.port=9090 \\ --set prometheus.prometheusSpec.additionalArgs[0].name=web.enable-otlp-receiver \\ --set prometheus.prometheusSpec.additionalArgs[0].value= Loki Installation First, add the Helm Charts. Read the documentation to learn more.\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update Install Loki with the following Helm command. Since we are exposing it with a Load Balancer, Minikube will start a new tunnel for the port 3100.\nhelm install loki grafana/loki \\ --namespace=loki --create-namespace \\ -f loki-config.yaml kubectl patch svc loki \\ -n loki \\ -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' Grafana Installation helm upgrade --install grafana grafana/grafana \\ --namespace grafana \\ --create-namespace \\ --set adminUser=admin \\ --set adminPassword=admin \\ --set service.type=LoadBalancer \\ --set service.port=3000 \\ --set datasources.\"datasources\\.yaml\".apiVersion=1 \\ --set datasources.\"datasources\\.yaml\".datasources[0].name=Jaeger \\ --set datasources.\"datasources\\.yaml\".datasources[0].type=jaeger \\ --set datasources.\"datasources\\.yaml\".datasources[0].url=http://jaeger-query.jaeger:16686 \\ --set datasources.\"datasources\\.yaml\".datasources[0].access=proxy \\ --set datasources.\"datasources\\.yaml\".datasources[1].name=Prometheus \\ --set datasources.\"datasources\\.yaml\".datasources[1].type=prometheus \\ --set datasources.\"datasources\\.yaml\".datasources[1].url=http://prometheus-kube-prometheus-prometheus.prometheus:9090 \\ --set datasources.\"datasources\\.yaml\".datasources[1].access=proxy \\ --set datasources.\"datasources\\.yaml\".datasources[2].name=Loki \\ --set datasources.\"datasources\\.yaml\".datasources[2].type=loki \\ --set datasources.\"datasources\\.yaml\".datasources[2].url=http://loki.loki:3100 \\ --set datasources.\"datasources\\.yaml\".datasources[2].access=proxy Uninstall If you want to uninstall them run:\nhelm uninstall jaeger -n jaeger kubectl delete namespace jaeger helm uninstall prometheus -n prometheus kubectl delete namespace prometheus helm uninstall loki -n loki kubectl delete namespace loki helm uninstall grafana -n grafana kubectl delete namespace grafana",
    "description": "Jaeger Installation We are going to use the Jaeger Helm Charts\nSave the values.yaml Jaeger provides as an example:\nwget -O jaeger-values.yaml https://raw.githubusercontent.com/jaegertracing/helm-charts/refs/heads/v2/charts/jaeger/values.yaml And use it to install Jaeger 2.9.0\nhelm install jaeger jaegertracing/jaeger -n jaeger \\ --create-namespace \\ --set allInOne.image.repository=jaegertracing/jaeger \\ --set allInOne.image.tag=2.9.0 \\ --values ./jaeger-values.yaml kubectl patch deployment jaeger -n jaeger --type json \\ -p='[ {\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/readinessProbe\"}, {\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"} ]' Check Jaeger’s log with:\nkubectl logs -f $(kubectl get pod -n jaeger -o json | jq -r '.items[].metadata | select(.name | startswith(\"jaeger-\"))' | jq -r '.name') -n jaeger Prometheus Installation Add the Helm Charts first:",
    "tags": [],
    "title": "Observability Stack Installation",
    "uri": "/20-observability/22-external/224-observability_stack_installation/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability \u003e External Observability Stack",
    "content": "Here’s a nice introduction of OTel Collector and Jaeger deployment. The Collector is going to be running as a “remote cluster” in a specific K8s deployument.\nOpenTelemetry Collector instantiation Create a collector declaration To get started we’re going to manage Traces first. Later on, we’ll enhance the collector to process both Metrics and Logs. Here’s the declaration:\ncat \u003e otelcollector.yaml \u003c\u003c 'EOF' apiVersion: opentelemetry.io/v1beta1 kind: OpenTelemetryCollector metadata: name: collector-kong namespace: opentelemetry-operator-system spec: image: otel/opentelemetry-collector-contrib:0.132.2 mode: deployment config: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 exporters: otlphttp: endpoint: http://jaeger-collector.jaeger:4318 #debug: # verbosity: detailed service: pipelines: traces: receivers: [otlp] exporters: [otlphttp] EOF The declaration has critical parameters defined:\nimage: it refers to the OTel Collector contrib distribution. mode: deployment. The collector can be deployed in 4 different modes: “Deployment”, “DaemonSet”, “StatefulSet” and “Sidecar”. For better control of the Controller, we’ve chosen regular Kubernetes Deployment mode. Please, refer to the Kubernetes documentation to learn more about them. The config section has the collector components (receivers and exporters) as well as the “service” section defining the Pipeline. The “receivers” section tells us the collector will be listening to ports 4317 and 4318 and will be receiving data over “grpc” and “http”. The “exporters” section used the endpoint and the API Token to send data to Jaeger. You can check the Jaeger’s APIs here. The “service” section defines the Pipeline. Deploy the collector kubectl apply -f otelcollector.yaml If you want to destroy it run:\nkubectl delete opentelemetrycollector collector-kong -n opentelemetry-operator-system Check the collector’s log with:\nkubectl logs -f $(kubectl get pod -n opentelemetry-operator-system -o json | jq '.items[].metadata | select(.name | startswith(\"collector\"))' | jq -r '.name') -n opentelemetry-operator-system Based on the declaration, the deployment creates a Kubernetes service named “collector-kong-collector” listening to ports 4317 and 4318. That means that any application, including Kong Data Plane, should refer to the OTel Collector’s Kubernetes FQDN (e.g., http://collector-kong-collector.opentelemetry-operator-system.svc.cluster.local:4318/v1/traces) to send data to the collector. The “/v1/traces” path is the default the collector uses to handle requests with trace data.\n% kubectl get service collector-kong-collector -n opentelemetry-operator-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE collector-kong-collector ClusterIP 10.102.44.48 \u003cnone\u003e 4317/TCP,4318/TCP 55s Update the DataPlane with tracing instrumentations kubectl delete dataplane dataplane1 -n kong cat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 env: - name: KONG_TRACING_INSTRUMENTATIONS value: all - name: KONG_TRACING_SAMPLING_RATE value: \"1.0\" network: services: ingress: name: proxy1 type: LoadBalancer EOF Apply the OTel Plugin to the Kong Service and consume the Kong Route cat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 plugins: - name: opentelemetry instance_name: opentelemetry1 enabled: true config: traces_endpoint: http://collector-kong-collector.opentelemetry-operator-system.svc.cluster.local:4318/v1/traces #propagation: # default_format: \"w3c\" # inject: [\"w3c\"] resource_attributes: service.name: \"kong-otel\" routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route EOF Submit the declaration\ndeck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT httpbin.yaml Consume the Kong Route\ncurl -v $DATA_PLANE_LB/httpbin-route/get Use Grafana to see your traces By default, Grafana administrador’s credentials are: admin/prom-operator\nIn MacOS, you can open Grafana with:\nopen -a \"Google Chrome\" \"http://localhost:3000\" Use the admin/admin as the username and password. You will be asked to change your password.\nCheck the Traces Click “Explore” in the left-side menu. Choose “Jaeger” as the data source. For “Query type”, click “Search”. You should see kong-otel as an option for “Service Name”. Click “Run query”. You should see your trace there. Click it and you should get the spans.",
    "description": "Here’s a nice introduction of OTel Collector and Jaeger deployment. The Collector is going to be running as a “remote cluster” in a specific K8s deployument.\nOpenTelemetry Collector instantiation Create a collector declaration To get started we’re going to manage Traces first. Later on, we’ll enhance the collector to process both Metrics and Logs. Here’s the declaration:\ncat \u003e otelcollector.yaml \u003c\u003c 'EOF' apiVersion: opentelemetry.io/v1beta1 kind: OpenTelemetryCollector metadata: name: collector-kong namespace: opentelemetry-operator-system spec: image: otel/opentelemetry-collector-contrib:0.132.2 mode: deployment config: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 exporters: otlphttp: endpoint: http://jaeger-collector.jaeger:4318 #debug: # verbosity: detailed service: pipelines: traces: receivers: [otlp] exporters: [otlphttp] EOF The declaration has critical parameters defined:",
    "tags": [],
    "title": "OTel Collector and Tracing",
    "uri": "/20-observability/22-external/225-otel_collector_tracing/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability \u003e External Observability Stack",
    "content": "Now, let’s add Metrics to our environment. Kong has supported Prometheus-based metrics for a long time through the Prometheus Plugin. In an OpenTelemetry configuration scenario the plugin is an option, where we could add a specific “prometheusreceiver” to the collector configuration. The receiver is responsible for scraping the Data Plane’s Status API, which, by default, is configured with the :8100/metrics endpoint.\nYou can check the port with:\nkubectl get pod -o yaml $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name') -n kong | yq '.spec.containers[].env[] | select(.name == \"KONG_STATUS_LISTEN\")' Expected result:\nname: KONG_STATUS_LISTEN value: 0.0.0.0:8100 and with:\nkubectl get pod -o yaml $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name') -n kong | yq '.spec.containers[].ports[] | select(.name == \"metrics\")' Expected result:\ncontainerPort: 8100 name: metrics protocol: TCP New collector configuration We need to add the new Prometheus Receiver to our OTel Collector configuration:\ncat \u003e otelcollector.yaml \u003c\u003c 'EOF' apiVersion: opentelemetry.io/v1beta1 kind: OpenTelemetryCollector metadata: name: collector-kong namespace: opentelemetry-operator-system spec: image: otel/opentelemetry-collector-contrib:0.132.2 serviceAccount: collector mode: deployment config: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 prometheus: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 5s kubernetes_sd_configs: - role: pod scheme: http tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt authorization: credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token metrics_path: /metrics relabel_configs: - source_labels: [__meta_kubernetes_namespace] action: keep regex: \"kong\" - source_labels: [__meta_kubernetes_pod_name] action: keep regex: \"dataplane-(.+)\" - source_labels: [__meta_kubernetes_pod_container_name] action: keep regex: \"proxy\" - source_labels: [__meta_kubernetes_pod_container_port_number] action: keep regex: \"8100\" exporters: otlphttp/jaeger: endpoint: http://jaeger-collector.jaeger:4318 otlphttp/prometheus: endpoint: http://prometheus-kube-prometheus-prometheus.prometheus:9090/api/v1/otlp prometheus: endpoint: 0.0.0.0:8889 #debug: # verbosity: detailed service: pipelines: traces: receivers: [otlp] exporters: [otlphttp/jaeger] metrics: receivers: [prometheus] exporters: [otlphttp/prometheus, prometheus] EOF The declaration has critical parameters defined:\nInside the “service” configuration section, a new “metrics” pipeline have been included. It has two exporters: The prometheus exporter configured, so we can access the metrics sending requests directly to the collector through port 8889 as described in the exporter section. It also includes the otlphttp/prometheus exporter responsible for pushing the metrics to Prometheus using its specific OTLP endpoint. Kubernetes Service Account for Prometheus Receiver The OTel Collector Prometheus Receiver fully supports the scraping configuration defined by Prometheus. The receiver, more precisely, uses the pod role of the Kubernetes Service Discovery configurations (kubernetes_sd_config). Specific relabel_config settings with “regex” expressions allow the receiver to discover Kubernetes Pods that belong to the Kong Data Plane deployment.\nOne of the relabeling configs is related to the port 8100, named metrics. This port configuration is part of the Data Plane deployment we used to get it running.\nThat’s the Kong Gateway’s Status API where the Prometheus plugin exposes the metrics produced. In fact, the endpoint the receiver scrapes is, as specified in the OTel Collector configuration, http://\u003cData_Plane_Pod_IP\u003e:8100/metrics\nOn the other hand, the OTel Collector has to be allowed to scrape the endpoint. We can define such permission with a Kubernetes ClusterRole and apply it to a Kubernetes Service Account with a Kubernetes ClusterRoleBinding.\nHere’s the ClusterRole declaration. It’s a quite open one but it’s good enough for this exercise.\nkubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: pod-reader rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] EOF Then we need to create a Kubernetes Service Account and bind the Role to it.\nkubectl create sa collector -n opentelemetry-operator-system kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: read-pods roleRef: kind: ClusterRole name: pod-reader subjects: - kind: ServiceAccount name: collector namespace: opentelemetry-operator-system EOF cat \u003c\u003cEOF | kubectl apply -f - apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: collector namespace: opentelemetry-operator-system labels: app.kubernetes.io/name: kong spec: selector: matchLabels: gateway-operator.konghq.com/dataplane-service-type: ingress endpoints: - targetPort: metrics scheme: http jobLabel: kong namespaceSelector: matchNames: - kong EOF Finally, note that the OTel Collector configuration is deployed using the Service Account with serviceAccount: collector and then it will be able to scrape the endpoint exposed by Kong Gateway.\nDeploy the collector Delete the current collector first and instantiate a new one simply submitting the declaration:\nkubectl delete opentelemetrycollector collector-kong -n opentelemetry-operator-system kubectl apply -f otelcollector.yaml Interestingly enough, the collector service now listens to three ports:\n% kubectl get service collector-kong-collector -n opentelemetry-operator-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE collector-kong-collector ClusterIP 10.100.67.18 \u003cnone\u003e 4317/TCP,4318/TCP,8889/TCP. 21h Configure the Prometheus Plugins Add the Prometheus and TCP Log plugins to our decK declaration and submit it to Konnect:\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 plugins: - name: opentelemetry instance_name: opentelemetry1 enabled: true config: traces_endpoint: http://collector-kong-collector.opentelemetry-operator-system.svc.cluster.local:4318/v1/traces #propagation: # default_format: \"w3c\" # inject: [\"w3c\"] resource_attributes: service.name: \"kong-otel\" - name: prometheus instance_name: prometheus1 enabled: true config: per_consumer: true status_code_metrics: true latency_metrics: true bandwidth_metrics: true upstream_health_metrics: true ai_metrics: true routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route EOF Submit the new plugin declaration with:\ndeck gateway sync --konnect-token $PAT httpbin.yaml Consume the Kong Route curl -v $DATA_PLANE_LB/httpbin-route/get Check OTel Collector’s Prometheus endpoint Using “port-forward”, send a request to the collector’s Prometheus endpoint. In a terminal run:\nkubectl port-forward service/collector-kong-collector -n opentelemetry-operator-system 8889 Continue navigating the Application to see some metrics getting generated. In another terminal send a request to Prometheus’ endpoint.\nhttp :8889/metrics You should see several related Kong metrics including, for example, Histogram metrics like “kong_kong_latency_ms_bucket”, “kong_request_latency_ms_bucket” and “kong_upstream_latency_ms_bucket”. Maybe one of the most important is “kong_http_requests_total” where we can see consumption metrics. Here’s a snippet of the output:\n# HELP kong_http_requests_total HTTP status codes per consumer/service/route in Kong # TYPE kong_http_requests_total counter kong_http_requests_total{code=\"200\",instance=\"192.168.76.233:8100\",job=\"otel-collector\",route=\"coupon_route\",service=\"coupon_service\",source=\"service\",workspace=\"default\"} 1 kong_http_requests_total{code=\"200\",instance=\"192.168.76.233:8100\",job=\"otel-collector\",route=\"inventory_route\",service=\"inventory_service\",source=\"service\",workspace=\"default\"} 1 kong_http_requests_total{code=\"200\",instance=\"192.168.76.233:8100\",job=\"otel-collector\",route=\"pricing_route\",service=\"pricing_service\",source=\"service\",workspace=\"default\"} 1 Check Prometheus In MacOS, you can open Grafana with:\nopen -a \"Google Chrome\" \"http://localhost:9090\" In the Query page you can see all metrics produced by Kong Prometheus plugin and pushed by the OTel Collector. Choose kong_http_requests_total. Check Metrics in Grafana In Grafana UI:\nClick “Explore” in the left-side menu. Choose “Prometheus” as the data source. Inside the “metrics” box you should see all new Kong metrics. Choose kong_http_requests_total. Click “Run query”. You should see the metric there.",
    "description": "Now, let’s add Metrics to our environment. Kong has supported Prometheus-based metrics for a long time through the Prometheus Plugin. In an OpenTelemetry configuration scenario the plugin is an option, where we could add a specific “prometheusreceiver” to the collector configuration. The receiver is responsible for scraping the Data Plane’s Status API, which, by default, is configured with the :8100/metrics endpoint.\nYou can check the port with:\nkubectl get pod -o yaml $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith(\"dataplane-\"))' | jq -r '.name') -n kong | yq '.spec.containers[].env[] | select(.name == \"KONG_STATUS_LISTEN\")' Expected result:",
    "tags": [],
    "title": "OTel Collector and Metrics",
    "uri": "/20-observability/22-external/226-otel_collector_metrics/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Observability \u003e External Observability Stack",
    "content": "We still need to add logs to our environment where Loki has been deployed. To inject Kong Gateway’s Access Logs, we can use a Log Processing plugin Kong Gateway provides, for example the TCP Log Plugin.\nHit the port to make sure Loki is ready to accept requests:\ncurl http://localhost:3100/ready New collector configuration cat \u003e otelcollector.yaml \u003c\u003c 'EOF' apiVersion: opentelemetry.io/v1beta1 kind: OpenTelemetryCollector metadata: name: collector-kong namespace: opentelemetry-operator-system spec: image: otel/opentelemetry-collector-contrib:0.132.2 serviceAccount: collector mode: deployment config: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 prometheus: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 5s kubernetes_sd_configs: - role: pod scheme: http tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt authorization: credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token metrics_path: /metrics relabel_configs: - source_labels: [__meta_kubernetes_namespace] action: keep regex: \"kong\" - source_labels: [__meta_kubernetes_pod_name] action: keep regex: \"dataplane-(.+)\" - source_labels: [__meta_kubernetes_pod_container_name] action: keep regex: \"proxy\" - source_labels: [__meta_kubernetes_pod_container_port_number] action: keep regex: \"8100\" tcplog: listen_address: 0.0.0.0:54525 operators: - type: json_parser exporters: otlphttp/jaeger: endpoint: http://jaeger-collector.jaeger:4318 otlphttp/prometheus: endpoint: http://prometheus-kube-prometheus-prometheus.prometheus:9090/api/v1/otlp otlphttp/loki: endpoint: http://loki.loki:3100/otlp prometheus: endpoint: 0.0.0.0:8889 #debug: # verbosity: detailed service: pipelines: traces: receivers: [otlp] exporters: [otlphttp/jaeger] metrics: receivers: [prometheus] exporters: [otlphttp/prometheus, prometheus] logs: receivers: [tcplog] exporters: [otlphttp/loki] EOF The declaration has critical parameters defined:\nA new TCP Receiver has been added, listening to the port 54525, used by the Kong Gateway TCP Log Plugin. It uses the “json_parser” operator to send formatted data to Loki. Still inside the “service” section we have included the new “logs” pipeline. Its “receivers” are set to “tcplog” to get data from the TCP Log Kong Gateway Plugin. Its “exporters” is set to a different “otlphttp/loki” which sends data to the Loki endpoint. Deploy the collector Delete the current collector first and instantiate a new one simply submitting the declaration:\nkubectl delete opentelemetrycollector collector-kong -n opentelemetry-operator-system kubectl apply -f otelcollector.yaml The collector service now listens to four ports:\n% kubectl get service collector-kong-collector -n opentelemetry-operator-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE collector-kong-collector ClusterIP 10.100.67.18 \u003cnone\u003e 4317/TCP,4318/TCP,8889/TCP,54525/TCP 21h Configure the Prometheus and TCP Log Plugins Add the Prometheus and TCP Log plugins to our decK declaration and submit it to Konnect:\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 plugins: - name: opentelemetry instance_name: opentelemetry1 enabled: true config: traces_endpoint: http://collector-kong-collector.opentelemetry-operator-system.svc.cluster.local:4318/v1/traces #propagation: # default_format: \"w3c\" # inject: [\"w3c\"] resource_attributes: service.name: \"kong-otel\" - name: prometheus instance_name: prometheus1 enabled: true config: per_consumer: true status_code_metrics: true latency_metrics: true bandwidth_metrics: true upstream_health_metrics: true ai_metrics: true - name: tcp-log instance_name: tcp-log1 enabled: true config: host: collector-kong-collector.opentelemetry-operator-system.svc.cluster.local port: 54525 custom_fields_by_lua: streams: local ts=string.format('%18.0f', os.time()*1000000000) local log_payload = kong.log.serialize() local service = log_payload['service'] or 'noService' local cjson = require \"cjson\" local payload_string = cjson.encode(log_payload) local t = { {stream = {gateway='kong-gateway', service_name=service['name']}, values={{ts, payload_string}}} } return t routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route EOF Loki requires a specific format for log ingestion. The Lua code inside the custom_fields_for_lua parameter processes the log to be supported by Loki.\nUpdate the DataPlane with new Lua configuration The cjon Lua function, used by the TCP Log plugin is considered untrusted so we need to configure the Data Plane with the KONG_UNTRUSTED_LUA_SANDBOX_REQUIRES paramenter to accept it.\nkubectl delete dataplane dataplane1 -n kong cat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 env: - name: KONG_TRACING_INSTRUMENTATIONS value: all - name: KONG_TRACING_SAMPLING_RATE value: \"1.0\" - name: KONG_UNTRUSTED_LUA_SANDBOX_REQUIRES value: cjson network: services: ingress: name: proxy1 type: LoadBalancer EOF Consume the Kong Route curl -v $DATA_PLANE_LB/httpbin-route/get Check Logs in Grafana In Grafana UI:\nClick “Explore” in the left-side menu. Choose “Loki” as the data source. Click “Run query” with the following parameters. You should see the logs there.",
    "description": "We still need to add logs to our environment where Loki has been deployed. To inject Kong Gateway’s Access Logs, we can use a Log Processing plugin Kong Gateway provides, for example the TCP Log Plugin.\nHit the port to make sure Loki is ready to accept requests:\ncurl http://localhost:3100/ready New collector configuration cat \u003e otelcollector.yaml \u003c\u003c 'EOF' apiVersion: opentelemetry.io/v1beta1 kind: OpenTelemetryCollector metadata: name: collector-kong namespace: opentelemetry-operator-system spec: image: otel/opentelemetry-collector-contrib:0.132.2 serviceAccount: collector mode: deployment config: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 prometheus: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 5s kubernetes_sd_configs: - role: pod scheme: http tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt authorization: credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token metrics_path: /metrics relabel_configs: - source_labels: [__meta_kubernetes_namespace] action: keep regex: \"kong\" - source_labels: [__meta_kubernetes_pod_name] action: keep regex: \"dataplane-(.+)\" - source_labels: [__meta_kubernetes_pod_container_name] action: keep regex: \"proxy\" - source_labels: [__meta_kubernetes_pod_container_port_number] action: keep regex: \"8100\" tcplog: listen_address: 0.0.0.0:54525 operators: - type: json_parser exporters: otlphttp/jaeger: endpoint: http://jaeger-collector.jaeger:4318 otlphttp/prometheus: endpoint: http://prometheus-kube-prometheus-prometheus.prometheus:9090/api/v1/otlp otlphttp/loki: endpoint: http://loki.loki:3100/otlp prometheus: endpoint: 0.0.0.0:8889 #debug: # verbosity: detailed service: pipelines: traces: receivers: [otlp] exporters: [otlphttp/jaeger] metrics: receivers: [prometheus] exporters: [otlphttp/prometheus, prometheus] logs: receivers: [tcplog] exporters: [otlphttp/loki] EOF The declaration has critical parameters defined:",
    "tags": [],
    "title": "OTel Collector and Logs",
    "uri": "/20-observability/22-external/227-otel_collector_logs/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases \u003e Rate Limiting Using Redis",
    "content": "Add Rate Limiting plugin Just like you did before, add the Rate Limiting plugin on the Route setting Minute as 5 requests per minute, and set the identifier to Service.\ncat \u003e rate-limiting.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: rate-limiting-route paths: - /rate-limiting-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 5 EOF Submit the declaration:\ndeck gateway sync --konnect-token $PAT rate-limiting.yaml Verify traffic control Again, test the rate-limiting policy by executing the following command multiple times and observe the rate-limit headers in the response, specially, X-RateLimit-Remaining-Minute, RateLimit-Reset and Retry-After :\ncurl -I $DATA_PLANE_LB/rate-limiting-route/get Response\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 389 Connection: keep-alive X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 4 RateLimit-Reset: 44 RateLimit-Remaining: 4 RateLimit-Limit: 5 Server: gunicorn Date: Mon, 11 Aug 2025 14:55:16 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 10 X-Kong-Proxy-Latency: 5 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 7f13e52db83e4e673798120134496d03 As explected, after sending too many requests,once the rate limiting is reached, you will see HTTP/1.1 429 Too Many Requests\n# curl -I $DATA_PLANE_LB/rate-limiting-route/get HTTP/1.1 429 Too Many Requests Date: Mon, 11 Aug 2025 14:55:20 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 0 RateLimit-Reset: 40 Retry-After: 40 RateLimit-Remaining: 0 RateLimit-Limit: 5 Content-Length: 92 X-Kong-Response-Latency: 2 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: c01dcb02ea13676ca5e49e7e1c40982b Results As there is a single Kong Data Plane Runtime instance running, Kong correctly imposes the rate-limit and you can make only 5 requests in a minute.",
    "description": "Add Rate Limiting plugin Just like you did before, add the Rate Limiting plugin on the Route setting Minute as 5 requests per minute, and set the identifier to Service.\ncat \u003e rate-limiting.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: rate-limiting-route paths: - /rate-limiting-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 5 EOF Submit the declaration:",
    "tags": [],
    "title": "Set up Rate Limiting plugin",
    "uri": "/12-api-gateway/15-use-cases/156-rate-limiting-using-redis/1563-setup-rate-limiting-plugin/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases \u003e Rate Limiting Using Redis",
    "content": "Let’s scale out the Kong Data Plane deployment to 3 pods, for scalability and redundancy:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 replicas: 3 network: services: ingress: name: proxy1 type: LoadBalancer EOF Wait for replicas to deploy It will take a couple minutes for the new pods to start up. Run the following command to show that the replicas are ready.\nkubectl get pods -n kong NAME READY STATUS RESTARTS AGE dataplane-dataplane1-qdc66-84d7746bbf-dnvp8 1/1 Running 0 4d23h dataplane-dataplane1-qdc66-84d7746bbf-hlxwx 1/1 Running 0 26s dataplane-dataplane1-qdc66-84d7746bbf-kpbpl 1/1 Running 0 26s httpbin-5c69574c95-xq76q 1/1 Running 0 6d19h Check Konnect Runtime Group Similarly you can see new Runtime Instances connected to your Runtime Group\nVerify traffic control Test the rate-limiting policy by executing the following command and observing the rate-limit headers.\ncurl -I $DATA_PLANE_LB/rate-limiting-route/get Response\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 389 Connection: keep-alive X-RateLimit-Remaining-Minute: 4 RateLimit-Limit: 5 X-RateLimit-Limit-Minute: 5 RateLimit-Remaining: 4 RateLimit-Reset: 57 Server: gunicorn Date: Mon, 11 Aug 2025 15:12:03 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 9 X-Kong-Proxy-Latency: 8 Via: 1.1 kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 278fcb4ae01a6e7fcc88ed701adaf942 Results You will observe that the rate-limit is not consistent anymore and you can make more than 5 requests in a minute.\nTo understand this behavior, we need to understand how we have configured Kong. In the current policy, each Kong node is tracking a rate-limit in-memory and it will allow 5 requests to go through for a client. There is no synchronization of the rate-limit information across Kong nodes. In use-cases where rate-limiting is used as a protection mechanism and to avoid over-loading your services, each Kong node tracking it’s own counter for requests is good enough as a malicious user will hit rate-limits on all nodes eventually. Or if the load-balance in-front of Kong is performing some sort of deterministic hashing of requests such that the same Kong node always receives the requests from a client, then we won’t have this problem at all.\nWhats Next ? In some cases, a synchronization of information that each Kong node maintains in-memory is needed. For that purpose, Redis can be used. Let’s go ahead and set this up next.",
    "description": "Let’s scale out the Kong Data Plane deployment to 3 pods, for scalability and redundancy:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 replicas: 3 network: services: ingress: name: proxy1 type: LoadBalancer EOF Wait for replicas to deploy It will take a couple minutes for the new pods to start up. Run the following command to show that the replicas are ready.",
    "tags": [],
    "title": "Scale Kong for Kubernetes to multiple pods",
    "uri": "/12-api-gateway/15-use-cases/156-rate-limiting-using-redis/1564-scale-kong-for-kubernetes-to-multiple-pods/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases \u003e Rate Limiting Using Redis",
    "content": "Update the Rate Limiting Plugin Let’s update our Kong Plugin configuration to use Redis as a data store rather than each Kong node storing the counter information in-memory. As a reminder, Redis was installed previously and it is available in the EKS cluster.\nHere’s the new declaration:\ncat \u003e rate-limiting.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: rate-limiting-route paths: - /rate-limiting-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 5 policy: redis redis: host: redis-stack.redis.svc.cluster.local port: 6379 EOF Submit the declaration:\ndeck gateway sync --konnect-token $PAT rate-limiting.yaml Test it Execute the following commands more than 5 times.\nWhat happens?\ncurl -I $DATA_PLANE_LB/rate-limiting-route/get Response\nHTTP/1.1 429 Too Many Requests Date: Mon, 11 Aug 2025 15:16:04 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive Retry-After: 56 X-RateLimit-Remaining-Minute: 0 RateLimit-Limit: 5 X-RateLimit-Limit-Minute: 5 RateLimit-Remaining: 0 RateLimit-Reset: 56 Content-Length: 92 X-Kong-Response-Latency: 7 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: b1c6ee6da33a042a51eeca1b5fb4f150 Expected Results Because Redis is the data-store for the rate-limiting plugin, you should be able to make only 5 requests in a minute\nReduce the number of replicas cat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 replicas: 1 network: services: ingress: name: proxy1 type: LoadBalancer EOF Kong-gratulations! have now reached the end of this section by configuring Redis as a data-store to synchronize information across multiple Kong nodes to enforce the rate-limiting policy. This can also be used for other plugins which support Redis as a data-store such as proxy-cache. You can now click Next to proceed with the next section of the module.",
    "description": "Update the Rate Limiting Plugin Let’s update our Kong Plugin configuration to use Redis as a data store rather than each Kong node storing the counter information in-memory. As a reminder, Redis was installed previously and it is available in the EKS cluster.\nHere’s the new declaration:\ncat \u003e rate-limiting.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: rate-limiting-route paths: - /rate-limiting-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 5 policy: redis redis: host: redis-stack.redis.svc.cluster.local port: 6379 EOF Submit the declaration:",
    "tags": [],
    "title": "Update the Rate Limiting Plugin",
    "uri": "/12-api-gateway/15-use-cases/156-rate-limiting-using-redis/1565-update-rate-limiting-plugin/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases \u003e OpenID Connect",
    "content": "The two next topics describe Authorization Code OAuth and Client Credentials grants implemented by Kong Konnect and Keycloak as the Identity Provider. Let’s start installing Keycloak in our Kubernetes Cluster.\nKeycloak Installation Run the following command to deploy Keycloak:\nwget https://raw.githubusercontent.com/keycloak/keycloak-quickstarts/refs/heads/main/kubernetes/keycloak.yaml yq 'select(.kind == \"Service\" and .metadata.name == \"keycloak\") |= .spec.type = \"LoadBalancer\"' -i keycloak.yaml yq 'select(.kind == \"StatefulSet\" and .metadata.name == \"keycloak\") |= .spec.replicas = 1' -i keycloak.yaml kubectl create namespace keycloak kubectl apply -n keycloak -f keycloak.yaml Keycloak “Realm” definition All our configuration will be done in a specific Keycloak Realm. Direct your browser to the Keycloak’s external IP address:\nhttp://localhost:8080 To login use the admin’s credentials: admin/admin. Click on Manage realms we can create a new realm. Create a realm called kong.\nClient_Id/Client_Secret creation For both OAuth grants we need a cliend_id and client_secret pair. Clicking on Clients and Create client we can define a new client representing Kong.\nChoose kong_id for the new Client ID. The configurations should be the following:\nCapability config Client authentication: on (Access Type: public) Authentication flow -\u003e Service accounts role: on (this allows us to implement the OAuth “Client Credentials” Grant) Login settings Valid Redirect URIs: http://localhost/oidc-route/get. This parameter is needed in the OAuth Authorization Code Grant. It defines which URIs are allowed to redirect users to Keycloak. Click on Save.\nTo get your client_secret, click on the Credentials option shown in the horizontal menu. Take note of the client_secret, for example: RVXO9SOJskjw4LeVupjRbIMJIAyyil8j\nTest the Keycloak Endpoint You can check the Keycloak setting sending a request directly to its Token Endpoint, passing the client_id/client_secret pair you have just created. You should get an Access Token as a result. Use jwt to decode the Access Token. Make sure you have jwt installed on your environment. For example:\ncurl -s -X POST 'http://localhost:8080/realms/kong/protocol/openid-connect/token' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'client_id=kong_id' \\ --data-urlencode 'client_secret=RVXO9SOJskjw4LeVupjRbIMJIAyyil8j' \\ --data-urlencode 'grant_type=client_credentials' | jq -r '.access_token' | jwt decode - Expected Output:\nToken header ------------ { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"JIao4TIXpSwJxcukz6W0hK8qc_vuYf6HrmGsDmT6kzY\" } Token claims ------------ { \"acr\": \"1\", \"allowed-origins\": [ \"/*\" ], \"aud\": \"account\", \"azp\": \"kong_id\", \"clientAddress\": \"10.244.0.1\", \"clientHost\": \"10.244.0.1\", \"client_id\": \"kong_id\", \"email_verified\": false, \"exp\": 1754940732, \"iat\": 1754940432, \"iss\": \"http://localhost:8080/realms/kong\", \"jti\": \"trrtcc:e2550f41-801f-e8e2-99b5-cc90a3648091\", \"preferred_username\": \"service-account-kong_id\", \"realm_access\": { \"roles\": [ \"offline_access\", \"uma_authorization\", \"default-roles-kong\" ] }, \"resource_access\": { \"account\": { \"roles\": [ \"manage-account\", \"manage-account-links\", \"view-profile\" ] } }, \"scope\": \"email profile\", \"sub\": \"e7b5a37b-d06a-4b40-92d7-36f09768ed79\", \"typ\": \"Bearer\" } User creation Now, specifically for the Authorization Code Grant, we need to create a Keycloak user. You can do it by clicking on Users and Create new user. Choose consumer1 for the Username and click on Create:\nClick on Credentials and Set password. Type kong for both Password and Password confirmation fields. Turn Temporary to off and click on Save and Save Password.",
    "description": "The two next topics describe Authorization Code OAuth and Client Credentials grants implemented by Kong Konnect and Keycloak as the Identity Provider. Let’s start installing Keycloak in our Kubernetes Cluster.\nKeycloak Installation Run the following command to deploy Keycloak:\nwget https://raw.githubusercontent.com/keycloak/keycloak-quickstarts/refs/heads/main/kubernetes/keycloak.yaml yq 'select(.kind == \"Service\" and .metadata.name == \"keycloak\") |= .spec.type = \"LoadBalancer\"' -i keycloak.yaml yq 'select(.kind == \"StatefulSet\" and .metadata.name == \"keycloak\") |= .spec.replicas = 1' -i keycloak.yaml kubectl create namespace keycloak kubectl apply -n keycloak -f keycloak.yaml Keycloak “Realm” definition All our configuration will be done in a specific Keycloak Realm. Direct your browser to the Keycloak’s external IP address:",
    "tags": [],
    "title": "Keycloak",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/1573-keycloak/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases \u003e OpenID Connect",
    "content": "This page describes a configuration of the Authorization Code Grant. Check the OpenID Connect plugin documentation to learn more about it.\nInstalling OpenID Connect Plugin All Keycloak settings are available for the OIDC plugin in the following address:\ncurl http://localhost:8080/realms/kong/.well-known/openid-configuration | jq The most important ones are the endpoints necessary to implement the Grant:\ncat \u003e oidc.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: oidc-route paths: - /oidc-route plugins: - name: openid-connect instance_name: openid-connect1 config: auth_methods: [\"authorization_code\"] redirect_uri: - http://localhost/oidc-route/get client_id: [\"kong_id\"] client_secret: [\"RVXO9SOJskjw4LeVupjRbIMJIAyyil8j\"] issuer: http://127.0.0.1:8080/realms/kong authorization_endpoint: http://127.0.0.1:8080/realms/kong/protocol/openid-connect/auth token_endpoint: http://keycloak.keycloak:8080/realms/kong/protocol/openid-connect/token extra_jwks_uris: [\"http://keycloak.keycloak.svc.cluster.local:8080/realms/kong/protocol/openid-connect/certs\"] consumer_optional: false consumer_claim: [\"preferred_username\"] consumer_by: [\"username\"] consumers: - username: consumer1 EOF An important observation here is that we have the OpenId Connect plugin configured with the Kong Consumer mapping. The consumer_claim setting specifies that the plugin will take the preferred_username field from the Access Token to map it to some Kong Consumer. The Kong Consumer chosen is the one that has the same preferred_username as its username. The declaration above configures the OIDC plugin as well as creates the necessary consumer. Later on you can apply plugin to the Kong Consumer to define specific policies.\nSubmit the declaration deck gateway sync --konnect-token $PAT oidc.yaml Verification Redirect your browser the following URL. Since you haven’t been authenticated, you will be redirected to Keycloak’s Authentication page:\nhttp://localhost/oidc-route/get As credentials, enter the Keycloak user and password previously created: consumer1/kong. After filling out the form with your email and name you, Keycloak authenticates you and redirects you back to the original URL (Data Plane), this time adding the Authorization Code. Following the steps described previously, the OpenId Connect plugin sends another request to Keycloak, using the client_id and client_secret pair, configured in the plugin, to validate the Authorization Code and ask Keycloak to issue the Access Token. The Data Plane finally routes the request to the HTTPbin application. As expected, the response, this time, includes the access token issued by Keycloak which was injected by the plugin as a Bearer Token\nIf you copy the token and decodes it with jwt you should see an output similar to this:\nToken header ------------ { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"JIao4TIXpSwJxcukz6W0hK8qc_vuYf6HrmGsDmT6kzY\" } Token claims ------------ { \"acr\": \"0\", \"allowed-origins\": [ \"/*\" ], \"aud\": \"account\", \"auth_time\": 1754941018, \"azp\": \"kong_id\", \"email\": \"claudio.acquaviva@gmail.com\", \"email_verified\": false, \"exp\": 1754941664, \"family_name\": \"Acquaviva\", \"given_name\": \"Claudio\", \"iat\": 1754941364, \"iss\": \"http://127.0.0.1:8080/realms/kong\", \"jti\": \"onrtac:db97da58-1793-834c-dae8-b46fd2d65cfd\", \"name\": \"Claudio Acquaviva\", \"preferred_username\": \"consumer1\", \"realm_access\": { \"roles\": [ \"offline_access\", \"uma_authorization\", \"default-roles-kong\" ] }, \"resource_access\": { \"account\": { \"roles\": [ \"manage-account\", \"manage-account-links\", \"view-profile\" ] } }, \"scope\": \"openid email profile\", \"sid\": \"8bdb5227-c84b-4f06-ad00-9632dbdd9397\", \"sub\": \"88dffdc3-c80f-4349-9a73-4c8da93bb290\", \"typ\": \"Bearer\" } Kong-gratulations! have now reached the end of this module by authenticating your API requests with Keycloak. You can now click Next to proceed with the next module.",
    "description": "This page describes a configuration of the Authorization Code Grant. Check the OpenID Connect plugin documentation to learn more about it.\nInstalling OpenID Connect Plugin All Keycloak settings are available for the OIDC plugin in the following address:\ncurl http://localhost:8080/realms/kong/.well-known/openid-configuration | jq The most important ones are the endpoints necessary to implement the Grant:\ncat \u003e oidc.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: oidc-route paths: - /oidc-route plugins: - name: openid-connect instance_name: openid-connect1 config: auth_methods: [\"authorization_code\"] redirect_uri: - http://localhost/oidc-route/get client_id: [\"kong_id\"] client_secret: [\"RVXO9SOJskjw4LeVupjRbIMJIAyyil8j\"] issuer: http://127.0.0.1:8080/realms/kong authorization_endpoint: http://127.0.0.1:8080/realms/kong/protocol/openid-connect/auth token_endpoint: http://keycloak.keycloak:8080/realms/kong/protocol/openid-connect/token extra_jwks_uris: [\"http://keycloak.keycloak.svc.cluster.local:8080/realms/kong/protocol/openid-connect/certs\"] consumer_optional: false consumer_claim: [\"preferred_username\"] consumer_by: [\"username\"] consumers: - username: consumer1 EOF An important observation here is that we have the OpenId Connect plugin configured with the Kong Consumer mapping. The consumer_claim setting specifies that the plugin will take the preferred_username field from the Access Token to map it to some Kong Consumer. The Kong Consumer chosen is the one that has the same preferred_username as its username. The declaration above configures the OIDC plugin as well as creates the necessary consumer. Later on you can apply plugin to the Kong Consumer to define specific policies.",
    "tags": [],
    "title": "Authorization Code",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/1574-oidc_authorizationcode/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases \u003e OpenID Connect",
    "content": "This page describes a configuration of the Client Credentials Grant. Check the OpenID Connect plugin documentation to learn more about it.\nThe main use case for the OAuth Client Credentials Grant is to address application authentication rather than user authentication. In such a scenario, authentication processes based on userid and password are not feasible. Instead, applications should deal with Client IDs and Client Secrets to authenticate and get a token.\nInstalling OpenID Connect Plugin cat \u003e oidc.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: oidc-route paths: - /oidc-route plugins: - name: openid-connect instance_name: openid-connect1 config: auth_methods: [\"client_credentials\"] issuer: http://keycloak.keycloak:8080/realms/kong token_endpoint: http://keycloak.keycloak:8080/realms/kong/protocol/openid-connect/token extra_jwks_uris: [\"http://keycloak.keycloak.svc.cluster.local:8080/realms/kong/protocol/openid-connect/certs\"] consumer_optional: false consumer_claim: [\"client_id\"] consumer_by: [\"username\"] consumers: - username: kong_id EOF Note that we are going to map the Access Token to the Kong Consumer based on the client_id now.\nSubmit the declaration deck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f deck gateway sync --konnect-token $PAT oidc.yaml Verification curl -sX GET http://localhost/oidc-route/get -u \"kong_id:RVXO9SOJskjw4LeVupjRbIMJIAyyil8j\" | jq -r '.headers.Authorization' | cut -d \" \" -f 2 | jwt decode - Expected Output\nToken header ------------ { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"JIao4TIXpSwJxcukz6W0hK8qc_vuYf6HrmGsDmT6kzY\" } Token claims ------------ { \"acr\": \"1\", \"allowed-origins\": [ \"/*\" ], \"aud\": \"account\", \"azp\": \"kong_id\", \"clientAddress\": \"10.244.0.106\", \"clientHost\": \"10.244.0.106\", \"client_id\": \"kong_id\", \"email_verified\": false, \"exp\": 1754944713, \"iat\": 1754944413, \"iss\": \"http://keycloak.keycloak:8080/realms/kong\", \"jti\": \"trrtcc:643cf4cd-44ae-2a3f-d664-6580b274a108\", \"preferred_username\": \"service-account-kong_id\", \"realm_access\": { \"roles\": [ \"offline_access\", \"uma_authorization\", \"default-roles-kong\" ] }, \"resource_access\": { \"account\": { \"roles\": [ \"manage-account\", \"manage-account-links\", \"view-profile\" ] } }, \"scope\": \"openid email profile\", \"sub\": \"e7b5a37b-d06a-4b40-92d7-36f09768ed79\", \"typ\": \"Bearer\" } Kong-gratulations! have now reached the end of this module by authenticating your API requests with Keycloak. You can now click Next to proceed with the next module.",
    "description": "This page describes a configuration of the Client Credentials Grant. Check the OpenID Connect plugin documentation to learn more about it.\nThe main use case for the OAuth Client Credentials Grant is to address application authentication rather than user authentication. In such a scenario, authentication processes based on userid and password are not feasible. Instead, applications should deal with Client IDs and Client Secrets to authenticate and get a token.\nInstalling OpenID Connect Plugin cat \u003e oidc.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: oidc-route paths: - /oidc-route plugins: - name: openid-connect instance_name: openid-connect1 config: auth_methods: [\"client_credentials\"] issuer: http://keycloak.keycloak:8080/realms/kong token_endpoint: http://keycloak.keycloak:8080/realms/kong/protocol/openid-connect/token extra_jwks_uris: [\"http://keycloak.keycloak.svc.cluster.local:8080/realms/kong/protocol/openid-connect/certs\"] consumer_optional: false consumer_claim: [\"client_id\"] consumer_by: [\"username\"] consumers: - username: kong_id EOF Note that we are going to map the Access Token to the Kong Consumer based on the client_id now.",
    "tags": [],
    "title": "Client Credentials",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/1575-oidc_clientcredentials/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases \u003e OpenID Connect",
    "content": "So far, we have used the OpenID Connect plugin to implement Authentication processes only.\nThe aud (Audience) claim comes from the JWT specification in [RFC 7519]. It allows the receiving party to verify whether a given JWT was intended for them. Per the specification, the aud value can be a single string or an array of strings. aud - Identifies the audience (resource URI or server) that this access token is intended for.\nThe scope claim originates from the OAuth 2.0 specification in [RFC 6749]. It defines the range of access granted by an access token, limiting it to specific claims or user data. For example, you might not want a third-party client to query any arbitrary resource using an OAuth 2.0 access token. Instead, the scope claim restricts the token’s permissions to a predefined set of resources or operations. scp - Array of scopes that are granted to this access token.\nThe OpenID Connect plugin supports some coarse-grained authorization mechanisms:\nClaims-based authorization ACL plugin authorization Consumer authorization This section is going to show how to use the plugin to implement an Authorization mechanism based on the OAuth Scopes.\nOAuth Scopes allow us to limit access to an Access Token. The configuration gets started, including a new setting to our OpenId Connect plugin: “audience_required”. The following configuration defines that the Kong Route can be consumed by requests that have Access Tokens with the “aud” field set as “gold”. This is a nice option to implement, for instance, Kong Konnect consumer classes.\nInstalling OpenID Connect Plugin cat \u003e oidc.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: oidc-route paths: - /oidc-route plugins: - name: openid-connect instance_name: openid-connect1 config: auth_methods: [\"client_credentials\"] issuer: http://keycloak.keycloak:8080/realms/kong token_endpoint: http://keycloak.keycloak:8080/realms/kong/protocol/openid-connect/token extra_jwks_uris: [\"http://keycloak.keycloak.svc.cluster.local:8080/realms/kong/protocol/openid-connect/certs\"] consumer_optional: false consumer_claim: [\"client_id\"] consumer_by: [\"username\"] audience_required: [\"gold\"] consumers: - username: kong_id EOF Submit the declaration deck gateway sync --konnect-token $PAT oidc.yaml If we try to consume the Kong Route we are going to get an new error:\ncurl -iX GET http://localhost/oidc-route/get -u \"kong_id:RVXO9SOJskjw4LeVupjRbIMJIAyyil8j\" HTTP/1.1 403 Forbidden Date: Mon, 11 Aug 2025 20:39:14 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Bearer realm=\"keycloak.keycloak\", error=\"insufficient_scope\" Content-Length: 23 X-Kong-Response-Latency: 38 Server: kong/3.11.0.2-enterprise-edition X-Kong-Request-Id: 19afd85ac97a9657b4920a3f69c8783e {\"message\":\"Forbidden\"} Note that the response describes the reason why we cannot consume the Route.\nCreate the Keycloak Client Scope The first thing to do is to create a Client Scope in Keycloak. Go to the kong realm and click the Client scopes option in the left menu. Name the Client Scope as kong_scope and click “Save”.\nClick the Mappers tab now and choose Configure a new mapper. Choose Audience and name it as kong_mapper. For the Included Custom Audience field type gold, which is the audience the plugin has been configured. Click Save.\nNow click on the Clients option in the left menu and choose our kong_id client. Client the Client scopes tab and add the new kong_scope we just created it as Default:\nAs you can see in our previous requests, Keycloak adds, by default, the account audience as aud: account field inside the Access Token. One last optional step is to remove it, so the token should have our “gold” audience only. To do that, click the default \u003cclient_id\u003e-dedicated Client Scope (in our case, kong_id-dedicated) and its Scope tab. Inside the Scope tab, turn the “Full scope allowed” option off.\nTest the Keycloak Endpoint Send a request to Keycloak again to test the new configuration:\ncurl -s -X POST 'http://localhost:8080/realms/kong/protocol/openid-connect/token' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'client_id=kong_id' \\ --data-urlencode 'client_secret=RVXO9SOJskjw4LeVupjRbIMJIAyyil8j' \\ --data-urlencode 'grant_type=client_credentials' | jq -r '.access_token' | jwt decode - | grep aud Expected output\n\"aud\": \"gold\", Consume the Kong Route again You should be able to consumer the Kong Route now.\ncurl -sX GET http://localhost:80/oidc-route/get -u \"kong_id:RVXO9SOJskjw4LeVupjRbIMJIAyyil8j\"| jq -r '.headers.Authorization' | cut -d \" \" -f 2 | jwt decode - Expected output\nToken header ------------ { \"typ\": \"JWT\", \"alg\": \"RS256\", \"kid\": \"JIao4TIXpSwJxcukz6W0hK8qc_vuYf6HrmGsDmT6kzY\" } Token claims ------------ { \"acr\": \"1\", \"allowed-origins\": [ \"/*\" ], \"aud\": \"gold\", \"azp\": \"kong_id\", \"clientAddress\": \"10.244.0.106\", \"clientHost\": \"10.244.0.106\", \"client_id\": \"kong_id\", \"email_verified\": false, \"exp\": 1754945838, \"iat\": 1754945538, \"iss\": \"http://keycloak.keycloak:8080/realms/kong\", \"jti\": \"trrtcc:332fad95-49bc-f8c7-7f1b-e4ef7d7b0973\", \"preferred_username\": \"service-account-kong_id\", \"scope\": \"openid email profile\", \"sub\": \"e7b5a37b-d06a-4b40-92d7-36f09768ed79\", \"typ\": \"Bearer\" } Kong-gratulations! have now reached the end of this module by authenticating your API requests with Keycloak. You can now click Next to proceed with the next module.",
    "description": "So far, we have used the OpenID Connect plugin to implement Authentication processes only.\nThe aud (Audience) claim comes from the JWT specification in [RFC 7519]. It allows the receiving party to verify whether a given JWT was intended for them. Per the specification, the aud value can be a single string or an array of strings. aud - Identifies the audience (resource URI or server) that this access token is intended for.",
    "tags": [],
    "title": "Authorization",
    "uri": "/12-api-gateway/15-use-cases/157-openidconnect/1576-oidc_accesscontrol/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Introduction Kong Konnect is an API lifecycle management platform delivered as a service. The management plane is hosted in the cloud by Kong, while the runtime environments are deployed in your personal environment. Management plane enables customers to securely execute API management activities such as create API routes, define services etc. Runtime environments connect with the management plane using mutual transport layer authentication (mTLS), receive the updates and take customer facing API traffic.\nLearning Objectives In this workshop, you will:\nGet an architectural overview of Kong Konnect platform.\nSet up Konnect runtime on Kubernetes Cluster.\nLearn what are services, routes and plugin.\nDeploy a sample microservice and access the application using the defined route.\nUse the platform to address the following API Gateway use cases\nProxy caching Authentication and Authorization Response Transformer Request Callout Rate limiting Observability And the following AI Gateway use cases\nPrompt Engineering LLM-based Request and Reponse transformation Semantic Caching Token-based Rate Limiting Semantic Routing RAG - Retrieval-Augmented Generation Expected Duration Workshop Introduction (15 minutes) Architectural Walkthrough (10 minutes) Sample Application and initial use case (15 minutes) Addressing API and AI Gateway use cases (90 minutes) Observability (30 minutes) Next Steps and Cleanup (5 min)",
    "description": "Introduction Kong Konnect is an API lifecycle management platform delivered as a service. The management plane is hosted in the cloud by Kong, while the runtime environments are deployed in your personal environment. Management plane enables customers to securely execute API management activities such as create API routes, define services etc. Runtime environments connect with the management plane using mutual transport layer authentication (mTLS), receive the updates and take customer facing API traffic.",
    "tags": [],
    "title": "API Management with Kong Konnect",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
