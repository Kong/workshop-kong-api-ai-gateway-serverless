<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Proxy Advanced :: API Management with Kong Konnect</title>
    <link>http://localhost:1313/20-observability/17-use-cases/159-ai-proxy-advanced/index.html</link>
    <description>The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.&#xA;The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.&#xA;Load balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/20-observability/17-use-cases/159-ai-proxy-advanced/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Round Robin</title>
      <link>http://localhost:1313/20-observability/17-use-cases/159-ai-proxy-advanced/2-round-robin/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/20-observability/17-use-cases/159-ai-proxy-advanced/2-round-robin/index.html</guid>
      <description>Let’s get started with a simple round-robin policy:&#xA;cat &gt; ai-proxy-advanced.yaml &lt;&lt; &#39;EOF&#39; _format_version: &#34;3.0&#34; _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: round-robin targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: &#34;llm/v1/chat&#34; auth: header_name: Authorization header_value: Bearer ${{ env &#34;DECK_OPENAI_API_KEY&#34; }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: &#34;llm/v1/chat&#34; EOF Apply the declaration with decK:</description>
    </item>
    <item>
      <title>Weight</title>
      <link>http://localhost:1313/20-observability/17-use-cases/159-ai-proxy-advanced/3-weight/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/20-observability/17-use-cases/159-ai-proxy-advanced/3-weight/index.html</guid>
      <description>Now, let’s redirect 80% of the request to OpenAI’s gpt-4.1 with a weight based policy:&#xA;cat &gt; ai-proxy-advanced.yaml &lt;&lt; &#39;EOF&#39; _format_version: &#34;3.0&#34; _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: &#34;llm/v1/chat&#34; auth: header_name: Authorization header_value: Bearer ${{ env &#34;DECK_OPENAI_API_KEY&#34; }} weight: 80 - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: &#34;llm/v1/chat&#34; weight: 20 EOF Apply the declaration with decK:</description>
    </item>
    <item>
      <title>Lowest-Latency and Lowest-Usage</title>
      <link>http://localhost:1313/20-observability/17-use-cases/159-ai-proxy-advanced/4-lowest-latency-usage/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/20-observability/17-use-cases/159-ai-proxy-advanced/4-lowest-latency-usage/index.html</guid>
      <description>Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.&#xA;Create a file with the following declaration:&#xA;cat &gt; ai-proxy-advanced.yaml &lt;&lt; &#39;EOF&#39; _format_version: &#34;3.0&#34; _info: select_tags: - llm _konnect: control_plane_name: kong-workshop services: - name: ai-proxy-advanced-service host: localhost port: 32000 routes: - name: route1 paths: - /route1 plugins: - name: ai-proxy-advanced instance_name: ai-proxy-advanced1 config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: - model: provider: openai name: gpt-4.1 options: temperature: 1.0 route_type: &#34;llm/v1/chat&#34; auth: header_name: Authorization header_value: Bearer ${{ env &#34;DECK_OPENAI_API_KEY&#34; }} - model: provider: llama2 name: llama3.2:1b options: llama2_format: ollama upstream_url: http://ollama.ollama:11434/api/chat route_type: &#34;llm/v1/chat&#34; EOF Apply the declaration with decK:</description>
    </item>
    <item>
      <title>Semantic Routing</title>
      <link>http://localhost:1313/20-observability/17-use-cases/159-ai-proxy-advanced/5-semantic-routing/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/20-observability/17-use-cases/159-ai-proxy-advanced/5-semantic-routing/index.html</guid>
      <description>Semantic The semantic algorithm distributes requests to different models based on the similarity between the prompt in the request and the description provided in the model configuration. This allows Kong to automatically select the model that is best suited for the given domain or use case. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.</description>
    </item>
  </channel>
</rss>