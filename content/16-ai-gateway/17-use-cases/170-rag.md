---
title : "RAG - Retrieval-Augmented Generation"
weight : 170
---

## What is RAG?

RAG offers a solution to some of the limitations in traditional generative AI models, such as accuracy and hallucinations, allowing companies to create more contextually relevant AI applications.

Basically, the RAG application comprises two main processes:

* Data Preparation: External data sources, including all sorts of documents, are chunked and submitted to an Embedding Model which converts them into embeddings. The embeddings are stored in a Vector Database.

* Query time: A consumer wants to send a query to the actual Prompt/Chat LLM model. However, the query should be enhanced with relevant data, taken from the Vector Database, and not available in the LLM model. The following steps are then performed:
1. The consumer builds a Prompt.
2. The RAG application converts the Prompt into an embedding, calling the Embedding Model.
3. Leveraging Semantic Search, RAG matches the Prompt Embedding with the most relevant information and retrieves that Vector Database.
4. The Vector Database returns relevant data as a response to the search query.
5. The RAG application sends a query to the Prompt/Chat LLM Model combining the Prompt with the Relevant Data returned by the Vector Database.
6. The LLM Model returns a response.


![rag](/static/images/rag.png)


## Implementation Architecture

#### Data Preparation time

![datapreparation_time](/static/images/rag_data_preparation_time.png)

During the preparation time, the following steps are executed:
1. The Document Loader script asks the AI RAG Injector plugin to provides the configuration regarding both Embedding Model and Vector Database.
2. The Document Loader sends data chunks to the Embedding Model to gets the embeddings related to them
3. The Document Loader stores the embeddings and content into the Vector Database.


#### RAG time

![rag_time](/static/images/rag_time.png)

The following steps are performed during the execution time:
1. The API Consumer send a request with a prompt.
2. The AI RAG Injector Plugin converts the prompt into embeddings calling the Embedding Model.
3. The AI RAG Injector Plugin sends a KNN vector search query to the Vector Database to find the top “k-nearest neighbors” to a query vector.
4. The AI Proxy Advanced Plugin sends a regular request to the LLM model adding the relevant data received from the Vector Database.

Here's the declaration including both plugins: AI RAG Injector and AI Proxy Advanced.

```
cat > ai-rag-injector.yaml << 'EOF'
_format_version: '3.0'
_konnect:
  control_plane_name: kong-workshop
services:
- name: ai-proxy
  url: http://localhost:65535
  routes:
  - name: route1
    paths:
    - /route1
  plugins:
  - name: ai-proxy-advanced
    instance_name: ai-proxy-advanced1
    config:
      targets:
      - logging:
          log_statistics: true
        route_type: llm/v1/chat
        auth:
          header_name: Authorization
          header_value: Bearer ${{ env "DECK_OPENAI_API_KEY" }}
        model:
          provider: openai
          name: gpt-5
          options:
            temperature: 1.0
  - name: ai-rag-injector
    id: 3194f12e-60c9-4cb6-9cbc-c8fd7a00cff1
    instance_name: ai-rag-injector1
    config:
      inject_template: |
        Only use the following information surrounded by <RAG></RAG> to and your existing knowledge to provide the best possible answer to the user.
        <RAG><CONTEXT></RAG>
        User's question: <PROMPT>
      fetch_chunks_count: 1
      embeddings:
        auth:
          header_name: Authorization
          header_value: Bearer ${{ env "DECK_OPENAI_API_KEY" }}
        model:
          provider: openai
          name: "text-embedding-3-small"
      vectordb:
        strategy: redis
        redis:
          host: redis-stack.redis
          port: 6379
        distance_metric: cosine
        dimensions: 1024
EOF
```


Apply the declaration with decK:
```
deck gateway reset --konnect-control-plane-name kong-workshop --konnect-token $PAT -f
deck gateway sync --konnect-token $PAT ai-rag-injector.yaml
```


If you send a request with no context, we'll see the LLM is not able to respond to it:
```
curl -s -X POST \
  --url $DATA_PLANE_LB/route1 \
  --header "Content-Type: application/json" \
  --data '{
     "messages": [
       {
         "role": "user",
         "content": "What did Marco say about AI Gateways?"
       }
     ]
   }' | jq '.choices[].message.content'
```

* Typical response:
```
"Could you clarify which Marco you mean and where he said it (e.g., a podcast, post, talk, or email)? If you can share a link, quote, or more context, I can find and summarize exactly what he said about AI Gateways."
```

We are going to inject some context using an [interview transcript](https://softwareengineeringdaily.com/2024/06/20/its-apis-all-the-way-down-with-marco-palladino/) snippet Marco Palladino, Kong's co-founder and CTO, gave some time ago.

The transcript snippet is available in the following file.

```
curl 'https://raw.githubusercontent.com/Kong/workshop-kong-api-ai-gateway/refs/heads/main/content/static/code/SED1683-Kong-short.txt' --output ./SED1683-Kong-short.txt
```

We have to copy the Document Loader script, algo available in a file, to the Data Plane:

```
curl 'https://raw.githubusercontent.com/Kong/workshop-kong-api-ai-gateway/refs/heads/main/content/static/code/ingest_update.lua' --output ./ingest_update.lua
kubectl cp ./ingest_update.lua -n kong $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith("dataplane-"))' | jq -r '.name'):/tmp/ingest_update.lua
```

Now, execute the Document Loader passing the content as a parameter. Note that, to make to process a bit easier, we have created the AI RAG Injector plugin with a pre-defined id.

```
kubectl exec -ti $(kubectl get pod -n kong -o json | jq -r '.items[].metadata | select(.name | startswith("dataplane-"))' | jq -r '.name') -n kong -- kong runner /tmp/ingest_update.lua 3194f12e-60c9-4cb6-9cbc-c8fd7a00cff1 "'"$(cat ./SED1683-Kong-short.txt)"'"
```


You should be able to get a much better response from the LLM model this time:

```
curl -s -X POST \
  --url $DATA_PLANE_LB/route1 \
  --header "Content-Type: application/json" \
  --data '{
     "messages": [
       {
         "role": "user",
         "content": "What did Marco say about AI Gateways?"
       }
     ]
   }' | jq '.choices[].message.content'
```


```
"Marco said that for AI use cases you need an AI gateway to connect multiple LLMs and orchestrate them. He added that Kong can deploy its gateway in this AI gateway role (alongside edge gateway and service mesh) under a unified control plane, so you can see, manage, monitor, consume, and expose APIs consistently across these use cases."
```


If you Redis, you'll se there a new entry for RAG Injector
```
% kubectl exec -it $(kubectl get pod -n redis -o json | jq -r '.items[].metadata.name') -n redis -- redis-cli --scan
"kong_rag_injector:7e9d1404-5cfb-4585-bada-132e6d6595c1"
```


